{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow in ML\n",
    "\n",
    "Refer to [Pytorch workflow](../pytorch_deep_learning/01_pytorch_workflow.ipynb)\n",
    "\n",
    "![](../pytorch_deep_learning/images/01_a_pytorch_workflow.png)\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| **1. Getting data ready** | Data can be almost anything but to get started we're going to create a simple straight line |\n",
    "| **2. Building a model** | Here we'll create a model to learn patterns in the data, we'll also choose a **loss function**, **optimizer** and build a **training loop**. | \n",
    "| **3. Fitting the model to data (training)** | We've got data and a model, now let's let the model (try to) find patterns in the (**training**) data. |\n",
    "| **4. Making predictions and evaluating a model (inference)** | Our model's found patterns in the data, let's compare its findings to the actual (**testing**) data. |\n",
    "| **5. Saving and loading a model** | You may want to use your model elsewhere, or come back to it later, here we'll cover that. |\n",
    "\n",
    "\n",
    "## 1. Data (preparing and loading)\n",
    "\n",
    "Data in ML **can be anything**, but **must be turned into numbers** (normally represented in tensors)\n",
    "\n",
    "### Turn data to number\n",
    "Turning data to numbers is called **Numerical encording**\n",
    "\n",
    "![](../pytorch_deep_learning/images/01-machine-learning-a-game-of-two-parts.png)\n",
    "\n",
    "### Split data into training and test sets\n",
    "\n",
    "| Split | Purpose | Amount of total data | How often is it used? |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **Training set** | The model learns from this data (like the course materials you study during the semester). | ~60-80% | Always |\n",
    "| **Validation set** | The model gets tuned on this data (like the practice exam you take before the final exam). | ~10-20% | Often but not always |\n",
    "| **Testing set** | The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester). | ~10-20% | Always |\n",
    "\n",
    "```{note}\n",
    "Should keep in mind the data explorer's motto... \"visualize, visualize, visualize!\"\n",
    "\n",
    "Think of this whenever you're working with data and turning it into numbers, if you can visualize something, it can do wonders for understanding.\n",
    "\n",
    "Machines love numbers and we humans like numbers too but we also like to look at things.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build model\n",
    "\n",
    "### PyTorch model building essentials\n",
    "\n",
    "PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.\n",
    "They are \n",
    "- [`torch.nn`](https://pytorch.org/docs/stable/nn.html), \n",
    "- [`torch.optim`](https://pytorch.org/docs/stable/optim.html), \n",
    "- [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and \n",
    "- [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html). \n",
    "  \n",
    "\n",
    "| PyTorch module | What does it do? |\n",
    "| ----- | ----- |\n",
    "| [`torch.nn`](https://pytorch.org/docs/stable/nn.html) | Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way). |\n",
    "| [`torch.nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter) | Stores tensors that can be used with `nn.Module`. If `requires_grad=True` gradients (used for updating model parameters via [**gradient descent**](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html))  are calculated automatically, this is often referred to as \"autograd\".  | \n",
    "| [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) | The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass `nn.Module`. Requires a `forward()` method be implemented. | \n",
    "| [`torch.optim`](https://pytorch.org/docs/stable/optim.html) | Contains various optimization algorithms (these tell the model parameters stored in `nn.Parameter` how to best change to improve gradient descent and in turn reduce the loss). | \n",
    "| `def forward()` | All `nn.Module` subclasses require a `forward()` method, this defines the computation that will take place on the data passed to the particular `nn.Module` (e.g. the linear regression formula above). |\n",
    "\n",
    "![](../pytorch_deep_learning/images/01-pytorch-linear-model-annotated.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Create a Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module): #  nn.Module is almost everything in PyTorch (as neural network lego blocks)\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1, # start with random weights (this will get adjusted as the model learns)\n",
    "                                    dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                    requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.randn(1, # start with random bias (this will get adjusted as the model learns)\n",
    "                                    dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                    requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions using torch.inference_mode()\n",
    "\n",
    "`torch.inference_mode()` is used when using a model for inference (making predictions).\n",
    "\n",
    "`torch.inference_mode()` turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make **forward-passes** (data going through the `forward()` method) faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Make predictions with model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(): \n\u001b[1;32m----> 3\u001b[0m     y_preds \u001b[39m=\u001b[39m model_0(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_0' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions with model\n",
    "with torch.inference_mode(): \n",
    "    y_preds = model_0(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model\n",
    "\n",
    "### Creating a loss function and optimizer in PyTorch\n",
    "\n",
    "In ML loss function is also called cost function, objective function,... that is needed to minimize.\n",
    "\n",
    "| Function | What does it do? | Where does it live in PyTorch? | Common values |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **Loss function** | Measures how wrong your models predictions (e.g. `y_preds`) are compared to the truth labels (e.g. `y_test`). Lower the better. | PyTorch has plenty of built-in loss functions in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions). | Mean absolute error (MAE) for regression problems ([`torch.nn.L1Loss()`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)). Binary cross entropy for binary classification problems ([`torch.nn.BCELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)).  |\n",
    "| **Optimizer** | Tells your model how to update its internal parameters to best lower the loss. | You can find various optimization function implementations in [`torch.optim`](https://pytorch.org/docs/stable/optim.html). | Stochastic gradient descent ([`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)). Adam optimizer ([`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)). | \n",
    "\n",
    "Common optimizers:\n",
    "- SGD (stochastic gradient descent) optimizer\n",
    "- Adam optimizer\n",
    "\n",
    "### Creating an optimization loop in PyTorch\n",
    "\n",
    "We will create a **training loop** (and **testing loop**).\n",
    "\n",
    "The training loop involves the model going through the training data and learning the relationships between the `features` and `labels`.\n",
    "\n",
    "The testing loop involves going through the testing data and evaluating how good the patterns are that the model learned on the training data (the model never see's the testing data during training).\n",
    "\n",
    "Each of these is called a \"loop\" because we want our model to look (loop through) at each sample in each dataset.\n",
    "\n",
    "#### PyTorch training loop\n",
    "For the training loop, we'll build the following steps:\n",
    "\n",
    "| Number | Step name | What does it do? | Code example |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Forward pass | The model goes through all of the training data once, performing its `forward()` function calculations. | `model(x_train)` |\n",
    "| 2 | Calculate the loss | The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. | `loss = loss_fn(y_pred, y_train)` | \n",
    "| 3 | Zero gradients | The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step. | `optimizer.zero_grad()` |\n",
    "| 4 | Perform backpropagation on the loss | Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with `requires_grad=True`). This is known as **backpropagation**, hence \"backwards\".  | `loss.backward()` |\n",
    "| 5 | Update the optimizer (**gradient descent**) | Update the parameters with `requires_grad=True` with respect to the loss gradients in order to improve them. | `optimizer.step()` |\n",
    "\n",
    "![pytorch training loop annotated](../pytorch_deep_learning/images/01-pytorch-training-loop-annotated.png)\n",
    "\n",
    "```{note}\n",
    "The above is just one example of how the steps could be ordered or described. With experience you'll find making PyTorch training loops can be quite flexible.\n",
    "\n",
    "And on the ordering of things, the above is a good default order but you may see slightly different orders. Some rules of thumb: \n",
    " * Calculate the loss (`loss = ...`) *before* performing backpropagation on it (`loss.backward()`).\n",
    " * Zero gradients (`optimizer.zero_grad()`) *before* stepping them (`optimizer.step()`).\n",
    " * Step the optimizer (`optimizer.step()`) *after* performing backpropagation on the loss (`loss.backward()`).\n",
    "```\n",
    "\n",
    "#### PyTorch testing loop\n",
    "\n",
    "As for the testing loop (evaluating our model), the typical steps include:\n",
    "\n",
    "| Number | Step name | What does it do? | Code example |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Forward pass | The model goes through all of the training data once, performing its `forward()` function calculations. | `model(x_test)` |\n",
    "| 2 | Calculate the loss | The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. | `loss = loss_fn(y_pred, y_test)` | \n",
    "| 3 | Calulate evaluation metrics (optional) | Alongisde the loss value you may want to calculate other evaluation metrics such as accuracy on the test set. | Custom functions |\n",
    "\n",
    "Notice the testing loop doesn't contain performing backpropagation (`loss.backward()`) or stepping the optimizer (`optimizer.step()`), this is because no parameters in the model are being changed during testing, they've already been calculated. For testing, we're only interested in the output of the forward pass through the model.\n",
    "\n",
    "![pytorch annotated testing loop](../pytorch_deep_learning/images/01-pytorch-testing-loop-annotated.png)\n",
    "\n",
    "```{note}\n",
    "- Training loop and testing loop are normally performed together\n",
    "- In ML, **epoch** means step, like in MD.\n",
    "```\n",
    "\n",
    "Let's put all of the above together and train our model for 100 **epochs** (forward passes through the data) and we'll evaluate it every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m epoch_count \u001b[39m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     11\u001b[0m     \u001b[39m### Training\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     model_0\u001b[39m.\u001b[39mtrain()           \u001b[39m# Put model in training mode (this is the default state of a model)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     y_pred \u001b[39m=\u001b[39m model_0(X_train)        \u001b[39m# 1. Forward pass on train data using the forward() method inside \u001b[39;00m\n\u001b[0;32m     15\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_pred, y_train)  \u001b[39m# 2. Calculate the loss (how different are our models predictions to the ground truth)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_0' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  # seed to make sure reproducing the same random number on different runs or machines \n",
    "\n",
    "epochs = 100    # Set the number of epochs (how many times the model will pass over the training data)\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()           # Put model in training mode (this is the default state of a model)\n",
    "\n",
    "    y_pred = model_0(X_train)        # 1. Forward pass on train data using the forward() method inside \n",
    "    loss = loss_fn(y_pred, y_train)  # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
    "    optimizer.zero_grad()            # 3. Zero grad of the optimizer\n",
    "    loss.backward()                  # 4. Loss backwards\n",
    "    optimizer.step()                 # 5. Progress the optimizer\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()  # Put the model in evaluation mode\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_0(X_test)                              # 1. Forward pass on test data\n",
    "        test_loss = loss_fn(test_pred, y_test.type(torch.float)) # 2. Calculate loss on test data (note: predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
    "\n",
    "        # Print out what's happening\n",
    "        if epoch % 10 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make a prediction with trained model (inference)\n",
    "\n",
    "We used it during training/testing loop.\n",
    "\n",
    "There are three things to remember when making predictions (also called performing inference) with a PyTorch model:\n",
    "1. Set the model in evaluation mode (`model.eval()`).\n",
    "2. Make the predictions using the inference mode context manager (`with torch.inference_mode(): ...`).\n",
    "3. All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.eval()                 # 1. Set the model in evaluation mode\n",
    "with torch.inference_mode():   # 2. Setup the inference mode context manager\n",
    "    model_0.to(device)         # 3. setup device-agnostic, to make all on the same device\n",
    "    X_test = X_test.to(device)\n",
    "    y_preds = model_0(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving and loading a model\n",
    "\n",
    "For saving and loading models in PyTorch, there are three main methods you should be aware of ([PyTorch saving and loading models guide](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference)):\n",
    "\n",
    "| PyTorch method | What does it do? | \n",
    "| ----- | ----- |\n",
    "| [`torch.save`](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save) | Saves a serialzed object to disk using Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. Models, tensors and various other Python objects like dictionaries can be saved using `torch.save`.  | \n",
    "| [`torch.load`](https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load) | Uses `pickle`'s unpickling features to deserialize and load pickled Python object files (like models, tensors or dictionaries) into memory. You can also set which device to load the object to (CPU, GPU etc). |\n",
    "| [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)| Loads a model's parameter dictionary (`model.state_dict()`) using a saved `state_dict()` object. | \n",
    "\n",
    "```{note}\n",
    "As stated in [Python's `pickle` documentation](https://docs.python.org/3/library/pickle.html), the `pickle` module **is not secure**. That means you should only ever unpickle (load) data you trust. That goes for loading PyTorch models as well. Only ever use saved PyTorch models from sources you trust.\n",
    "```\n",
    "\n",
    "### Saving a PyTorch model's `state_dict()`\n",
    "\n",
    "The [recommended way](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference) for saving and loading a model for inference (making predictions) is by saving and loading a model's `state_dict()`.\n",
    "\n",
    "call `torch.save(obj, f)` where `obj` is the target model's `state_dict()` and `f` is the filename of where to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj=model_0.state_dict(), f=filename)  # Save the model state_dict(), only saves the models learned parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a saved PyTorch model's `state_dict()`\n",
    "\n",
    "Since we've now got a saved model `state_dict()` at `models/01_pytorch_workflow_model_0.pth` we can now load it in using `torch.nn.Module.load_state_dict(torch.load(f))` where `f` is the filepath of our saved model `state_dict()`.\n",
    "\n",
    "Why call `torch.load()` inside `torch.nn.Module.load_state_dict()`? \n",
    "\n",
    "Because we only saved the model's `state_dict()` which is a dictionary of learned parameters and not the *entire* model, we first have to load the `state_dict()` with `torch.load()` and then pass that `state_dict()` to a new instance of our model (which is a subclass of `nn.Module`).\n",
    "\n",
    "Why not save the entire model?\n",
    "\n",
    "[Saving the entire model](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model) rather than just the `state_dict()` is more intuitive, however, to quote the PyTorch documentation (italics mine):\n",
    "\n",
    "```{tip}\n",
    "The disadvantage of this approach *(saving the whole model)* is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved...\n",
    "\n",
    "Because of this, your code can break in various ways when used in other projects or after refactors.\n",
    "```\n",
    "\n",
    "So instead, we're using the flexible method of saving and loading just the `state_dict()`, which again is basically a dictionary of model parameters.\n",
    "\n",
    "Let's test it out by created another instance of `LinearRegressionModel()`, which is a subclass of `torch.nn.Module` and will hence have the in-built method `load_state_dit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new instance of our model (this will be instantiated with random weights)\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "\n",
    "# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\n",
    "loaded_model_0.load_state_dict(torch.load(f=filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39mlcvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffd1abbb83318c5604f0d207c5fcbf9a26b71537d918692fbc05e98eeb47c7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
