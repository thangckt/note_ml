{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow in ML\n",
    "\n",
    "Refer to [Pytorch workflow](../pytorch_deep_learning/01_pytorch_workflow.ipynb)\n",
    "\n",
    "![](../01_a_pytorch_workflow.png)\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| **1. Getting data ready** | Data can be almost anything but to get started we're going to create a simple straight line |\n",
    "| **2. Building a model** | Here we'll create a model to learn patterns in the data, we'll also choose a **loss function**, **optimizer** and build a **training loop**. | \n",
    "| **3. Fitting the model to data (training)** | We've got data and a model, now let's let the model (try to) find patterns in the (**training**) data. |\n",
    "| **4. Making predictions and evaluating a model (inference)** | Our model's found patterns in the data, let's compare its findings to the actual (**testing**) data. |\n",
    "| **5. Saving and loading a model** | You may want to use your model elsewhere, or come back to it later, here we'll cover that. |\n",
    "| **6. Putting it all together** | Let's take all of the above and combine it. |\n",
    "\n",
    "\n",
    "## 1. Data (preparing and loading)\n",
    "\n",
    "Data in ML **can be anything**, but **must be turned into numbers** (normally represented in tensors)\n",
    "\n",
    "### Turn data to number\n",
    "Turning data to numbers is called **Numerical encording**\n",
    "\n",
    "![](../pytorch_deep_learning/images/01-machine-learning-a-game-of-two-parts.png)\n",
    "\n",
    "## Split data into training and test sets\n",
    "\n",
    "| Split | Purpose | Amount of total data | How often is it used? |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **Training set** | The model learns from this data (like the course materials you study during the semester). | ~60-80% | Always |\n",
    "| **Validation set** | The model gets tuned on this data (like the practice exam you take before the final exam). | ~10-20% | Often but not always |\n",
    "| **Testing set** | The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester). | ~10-20% | Always |\n",
    "\n",
    "```{note}\n",
    "Should keep in mind the data explorer's motto... \"visualize, visualize, visualize!\"\n",
    "\n",
    "Think of this whenever you're working with data and turning it into numbers, if you can visualize something, it can do wonders for understanding.\n",
    "\n",
    "Machines love numbers and we humans like numbers too but we also like to look at things.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build model\n",
    "\n",
    "### PyTorch model building essentials\n",
    "\n",
    "PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.\n",
    "They are \n",
    "- [`torch.nn`](https://pytorch.org/docs/stable/nn.html), \n",
    "- [`torch.optim`](https://pytorch.org/docs/stable/optim.html), \n",
    "- [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and \n",
    "- [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html). \n",
    "  \n",
    "\n",
    "| PyTorch module | What does it do? |\n",
    "| ----- | ----- |\n",
    "| [`torch.nn`](https://pytorch.org/docs/stable/nn.html) | Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way). |\n",
    "| [`torch.nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter) | Stores tensors that can be used with `nn.Module`. If `requires_grad=True` gradients (used for updating model parameters via [**gradient descent**](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html))  are calculated automatically, this is often referred to as \"autograd\".  | \n",
    "| [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) | The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass `nn.Module`. Requires a `forward()` method be implemented. | \n",
    "| [`torch.optim`](https://pytorch.org/docs/stable/optim.html) | Contains various optimization algorithms (these tell the model parameters stored in `nn.Parameter` how to best change to improve gradient descent and in turn reduce the loss). | \n",
    "| `def forward()` | All `nn.Module` subclasses require a `forward()` method, this defines the computation that will take place on the data passed to the particular `nn.Module` (e.g. the linear regression formula above). |\n",
    "\n",
    "![](../pytorch_deep_learning/images/01-pytorch-linear-model-annotated.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module): #  nn.Module is almost everything in PyTorch (as neural network lego blocks)\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1, # start with random weights (this will get adjusted as the model learns)\n",
    "                                    dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                    requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.randn(1, # start with random bias (this will get adjusted as the model learns)\n",
    "                                    dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                    requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions using torch.inference_mode()\n",
    "\n",
    "`torch.inference_mode()` is used when using a model for inference (making predictions).\n",
    "\n",
    "`torch.inference_mode()` turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make **forward-passes** (data going through the `forward()` method) faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "with torch.inference_mode(): \n",
    "    y_preds = model_0(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model\n",
    "\n",
    "### Creating a loss function and optimizer in PyTorch\n",
    "\n",
    "In ML loss function is also called cost function, objective function,... that is needed to minimize.\n",
    "\n",
    "| Function | What does it do? | Where does it live in PyTorch? | Common values |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **Loss function** | Measures how wrong your models predictions (e.g. `y_preds`) are compared to the truth labels (e.g. `y_test`). Lower the better. | PyTorch has plenty of built-in loss functions in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions). | Mean absolute error (MAE) for regression problems ([`torch.nn.L1Loss()`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)). Binary cross entropy for binary classification problems ([`torch.nn.BCELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)).  |\n",
    "| **Optimizer** | Tells your model how to update its internal parameters to best lower the loss. | You can find various optimization function implementations in [`torch.optim`](https://pytorch.org/docs/stable/optim.html). | Stochastic gradient descent ([`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)). Adam optimizer ([`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)). | \n",
    "\n",
    "Common optimizers:\n",
    "- SGD (stochastic gradient descent) optimizer\n",
    "- Adam optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b6e7cfdce5ef245d32482b7f80393907c6182a3f3a40203474e09cb3d62b454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
