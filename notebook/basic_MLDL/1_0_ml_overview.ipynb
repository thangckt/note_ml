{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Overview of Machine Learning\n",
    "\n",
    "We will learn about how machine learning is a method of modeling data, typically with predictive functions. Machine learning includes many techniques, but here we will focus on only those necessary to transition into deep learning. For example, random forests, support vector machines, and nearest neighbor are widely-used machine learning techniques that are effective but not covered here.\n",
    "\n",
    "What is about the model ?\n",
    "\n",
    "We want a model capable of handling our `inputs` and producing something in the shape of our `ouputs`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "Additional Dimensions\n",
    "- Complexity: multiple source and data streams\n",
    "- Variability\n",
    "    - Unpredictable Data flows\n",
    "    - Social media trending\n",
    "\n",
    "Why Big Data is important\n",
    "- Data constains information\n",
    "- information lead to insights\n",
    "- Insights helps in making better decisions\n",
    "\n",
    "How to derive insights from data?\n",
    "\n",
    "--> Machine Leanring\n",
    "\n",
    "Conclusions:\n",
    "- Data is nothing without insights\n",
    "- Machine Learning is the key for deriving inisghts from data\n",
    "- Big Data and Machine Learning ha a huge potential"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm in ML\n",
    "\n",
    "The below picture shows an overview of machine learning\n",
    "\n",
    "<img src=\"image/1_1_machine-learning.png\" style=\"width:600; align:center\" />\n",
    "\n",
    "### Supervised Learning\n",
    "Given `features` we want our model to predict `label`. [See more](https://thangckt.github.io/pytorch_deep_learning/02_pytorch_classification/#2-building-a-model)\n",
    "\n",
    "- Classification\n",
    "    - Decision Trees\n",
    "    - Naive Bayers Classification\n",
    "- Regession\n",
    "    - Ordinary Least Squares Regression\n",
    "    - Logistic Regession\n",
    "    - Support Vector Machines\n",
    "    - Ensemble Methods\n",
    "\n",
    "### Unsuppervised Learning\n",
    "No `label` in this type\n",
    "- Clustering\n",
    "    - Centroid-based algorithm\n",
    "    - Connectivity-based algorithm\n",
    "    - Density-based algorithm\n",
    "    - Probabilistic\n",
    "    - Dimensionality Reduction\n",
    "    - Neural network/ Deep Learning\n",
    "- Pricipal Component Analysis\n",
    "- Independent Component Analysis\n",
    "- Singular Value Decomposition\n",
    "\n",
    "### Reinforement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ingredients \n",
    "\n",
    "Machine learning the fitting of models $\\hat{f}(\\vec{x})$ to data $\\vec{x}, y$ that we know came from some ``data generation'' process $f(x)$ . Firstly, definitions:\n",
    "\n",
    "**Features** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ vectors $\\{\\vec{x}_i\\}$ of dimension $D$. Can be reals, integers, etc.\n",
    "\n",
    "**Labels** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ integers or reals $\\{y_i\\}$. $y_i$ is usually a scalar\n",
    "  \n",
    "**Labeled Data** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ tuples $\\{\\left(\\vec{x}_i, y_i\\right)\\}$ \n",
    "\n",
    "**Unlabeled Data** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ features  $\\{\\vec{x}_i\\}$  that may have unknown $y$ labels\n",
    "\n",
    "**Data generation process**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The unseen process $f(\\vec{x})$ that takes a given feature vector in and returns a real label $y$ (what we're trying to model)\n",
    "\n",
    "**Model**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;A function $\\hat{f}(\\vec{x})$ that takes a given feature vector in and returns a predicted $\\hat{y}$\n",
    "\n",
    "**Predictions**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{y}$, our predicted output for a given input $\\vec{x}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The content in this part is primary from: \n",
    "- [Deep Learning for molecules & materials](https://dmol.pub/ml)\n",
    "```\n",
    "\n",
    "```{seealso}\n",
    "1. [<ins>Introductory Machine Learning</ins>](https://ai.stanford.edu/~nilsson/mlbook.html)\n",
    "2. Two reviews of machine learning in materials{cite}`fung2021benchmarking,balachandran2019machine`\n",
    "3. A review of machine learning in computational chemistry{cite}`gomez2020machine`\n",
    "4. A review of machine learning in metals{cite}`nandy2018strategies`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminologies in ML\n",
    "\n",
    "- The patterns: the learned parameters in model, or the parameters to find in the relationship between inputs and outputs. For e.g., in linear model $y = ax +b$, the learned patterns (paramters to be found) are the weight `a` and the bias `b`.\n",
    "- Hidden units: neurons in hidden layers\n",
    "- Hypeparameters: are all user-choice parameters in model (e.g., learning rate, number of layers, number of neuron in layers,...)\n",
    "- Epoch: step\n",
    "- Loss function: measures how wrong your model predictions are. The higher the loss, the worse your model. It is sometimes calles \"loss criterion\", \"criterion\", or \"cost function\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow in ML\n",
    "\n",
    "This workflow work with PyTorch. See [this lesson](https://thangckt.github.io/pytorch_deep_learning/01_pytorch_workflow/)\n",
    "\n",
    "### 1. Prepare data\n",
    "1. Prepare inputs and output in the format suitable for ML framework will be used (e.g., Pytorch only work with data in the form of torch.tensor)\n",
    "2. Split data into sets of train and test (somtimes are: strain, validation, test)\n",
    "\n",
    "### 2. Build model\n",
    "1. Constructing a model by subclassing `nn.Module` \n",
    "2. Defining a loss function and optimizer.\n",
    "\n",
    "May consider more step: Setting up device agnostic code (so our model can run on CPU or GPU if it's available).\n",
    "\n",
    "### 3. Train model\n",
    "\n",
    "PyTorch steps in training:\n",
    "1. **Forward pass** - The model goes through all of the training data once, performing its `forward()` function calculations (compute `model(x_train)`).\n",
    "2. **Calculate the loss** - The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are (`loss = loss_fn(y_pred, y_train)`).\n",
    "3. **Zero gradients** - The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step (`optimizer.zero_grad()`).\n",
    "4. **Perform backpropagation on the loss** - Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with `requires_grad=True`). This is known as backpropagation, hence \"backwards\" (`loss.backward()`).\n",
    "5. **Step the optimizer (gradient descent)** - Update the parameters with `requires_grad=True` with respect to the loss gradients in order to improve them (`optimizer.step()`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6VivsLTmHC8B"
   },
   "source": [
    "## Improving a model (Hyperparameters tuning) \n",
    "\n",
    "When the model gives bad predictions, there are a few ways to try for making it better. See [example here](pytorch_deep_learning/02_pytorch_classification.ipynb)\n",
    "\n",
    "| Model improvement technique* | What does it do? |\n",
    "| ----- | ----- |\n",
    "| **Add more layers** | Each layer *potentially* increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network *deeper*. |\n",
    "| **Add more hidden units** | Similar to the above, more hidden units per layer means a *potential* increase in learning capabilities of the model, more hidden units is often referred to as making your neural network *wider*. |\n",
    "| **Fitting for longer (more epochs)** | Your model might learn more if it had more opportunities to look at the data. |\n",
    "| **Changing the activation functions** | Some data just can't be fit with only straight lines (like what we've seen), using non-linear activation functions can help with this (hint, hint). |\n",
    "| **Change the learning rate** | Less model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn't learn enough. |\n",
    "| **Change the loss function** | Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won't work with a multi-class classification problem. |\n",
    "| **Use transfer learning** | Take a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in [notebook 06](pytorch_deep_learning/06_pytorch_transfer_learning/). |\n",
    "\n",
    "```{note}\n",
    "- Because you can adjust all of these by hand, they're referred to as **hyperparameters**. \n",
    "- And this is also where **machine learning's half art half science** comes in, there's no real way to know here what the best combination of values is for your project, best to follow the data scientist's motto of *\"experiment, experiment, experiment\"*.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:12:32) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b6e7cfdce5ef245d32482b7f80393907c6182a3f3a40203474e09cb3d62b454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
