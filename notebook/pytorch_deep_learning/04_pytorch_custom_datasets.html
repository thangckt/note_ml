

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>04. PyTorch Custom Datasets</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebook/pytorch_deep_learning/04_pytorch_custom_datasets';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="05. PyTorch Going Modular" href="05_pytorch_going_modular.html" />
    <link rel="prev" title="03. PyTorch Computer Vision" href="03_pytorch_computer_vision.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
  
    <p class="title logo__title"></p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic of ML &amp; DL</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../0_basic_MLDL/1_0_ml_overview.html">Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/1_1_ml_supervised_unsuppersives.html">Supervised vs. Unsuppervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/1_2_regression.html">Regression &amp; Model Assessment</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../0_basic_MLDL/2_0_dl_overview.html">Deep Learning Overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/2_1_dl_neural_network.html">What is a neural network?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/2_2_layers.html">Standard Layers</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../0_basic_MLDL/3_1_workflow.html">Workflow in ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_basic_MLDL/3_2_Model_template.html">Core Ml templates</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PyTorch for Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="00_pytorch_fundamentals.html">00. PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_pytorch_workflow.html">01. PyTorch Workflow Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_pytorch_classification.html">02. PyTorch Neural Network Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_pytorch_computer_vision.html">03. PyTorch Computer Vision</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">04. PyTorch Custom Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_pytorch_going_modular.html">05. PyTorch Going Modular</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_pytorch_transfer_learning.html">06. PyTorch Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_pytorch_experiment_tracking.html">07. PyTorch Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_pytorch_paper_replicating.html">08. PyTorch Paper Replicating</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_pytorch_model_deployment.html">09. PyTorch Model Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_extra_resources.html">PyTorch Extra Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_cheatsheet.html">PyTorch Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_most_common_errors.html">The Three Most Common Errors in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_setup.html">Setup to code PyTorch</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Zero to Mastery Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../zero_to_mastery_ml/README.html">Zero to Mastery Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Practices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1_Practices/1_PT_Linear_Regression.html">Linear Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_Practices/2_PT_Logistic_Regression.html">Logistic Regression</a></li>





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/thangckt/note_ml/edit/main/notebook/pytorch_deep_learning/04_pytorch_custom_datasets.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button"
   title="Suggest edit"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>

</a>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>04. PyTorch Custom Datasets</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-custom-dataset">What is a custom dataset?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-were-going-to-cover">What we’re going to cover</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-can-can-you-get-help">Where can can you get help?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-pytorch-and-setting-up-device-agnostic-code">0. Importing PyTorch and setting up device-agnostic code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-data">1. Get data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#become-one-with-the-data-data-preparation">2. Become one with the data (data preparation)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-an-image">2.1 Visualize an image</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-data">3. Transforming data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-data-with-torchvision-transforms">3.1 Transforming data with <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#option-1-loading-image-data-using-imagefolder">4. Option 1: Loading Image Data Using <code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-loaded-images-into-dataloaders">4.1 Turn loaded images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#option-2-loading-image-data-with-a-custom-dataset">5. Option 2: Loading Image Data with a Custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-helper-function-to-get-class-names">5.1 Creating a helper function to get class names</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-custom-dataset-to-replicate-imagefolder">5.2 Create a custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> to replicate <code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-function-to-display-random-images">5.3 Create a function to display random images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-custom-loaded-images-into-dataloaders">5.4 Turn custom loaded images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-forms-of-transforms-data-augmentation">6. Other forms of transforms (data augmentation)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-0-tinyvgg-without-data-augmentation">7. Model 0: TinyVGG without data augmentation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-transforms-and-loading-data-for-model-0">7.1 Creating transforms and loading data for Model 0</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-tinyvgg-model-class">7.2 Create TinyVGG model class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#try-a-forward-pass-on-a-single-image-to-test-the-model">7.3 Try a forward pass on a single image (to test the model)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-torchinfo-to-get-an-idea-of-the-shapes-going-through-our-model">7.4 Use <code class="docutils literal notranslate"><span class="pre">torchinfo</span></code> to get an idea of the shapes going through our model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-train-test-loop-functions">7.5 Create train &amp; test loop functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-train-function-to-combine-train-step-and-test-step">7.6 Creating a <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to combine <code class="docutils literal notranslate"><span class="pre">train_step()</span></code> and <code class="docutils literal notranslate"><span class="pre">test_step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-and-evaluate-model-0">7.7 Train and Evaluate Model 0</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss-curves-of-model-0">7.8 Plot the loss curves of Model 0</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-should-an-ideal-loss-curve-look-like">8. What should an ideal loss curve look like?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-overfitting">8.1 How to deal with overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-underfitting">8.2 How to deal with underfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-balance-between-overfitting-and-underfitting">8.3 The balance between overfitting and underfitting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1-tinyvgg-with-data-augmentation">9. Model 1: TinyVGG with Data Augmentation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-transform-with-data-augmentation">9.1 Create transform with data augmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-train-and-test-datasets-and-dataloaders">9.2 Create train and test <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#construct-and-train-model-1">9.3 Construct and train Model 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss-curves-of-model-1">9.4 Plot the loss curves of Model 1</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compare-model-results">10. Compare model results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-a-prediction-on-a-custom-image">11. Make a prediction on a custom image</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-in-a-custom-image-with-pytorch">11.1 Loading in a custom image with PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-on-custom-images-with-a-trained-pytorch-model">11.2 Predicting on custom images with a trained PyTorch model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-custom-image-prediction-together-building-a-function">11.3 Putting custom image prediction together: building a function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-takeaways">Main takeaways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extra-curriculum">Extra-curriculum</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/thangckt/pytorch-deep-learning/blob/main/04_pytorch_custom_datasets.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p><a class="reference external" href="https://thangckt.github.io/pytorch_deep_learning/slides/04_pytorch_custom_datasets.pdf">View Slides</a> | <a class="reference external" href="https://youtu.be/Z_ikDlimN6A?t=71010">Watch Video Walkthrough</a></p>
<section class="tex2jax_ignore mathjax_ignore" id="pytorch-custom-datasets">
<h1>04. PyTorch Custom Datasets<a class="headerlink" href="#pytorch-custom-datasets" title="Permalink to this heading">#</a></h1>
<p>In the last notebook, <a class="reference external" href="https://www.learnpytorch.io/03_pytorch_computer_vision/">notebook 03</a>, we looked at how to build computer vision models on an in-built dataset in PyTorch (FashionMNIST).</p>
<p>The steps we took are similar across many different problems in machine learning.</p>
<p>Find a dataset, turn the dataset into numbers, build a model (or find an existing model) to find patterns in those numbers that can be used for prediction.</p>
<p>PyTorch has many built-in datasets used for a wide number of machine learning benchmarks, however, you’ll often want to use your own <strong>custom dataset</strong>.</p>
<section id="what-is-a-custom-dataset">
<h2>What is a custom dataset?<a class="headerlink" href="#what-is-a-custom-dataset" title="Permalink to this heading">#</a></h2>
<p>A <strong>custom dataset</strong> is a collection of data relating to a specific problem you’re working on.</p>
<p>In essence, a <strong>custom dataset</strong> can be comprised of almost anything.</p>
<p>For example, if we were building a food image classification app like <a class="reference external" href="https://nutrify.app">Nutrify</a>, our custom dataset might be images of food.</p>
<p>Or if we were trying to build a model to classify whether or not a text-based review on a website was positive or negative, our custom dataset might be examples of existing customer reviews and their ratings.</p>
<p>Or if we were trying to build a sound classification app, our custom dataset might be sound samples alongside their sample labels.</p>
<p>Or if we were trying to build a recommendation system for customers purchasing things on our website, our custom dataset might be examples of products other people have bought.</p>
<img alt="different pytorch domain libraries can be used for specific PyTorch problems" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-pytorch-domain-libraries.png" />
<p><em>PyTorch includes many existing functions to load in various custom datasets in the <a class="reference external" href="https://pytorch.org/vision/stable/index.html"><code class="docutils literal notranslate"><span class="pre">TorchVision</span></code></a>, <a class="reference external" href="https://pytorch.org/text/stable/index.html"><code class="docutils literal notranslate"><span class="pre">TorchText</span></code></a>, <a class="reference external" href="https://pytorch.org/audio/stable/index.html"><code class="docutils literal notranslate"><span class="pre">TorchAudio</span></code></a> and <a class="reference external" href="https://pytorch.org/torchrec/"><code class="docutils literal notranslate"><span class="pre">TorchRec</span></code></a> domain libraries.</em></p>
<p>But sometimes these existing functions may not be enough.</p>
<p>In that case, we can always subclass <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> and customize it to our liking.</p>
</section>
<section id="what-were-going-to-cover">
<h2>What we’re going to cover<a class="headerlink" href="#what-were-going-to-cover" title="Permalink to this heading">#</a></h2>
<p>We’re going to be applying the PyTorch Workflow we covered in <a class="reference external" href="https://www.learnpytorch.io/01_pytorch_workflow/">notebook 01</a> and <a class="reference external" href="https://www.learnpytorch.io/02_pytorch_classification/">notebook 02</a> to a computer vision problem.</p>
<p>But instead of using an in-built PyTorch dataset, we’re going to be using our own dataset of pizza, steak and sushi images.</p>
<p>The goal will be to load these images and then build a model to train and predict on them.</p>
<img alt="building a pipeline to load in food images and then building a pytorch model to classify those food images" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-pytorch-food-vision-layout.png" />
<p><em>What we’re going to build. We’ll use <code class="docutils literal notranslate"><span class="pre">torchvision.datasets</span></code> as well as our own custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class to load in images of food and then we’ll build a PyTorch computer vision model to hopefully be able to classify them.</em></p>
<p>Specifically, we’re going to cover:</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Topic</strong></p></th>
<th class="head"><p><strong>Contents</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>0. Importing PyTorch and setting up device-agnostic code</strong></p></td>
<td><p>Let’s get PyTorch loaded and then follow best practice to setup our code to be device-agnostic.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>1. Get data</strong></p></td>
<td><p>We’re going to be using our own <strong>custom dataset</strong> of pizza, steak and sushi images.</p></td>
</tr>
<tr class="row-even"><td><p><strong>2. Become one with the data (data preparation)</strong></p></td>
<td><p>At the beginning of any new machine learning problem, it’s paramount to understand the data you’re working with. Here we’ll take some steps to figure out what data we have.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>3. Transforming data</strong></p></td>
<td><p>Often, the data you get won’t be 100% ready to use with a machine learning model, here we’ll look at some steps we can take to <em>transform</em> our images so they’re ready to be used with a model.</p></td>
</tr>
<tr class="row-even"><td><p><strong>4. Loading data with <code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code> (option 1)</strong></p></td>
<td><p>PyTorch has many in-built data loading functions for common types of data. <code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code> is helpful if our images are in standard image classification format.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>5. Loading image data with a custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code></strong></p></td>
<td><p>What if PyTorch didn’t have an in-built function to load data with? This is where we can build our own custom subclass of <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><strong>6. Other forms of transforms (data augmentation)</strong></p></td>
<td><p>Data augmentation is a common technique for expanding the diversity of your training data. Here we’ll explore some of <code class="docutils literal notranslate"><span class="pre">torchvision</span></code>’s in-built data augmentation functions.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>7. Model 0: TinyVGG without data augmentation</strong></p></td>
<td><p>By this stage, we’ll have our data ready, let’s build a model capable of fitting it. We’ll also create some training and testing functions for training and evaluating our model.</p></td>
</tr>
<tr class="row-even"><td><p><strong>8. Exploring loss curves</strong></p></td>
<td><p>Loss curves are a great way to see how your model is training/improving over time. They’re also a good way to see if your model is <strong>underfitting</strong> or <strong>overfitting</strong>.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>9. Model 1: TinyVGG with data augmentation</strong></p></td>
<td><p>By now, we’ve tried a model <em>without</em>, how about we try one <em>with</em> data augmentation?</p></td>
</tr>
<tr class="row-even"><td><p><strong>10. Compare model results</strong></p></td>
<td><p>Let’s compare our different models’ loss curves and see which performed better and discuss some options for improving performance.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>11. Making a prediction on a custom image</strong></p></td>
<td><p>Our model is trained to on a dataset of pizza, steak and sushi images. In this section we’ll cover how to use our trained model to predict on an image <em>outside</em> of our existing dataset.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="where-can-can-you-get-help">
<h2>Where can can you get help?<a class="headerlink" href="#where-can-can-you-get-help" title="Permalink to this heading">#</a></h2>
<p>All of the materials for this course <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning">live on GitHub</a>.</p>
<p>If you run into trouble, you can ask a question on the course <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/discussions">GitHub Discussions page</a> there too.</p>
<p>And of course, there’s the <a class="reference external" href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a> and <a class="reference external" href="https://discuss.pytorch.org/">PyTorch developer forums</a>, a very helpful place for all things PyTorch.</p>
</section>
<section id="importing-pytorch-and-setting-up-device-agnostic-code">
<h2>0. Importing PyTorch and setting up device-agnostic code<a class="headerlink" href="#importing-pytorch-and-setting-up-device-agnostic-code" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Note: this notebook requires torch &gt;= 1.10.0</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;1.12.1+cu113&#39;
</pre></div>
</div>
</div>
</div>
<p>And now let’s follow best practice and setup device-agnostic code.</p>
<blockquote>
<div><p><strong>Note:</strong> If you’re using Google Colab, and you don’t have a GPU turned on yet, it’s now time to turn one on via <code class="docutils literal notranslate"><span class="pre">Runtime</span> <span class="pre">-&gt;</span> <span class="pre">Change</span> <span class="pre">runtime</span> <span class="pre">type</span> <span class="pre">-&gt;</span> <span class="pre">Hardware</span> <span class="pre">accelerator</span> <span class="pre">-&gt;</span> <span class="pre">GPU</span></code>. If you do this, your runtime will likely reset and you’ll have to run all of the cells above by going <code class="docutils literal notranslate"><span class="pre">Runtime</span> <span class="pre">-&gt;</span> <span class="pre">Run</span> <span class="pre">before</span></code>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup device-agnostic code</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="get-data">
<h2>1. Get data<a class="headerlink" href="#get-data" title="Permalink to this heading">#</a></h2>
<p>First thing’s first we need some data.</p>
<p>And like any good cooking show, some data has already been prepared for us.</p>
<p>We’re going to start small.</p>
<p>Because we’re not looking to train the biggest model or use the biggest dataset yet.</p>
<p>Machine learning is an iterative process, start small, get something working and increase when necessary.</p>
<p>The data we’re going to be using is a subset of the <a class="reference external" href="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/">Food101 dataset</a>.</p>
<p>Food101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).</p>
<p>Can you think of 101 different foods?</p>
<p>Can you think of a computer program to classify 101 foods?</p>
<p>I can.</p>
<p>A machine learning model!</p>
<p>Specifically, a PyTorch computer vision model like we covered in <a class="reference external" href="https://www.learnpytorch.io/03_pytorch_computer_vision/">notebook 03</a>.</p>
<p>Instead of 101 food classes though, we’re going to start with 3: pizza, steak and sushi.</p>
<p>And instead of 1,000 images per class, we’re going to start with a random 10% (start small, increase when necessary).</p>
<p>If you’d like to see where the data came from you see the following resources:</p>
<ul class="simple">
<li><p>Original <a class="reference external" href="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/">Food101 dataset and paper website</a>.</p></li>
<li><p><a class="reference external" href="https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.Food101</span></code></a> - the version of the data I downloaded for this notebook.</p></li>
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb"><code class="docutils literal notranslate"><span class="pre">extras/04_custom_data_creation.ipynb</span></code></a> - a notebook I used to format the Food101 dataset to use for this notebook.</p></li>
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip"><code class="docutils literal notranslate"><span class="pre">data/pizza_steak_sushi.zip</span></code></a> - the zip archive of pizza, steak and sushi images from Food101, created with the notebook linked above.</p></li>
</ul>
<p>Let’s write some code to download the formatted data from GitHub.</p>
<blockquote>
<div><p><strong>Note:</strong> The dataset we’re about to use has been pre-formatted for what we’d like to use it for. However, you’ll often have to format your own datasets for whatever problem you’re working on. This is a regular practice in the machine learning world.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># Setup path to data folder</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;data/&quot;</span><span class="p">)</span>
<span class="n">image_path</span> <span class="o">=</span> <span class="n">data_path</span> <span class="o">/</span> <span class="s2">&quot;pizza_steak_sushi&quot;</span>

<span class="c1"># If the image folder doesn&#39;t exist, download it and prepare it... </span>
<span class="k">if</span> <span class="n">image_path</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="s2"> directory exists.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Did not find </span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="s2"> directory, creating one...&quot;</span><span class="p">)</span>
    <span class="n">image_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Download pizza, steak, sushi data</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span> <span class="o">/</span> <span class="s2">&quot;pizza_steak_sushi.zip&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downloading pizza, steak, sushi data...&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

    <span class="c1"># Unzip pizza, steak, sushi data</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">data_path</span> <span class="o">/</span> <span class="s2">&quot;pizza_steak_sushi.zip&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_ref</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Unzipping pizza, steak, sushi data...&quot;</span><span class="p">)</span> 
        <span class="n">zip_ref</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>data/pizza_steak_sushi directory exists.
</pre></div>
</div>
</div>
</div>
</section>
<section id="become-one-with-the-data-data-preparation">
<h2>2. Become one with the data (data preparation)<a class="headerlink" href="#become-one-with-the-data-data-preparation" title="Permalink to this heading">#</a></h2>
<p>Dataset downloaded!</p>
<p>Time to become one with it.</p>
<p>This is another important step before building a model.</p>
<p>As Abraham Lossfunction said…</p>
<img alt="tweet by mrdbourke, if I had eight hours to build a machine learning model, I'd spend the first 6 hours preparing my dataset" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-abraham-lossfunction.png" />
<p><em>Data preparation is paramount. Before building a model, become one with the data. Ask: What am I trying to do here? Source: <a class="reference external" href="https://twitter.com/mrdbourke">&#64;mrdbourke Twitter</a>.</em></p>
<p>What’s inspecting the data and becoming one with it?</p>
<p>Before starting a project or building any kind of model, it’s important to know what data you’re working with.</p>
<p>In our case, we have images of pizza, steak and sushi in standard image classification format.</p>
<p>Image classification format contains separate classes of images in seperate directories titled with a particular class name.</p>
<p>For example, all images of <code class="docutils literal notranslate"><span class="pre">pizza</span></code> are contained in the <code class="docutils literal notranslate"><span class="pre">pizza/</span></code> directory.</p>
<p>This format is popular across many different image classification benchmarks, including <a class="reference external" href="https://www.image-net.org/">ImageNet</a> (of the most popular computer vision benchmark datasets).</p>
<p>You can see an example of the storage format below, the images numbers are arbitrary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pizza_steak_sushi</span><span class="o">/</span> <span class="o">&lt;-</span> <span class="n">overall</span> <span class="n">dataset</span> <span class="n">folder</span>
    <span class="n">train</span><span class="o">/</span> <span class="o">&lt;-</span> <span class="n">training</span> <span class="n">images</span>
        <span class="n">pizza</span><span class="o">/</span> <span class="o">&lt;-</span> <span class="k">class</span> <span class="nc">name</span> <span class="k">as</span> <span class="n">folder</span> <span class="n">name</span>
            <span class="n">image01</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="n">image02</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="o">...</span>
        <span class="n">steak</span><span class="o">/</span>
            <span class="n">image24</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="n">image25</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="o">...</span>
        <span class="n">sushi</span><span class="o">/</span>
            <span class="n">image37</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="o">...</span>
    <span class="n">test</span><span class="o">/</span> <span class="o">&lt;-</span> <span class="n">testing</span> <span class="n">images</span>
        <span class="n">pizza</span><span class="o">/</span>
            <span class="n">image101</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="n">image102</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="o">...</span>
        <span class="n">steak</span><span class="o">/</span>
            <span class="n">image154</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="n">image155</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="o">...</span>
        <span class="n">sushi</span><span class="o">/</span>
            <span class="n">image167</span><span class="o">.</span><span class="n">jpeg</span>
            <span class="o">...</span>
</pre></div>
</div>
<p>The goal will be to <strong>take this data storage structure and turn it into a dataset usable with PyTorch</strong>.</p>
<blockquote>
<div><p><strong>Note:</strong> The structure of the data you work with will vary depending on the problem you’re working on. But the premise still remains: become one with the data, then find a way to best turn it into a dataset compatible with PyTorch.</p>
</div></blockquote>
<p>We can inspect what’s in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.</p>
<p>To do so, we’ll use Python’s in-built <a class="reference external" href="https://docs.python.org/3/library/os.html#os.walk"><code class="docutils literal notranslate"><span class="pre">os.walk()</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="k">def</span> <span class="nf">walk_through_dir</span><span class="p">(</span><span class="n">dir_path</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Walks through dir_path returning its contents.</span>
<span class="sd">  Args:</span>
<span class="sd">    dir_path (str or pathlib.Path): target directory</span>
<span class="sd">  </span>
<span class="sd">  Returns:</span>
<span class="sd">    A print out of:</span>
<span class="sd">      number of subdiretories in dir_path</span>
<span class="sd">      number of images (files) in each subdirectory</span>
<span class="sd">      name of each subdirectory</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">for</span> <span class="n">dirpath</span><span class="p">,</span> <span class="n">dirnames</span><span class="p">,</span> <span class="n">filenames</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="n">dir_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dirnames</span><span class="p">)</span><span class="si">}</span><span class="s2"> directories and </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">)</span><span class="si">}</span><span class="s2"> images in &#39;</span><span class="si">{</span><span class="n">dirpath</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">walk_through_dir</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>There are 2 directories and 1 images in &#39;data/pizza_steak_sushi&#39;.
There are 3 directories and 0 images in &#39;data/pizza_steak_sushi/test&#39;.
There are 0 directories and 19 images in &#39;data/pizza_steak_sushi/test/steak&#39;.
There are 0 directories and 31 images in &#39;data/pizza_steak_sushi/test/sushi&#39;.
There are 0 directories and 25 images in &#39;data/pizza_steak_sushi/test/pizza&#39;.
There are 3 directories and 0 images in &#39;data/pizza_steak_sushi/train&#39;.
There are 0 directories and 75 images in &#39;data/pizza_steak_sushi/train/steak&#39;.
There are 0 directories and 72 images in &#39;data/pizza_steak_sushi/train/sushi&#39;.
There are 0 directories and 78 images in &#39;data/pizza_steak_sushi/train/pizza&#39;.
</pre></div>
</div>
</div>
</div>
<p>Excellent!</p>
<p>It looks like we’ve got about 75 images per training class and 25 images per testing class.</p>
<p>That should be enough to get started.</p>
<p>Remember, these images are subsets of the original Food101 dataset.</p>
<p>You can see how they were created in the <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb">data creation notebook</a>.</p>
<p>While we’re at it, let’s setup our training and testing paths.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup train and testing paths</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;train&quot;</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;test&quot;</span>

<span class="n">train_dir</span><span class="p">,</span> <span class="n">test_dir</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(PosixPath(&#39;data/pizza_steak_sushi/train&#39;),
 PosixPath(&#39;data/pizza_steak_sushi/test&#39;))
</pre></div>
</div>
</div>
</div>
<section id="visualize-an-image">
<h3>2.1 Visualize an image<a class="headerlink" href="#visualize-an-image" title="Permalink to this heading">#</a></h3>
<p>Okay, we’ve seen how our directory structure is formatted.</p>
<p>Now in the spirit of the data explorer, it’s time to <em>visualize, visualize, visualize!</em></p>
<p>Let’s write some code to:</p>
<ol class="arabic simple">
<li><p>Get all of the image paths using <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob"><code class="docutils literal notranslate"><span class="pre">pathlib.Path.glob()</span></code></a> to find all of the files ending in <code class="docutils literal notranslate"><span class="pre">.jpg</span></code>.</p></li>
<li><p>Pick a random image path using Python’s <a class="reference external" href="https://docs.python.org/3/library/random.html#random.choice"><code class="docutils literal notranslate"><span class="pre">random.choice()</span></code></a>.</p></li>
<li><p>Get the image class name using <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.parent"><code class="docutils literal notranslate"><span class="pre">pathlib.Path.parent.stem</span></code></a>.</p></li>
<li><p>And since we’re working with images, we’ll open the random image path using <a class="reference external" href="https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open"><code class="docutils literal notranslate"><span class="pre">PIL.Image.open()</span></code></a> (PIL stands for Python Image Library).</p></li>
<li><p>We’ll then show the image and print some metadata.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Set seed</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># &lt;- try changing this and see what happens</span>

<span class="c1"># 1. Get all image paths (* means &quot;any combination&quot;)</span>
<span class="n">image_path_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">image_path</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*/*/*.jpg&quot;</span><span class="p">))</span>

<span class="c1"># 2. Get random image path</span>
<span class="n">random_image_path</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">image_path_list</span><span class="p">)</span>

<span class="c1"># 3. Get image class from path name (the image class is the name of the directory where the image is stored)</span>
<span class="n">image_class</span> <span class="o">=</span> <span class="n">random_image_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">stem</span>

<span class="c1"># 4. Open image</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">random_image_path</span><span class="p">)</span>

<span class="c1"># 5. Print metadata</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random image path: </span><span class="si">{</span><span class="n">random_image_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image class: </span><span class="si">{</span><span class="n">image_class</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image height: </span><span class="si">{</span><span class="n">img</span><span class="o">.</span><span class="n">height</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image width: </span><span class="si">{</span><span class="n">img</span><span class="o">.</span><span class="n">width</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">img</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random image path: data/pizza_steak_sushi/test/pizza/2124579.jpg
Image class: pizza
Image height: 384
Image width: 512
</pre></div>
</div>
<img alt="../../_images/7f4ded0958d49afa842791189f0d1c47711eee9bc276eaa9f3f261083f01363c.png" src="../../_images/7f4ded0958d49afa842791189f0d1c47711eee9bc276eaa9f3f261083f01363c.png" />
</div>
</div>
<p>We can do the same with <a class="reference external" href="https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.imshow.html"><code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot.imshow()</span></code></a>, except we have to convert the image to a NumPy array first.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Turn the image into an array</span>
<span class="n">img_as_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="c1"># Plot the image with matplotlib</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_as_array</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image class: </span><span class="si">{</span><span class="n">image_class</span><span class="si">}</span><span class="s2"> | Image shape: </span><span class="si">{</span><span class="n">img_as_array</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [height, width, color_channels]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/04f178149af329f979a8292db0a2ee88d289d41683e6f57955223c3874c4db7e.png" src="../../_images/04f178149af329f979a8292db0a2ee88d289d41683e6f57955223c3874c4db7e.png" />
</div>
</div>
</section>
</section>
<section id="transforming-data">
<h2>3. Transforming data<a class="headerlink" href="#transforming-data" title="Permalink to this heading">#</a></h2>
<p>Now what if we wanted to load our image data into PyTorch?</p>
<p>Before we can use our image data with PyTorch we need to:</p>
<ol class="arabic simple">
<li><p>Turn it into tensors (numerical representations of our images).</p></li>
<li><p>Turn it into a <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> and subsequently a <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>, we’ll call these <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> for short.</p></li>
</ol>
<p>There are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem you’re working on.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Problem space</strong></p></th>
<th class="head"><p><strong>Pre-built Datasets and Functions</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Vision</strong></p></td>
<td><p><a class="reference external" href="https://pytorch.org/vision/stable/datasets.html"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Audio</strong></p></td>
<td><p><a class="reference external" href="https://pytorch.org/audio/stable/datasets.html"><code class="docutils literal notranslate"><span class="pre">torchaudio.datasets</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><strong>Text</strong></p></td>
<td><p><a class="reference external" href="https://pytorch.org/text/stable/datasets.html"><code class="docutils literal notranslate"><span class="pre">torchtext.datasets</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Recommendation system</strong></p></td>
<td><p><a class="reference external" href="https://pytorch.org/torchrec/torchrec.datasets.html"><code class="docutils literal notranslate"><span class="pre">torchrec.datasets</span></code></a></p></td>
</tr>
</tbody>
</table>
</div>
<p>Since we’re working with a vision problem, we’ll be looking at <code class="docutils literal notranslate"><span class="pre">torchvision.datasets</span></code> for our data loading functions as well as <a class="reference external" href="https://pytorch.org/vision/stable/transforms.html"><code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code></a> for preparing our data.</p>
<p>Let’s import some base libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
</pre></div>
</div>
</div>
</div>
<section id="transforming-data-with-torchvision-transforms">
<h3>3.1 Transforming data with <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code><a class="headerlink" href="#transforming-data-with-torchvision-transforms" title="Permalink to this heading">#</a></h3>
<p>We’ve got folders of images but before we can use them with PyTorch, we need to convert them into tensors.</p>
<p>One of the ways we can do this is by using the <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> module.</p>
<p><code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for <strong>data augmentation</strong> (the practice of altering data to make it harder for a model to learn, we’ll see this later on) purposes .</p>
<p>To get experience with <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code>, let’s write a series of transform steps that:</p>
<ol class="arabic simple">
<li><p>Resize the images using <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize"><code class="docutils literal notranslate"><span class="pre">transforms.Resize()</span></code></a> (from about 512x512 to 64x64, the same shape as the images on the <a class="reference external" href="https://poloclub.github.io/cnn-explainer/">CNN Explainer website</a>).</p></li>
<li><p>Flip our images randomly on the horizontal using <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip"><code class="docutils literal notranslate"><span class="pre">transforms.RandomHorizontalFlip()</span></code></a> (this could be considered a form of data augmentation because it will artificially change our image data).</p></li>
<li><p>Turn our images from a PIL image to a PyTorch tensor using <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"><code class="docutils literal notranslate"><span class="pre">transforms.ToTensor()</span></code></a>.</p></li>
</ol>
<p>We can compile all of these steps using <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose"><code class="docutils literal notranslate"><span class="pre">torchvision.transforms.Compose()</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write transform for image</span>
<span class="n">data_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="c1"># Resize the images to 64x64</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
    <span class="c1"># Flip the images randomly on the horizontal</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="c1"># p = probability of flip, 0.5 = 50% chance</span>
    <span class="c1"># Turn the image into a torch.Tensor</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span> <span class="c1"># this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 </span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Now we’ve got a composition of transforms, let’s write a function to try them out on various images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_transformed_images</span><span class="p">(</span><span class="n">image_paths</span><span class="p">,</span> <span class="n">transform</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plots a series of random images from image_paths.</span>

<span class="sd">    Will open n image paths from image_paths, transform them</span>
<span class="sd">    with transform and plot them side by side.</span>

<span class="sd">    Args:</span>
<span class="sd">        image_paths (list): List of target image paths. </span>
<span class="sd">        transform (PyTorch Transforms): Transforms to apply to images.</span>
<span class="sd">        n (int, optional): Number of images to plot. Defaults to 3.</span>
<span class="sd">        seed (int, optional): Random seed for the random generator. Defaults to 42.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random_image_paths</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">image_paths</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">image_path</span> <span class="ow">in</span> <span class="n">random_image_paths</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> 
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original </span><span class="se">\n</span><span class="s2">Size: </span><span class="si">{</span><span class="n">f</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

            <span class="c1"># Transform and plot image</span>
            <span class="c1"># Note: permute() will change shape of image to suit matplotlib </span>
            <span class="c1"># (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])</span>
            <span class="n">transformed_image</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> 
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">transformed_image</span><span class="p">)</span> 
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformed </span><span class="se">\n</span><span class="s2">Size: </span><span class="si">{</span><span class="n">transformed_image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>

            <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Class: </span><span class="si">{</span><span class="n">image_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">stem</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plot_transformed_images</span><span class="p">(</span><span class="n">image_path_list</span><span class="p">,</span> 
                        <span class="n">transform</span><span class="o">=</span><span class="n">data_transform</span><span class="p">,</span> 
                        <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/1048268bfca4d73e0c8be559dbfc828806c87ca0e9ff061ac382018365d38852.png" src="../../_images/1048268bfca4d73e0c8be559dbfc828806c87ca0e9ff061ac382018365d38852.png" />
<img alt="../../_images/828ca570b2ad7f4ea8b0d1906c78a29bef05799256cd332db387290d6990a3f6.png" src="../../_images/828ca570b2ad7f4ea8b0d1906c78a29bef05799256cd332db387290d6990a3f6.png" />
<img alt="../../_images/27a39e26181164ff4adcb094dca2589b39056d5085f5b9edfccb2f36217c0f2b.png" src="../../_images/27a39e26181164ff4adcb094dca2589b39056d5085f5b9edfccb2f36217c0f2b.png" />
</div>
</div>
<p>Nice!</p>
<p>We’ve now got a way to convert our images to tensors using <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code>.</p>
<p>We also manipulate their size and orientation if needed (some models prefer images of different sizes and shapes).</p>
<p>Generally, the larger the shape of the image, the more information a model can recover.</p>
<p>For example, an image of size <code class="docutils literal notranslate"><span class="pre">[256,</span> <span class="pre">256,</span> <span class="pre">3]</span></code> will have 16x more pixels than an image of size <code class="docutils literal notranslate"><span class="pre">[64,</span> <span class="pre">64,</span> <span class="pre">3]</span></code> (<code class="docutils literal notranslate"><span class="pre">(256*256*3)/(64*64*3)=16</span></code>).</p>
<p>However, the tradeoff is that more pixels requires more computations.</p>
<blockquote>
<div><p><strong>Exercise:</strong> Try commenting out one of the transforms in <code class="docutils literal notranslate"><span class="pre">data_transform</span></code> and running the plotting function <code class="docutils literal notranslate"><span class="pre">plot_transformed_images()</span></code> again, what happens?</p>
</div></blockquote>
</section>
</section>
<section id="option-1-loading-image-data-using-imagefolder">
<h2>4. Option 1: Loading Image Data Using <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder"><code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code></a><a class="headerlink" href="#option-1-loading-image-data-using-imagefolder" title="Permalink to this heading">#</a></h2>
<p>Alright, time to turn our image data into a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> capable of being used with PyTorch.</p>
<p>Since our data is in standard image classification format, we can use the class <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder</span></code></a>.</p>
<p>Where we can pass it the file path of a target image directory as well as a series of transforms we’d like to perform on our images.</p>
<p>Let’s test it out on our data folders <code class="docutils literal notranslate"><span class="pre">train_dir</span></code> and <code class="docutils literal notranslate"><span class="pre">test_dir</span></code> passing in <code class="docutils literal notranslate"><span class="pre">transform=data_transform</span></code> to turn our images into tensors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use ImageFolder to create dataset(s)</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">train_dir</span><span class="p">,</span> <span class="c1"># target folder of images</span>
                                  <span class="n">transform</span><span class="o">=</span><span class="n">data_transform</span><span class="p">,</span> <span class="c1"># transforms to perform on data (images)</span>
                                  <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># transforms to perform on labels (if necessary)</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span> 
                                 <span class="n">transform</span><span class="o">=</span><span class="n">data_transform</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Train data:</span><span class="se">\n</span><span class="si">{</span><span class="n">train_data</span><span class="si">}</span><span class="se">\n</span><span class="s2">Test data:</span><span class="se">\n</span><span class="si">{</span><span class="n">test_data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train data:
Dataset ImageFolder
    Number of datapoints: 225
    Root location: data/pizza_steak_sushi/train
    StandardTransform
Transform: Compose(
               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
           )
Test data:
Dataset ImageFolder
    Number of datapoints: 75
    Root location: data/pizza_steak_sushi/test
    StandardTransform
Transform: Compose(
               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
           )
</pre></div>
</div>
</div>
</div>
<p>Beautiful!</p>
<p>It looks like PyTorch has registered our <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s.</p>
<p>Let’s inspect them by checking out the <code class="docutils literal notranslate"><span class="pre">classes</span></code> and <code class="docutils literal notranslate"><span class="pre">class_to_idx</span></code> attributes as well as the lengths of our training and test sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get class names as a list</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">classes</span>
<span class="n">class_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Can also get class names as a dict</span>
<span class="n">class_dict</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">class_to_idx</span>
<span class="n">class_dict</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;pizza&#39;: 0, &#39;steak&#39;: 1, &#39;sushi&#39;: 2}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the lengths</span>
<span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(225, 75)
</pre></div>
</div>
</div>
</div>
<p>Nice! Looks like we’ll be able to use these to reference for later.</p>
<p>How about our images and labels?</p>
<p>How do they look?</p>
<p>We can index on our <code class="docutils literal notranslate"><span class="pre">train_data</span></code> and <code class="docutils literal notranslate"><span class="pre">test_data</span></code> <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s to find samples and their target labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">img</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image shape: </span><span class="si">{</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image datatype: </span><span class="si">{</span><span class="n">img</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image label: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Label datatype: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">label</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image tensor:
tensor([[[0.1137, 0.1020, 0.0980,  ..., 0.1255, 0.1216, 0.1176],
         [0.1059, 0.0980, 0.0980,  ..., 0.1294, 0.1294, 0.1294],
         [0.1020, 0.0980, 0.0941,  ..., 0.1333, 0.1333, 0.1333],
         ...,
         [0.1098, 0.1098, 0.1255,  ..., 0.1686, 0.1647, 0.1686],
         [0.0863, 0.0941, 0.1098,  ..., 0.1686, 0.1647, 0.1686],
         [0.0863, 0.0863, 0.0980,  ..., 0.1686, 0.1647, 0.1647]],

        [[0.0745, 0.0706, 0.0745,  ..., 0.0588, 0.0588, 0.0588],
         [0.0706, 0.0706, 0.0745,  ..., 0.0627, 0.0627, 0.0627],
         [0.0706, 0.0745, 0.0745,  ..., 0.0706, 0.0706, 0.0706],
         ...,
         [0.1255, 0.1333, 0.1373,  ..., 0.2510, 0.2392, 0.2392],
         [0.1098, 0.1176, 0.1255,  ..., 0.2510, 0.2392, 0.2314],
         [0.1020, 0.1059, 0.1137,  ..., 0.2431, 0.2353, 0.2275]],

        [[0.0941, 0.0902, 0.0902,  ..., 0.0196, 0.0196, 0.0196],
         [0.0902, 0.0863, 0.0902,  ..., 0.0196, 0.0157, 0.0196],
         [0.0902, 0.0902, 0.0902,  ..., 0.0157, 0.0157, 0.0196],
         ...,
         [0.1294, 0.1333, 0.1490,  ..., 0.1961, 0.1882, 0.1804],
         [0.1098, 0.1137, 0.1255,  ..., 0.1922, 0.1843, 0.1804],
         [0.1059, 0.1020, 0.1059,  ..., 0.1843, 0.1804, 0.1765]]])
Image shape: torch.Size([3, 64, 64])
Image datatype: torch.float32
Image label: 0
Label datatype: &lt;class &#39;int&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>Our images are now in the form of a tensor (with shape <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">64,</span> <span class="pre">64]</span></code>) and the labels are in the form of an integer relating to a specific class (as referenced by the <code class="docutils literal notranslate"><span class="pre">class_to_idx</span></code> attribute).</p>
<p>How about we plot a single image tensor using <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>?</p>
<p>We’ll first have to to permute (rearrange the order of its dimensions) so it’s compatible.</p>
<p>Right now our image dimensions are in the format <code class="docutils literal notranslate"><span class="pre">CHW</span></code> (color channels, height, width) but <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> prefers <code class="docutils literal notranslate"><span class="pre">HWC</span></code> (height, width, color channels).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Rearrange the order of dimensions</span>
<span class="n">img_permute</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Print out different shapes (before and after permute)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original shape: </span><span class="si">{</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [color_channels, height, width]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image permute shape: </span><span class="si">{</span><span class="n">img_permute</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [height, width, color_channels]&quot;</span><span class="p">)</span>

<span class="c1"># Plot the image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]
Image permute shape: torch.Size([64, 64, 3]) -&gt; [height, width, color_channels]
</pre></div>
</div>
<img alt="../../_images/5fb97bd81817b7cf7915638d3bfdbf654c4fbf3d9b6efcc8090f3bc1d1822b57.png" src="../../_images/5fb97bd81817b7cf7915638d3bfdbf654c4fbf3d9b6efcc8090f3bc1d1822b57.png" />
</div>
</div>
<p>Notice the image is now more pixelated (less quality).</p>
<p>This is due to it being resized from <code class="docutils literal notranslate"><span class="pre">512x512</span></code> to <code class="docutils literal notranslate"><span class="pre">64x64</span></code> pixels.</p>
<p>The intuition here is that if you think the image is harder to recognize what’s going on, chances are a model will find it harder to understand too.</p>
<section id="turn-loaded-images-into-dataloaders">
<h3>4.1 Turn loaded images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s<a class="headerlink" href="#turn-loaded-images-into-dataloaders" title="Permalink to this heading">#</a></h3>
<p>We’ve got our images as PyTorch <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s but now let’s turn them into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s.</p>
<p>We’ll do so using <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a>.</p>
<p>Turning our <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s makes them iterable so a model can go through learn the relationships between samples and targets (features and labels).</p>
<p>To keep things simple, we’ll use a <code class="docutils literal notranslate"><span class="pre">batch_size=1</span></code> and <code class="docutils literal notranslate"><span class="pre">num_workers=1</span></code>.</p>
<p>What’s <code class="docutils literal notranslate"><span class="pre">num_workers</span></code>?</p>
<p>Good question.</p>
<p>It defines how many subprocesses will be created to load your data.</p>
<p>Think of it like this, the higher value <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> is set to, the more compute power PyTorch will use to load your data.</p>
<p>Personally, I usually set it to the total number of CPUs on my machine via Python’s <a class="reference external" href="https://docs.python.org/3/library/os.html#os.cpu_count"><code class="docutils literal notranslate"><span class="pre">os.cpu_count()</span></code></a>.</p>
<p>This ensures the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> recruits as many cores as possible to load data.</p>
<blockquote>
<div><p><strong>Note:</strong> There are more parameters you can get familiar with using <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> in the <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">PyTorch documentation</a>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn train and test Datasets into DataLoaders</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span> 
                              <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># how many samples per batch?</span>
                              <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># how many subprocesses to use for data loading? (higher = more)</span>
                              <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># shuffle the data?</span>

<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span> 
                             <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                             <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                             <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># don&#39;t usually need to shuffle testing data</span>

<span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9dca0&gt;,
 &lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9de50&gt;)
</pre></div>
</div>
</div>
</div>
<p>Wonderful!</p>
<p>Now our data is iterable.</p>
<p>Let’s try it out and check the shapes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>

<span class="c1"># Batch size will now be 1, try changing the batch_size parameter above and see what happens</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image shape: </span><span class="si">{</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, color_channels, height, width]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Label shape: </span><span class="si">{</span><span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]
Label shape: torch.Size([1])
</pre></div>
</div>
</div>
</div>
<p>We could now use these <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s with a training and testing loop to train a model.</p>
<p>But before we do, let’s look at another option to load images (or almost any other kind of data).</p>
</section>
</section>
<section id="option-2-loading-image-data-with-a-custom-dataset">
<h2>5. Option 2: Loading Image Data with a Custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code><a class="headerlink" href="#option-2-loading-image-data-with-a-custom-dataset" title="Permalink to this heading">#</a></h2>
<p>What if a pre-built <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> creator like <a class="reference external" href="https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.ImageFolder"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder()</span></code></a> didn’t exist?</p>
<p>Or one for your specific problem didn’t exist?</p>
<p>Well, you could build your own.</p>
<p>But wait, what are the pros and cons of creating your own custom way to load <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s?</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Pros of creating a custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code></p></th>
<th class="head"><p>Cons of creating a custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Can create a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> out of almost anything.</p></td>
<td><p>Even though you <em>could</em> create a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> out of almost anything, it doesn’t mean it will work.</p></td>
</tr>
<tr class="row-odd"><td><p>Not limited to PyTorch pre-built <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> functions.</p></td>
<td><p>Using a custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> often results in writing more code, which could be prone to errors or performance issues.</p></td>
</tr>
</tbody>
</table>
</div>
<p>To see this in action, let’s work towards replicating <code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder()</span></code> by subclassing <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> (the base class for all <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s in PyTorch).</p>
<p>We’ll start by importing the modules we need:</p>
<ul class="simple">
<li><p>Python’s <code class="docutils literal notranslate"><span class="pre">os</span></code> for dealing with directories (our data is stored in directories).</p></li>
<li><p>Python’s <code class="docutils literal notranslate"><span class="pre">pathlib</span></code> for dealing with filepaths (each of our images has a unique filepath).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch</span></code> for all things PyTorch.</p></li>
<li><p>PIL’s <code class="docutils literal notranslate"><span class="pre">Image</span></code> class for loading images.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> to subclass and create our own custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> to turn our images into tensors.</p></li>
<li><p>Various types from Python’s <code class="docutils literal notranslate"><span class="pre">typing</span></code> module to add type hints to our code.</p></li>
</ul>
<blockquote>
<div><p><strong>Note:</strong> You can customize the following steps for your own dataset. The premise remains: write code to load your data in the format you’d like it.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>
</pre></div>
</div>
</div>
</div>
<p>Remember how our instances of <code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder()</span></code> allowed us to use the <code class="docutils literal notranslate"><span class="pre">classes</span></code> and <code class="docutils literal notranslate"><span class="pre">class_to_idx</span></code> attributes?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instance of torchvision.datasets.ImageFolder()</span>
<span class="n">train_data</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">train_data</span><span class="o">.</span><span class="n">class_to_idx</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>([&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;], {&#39;pizza&#39;: 0, &#39;steak&#39;: 1, &#39;sushi&#39;: 2})
</pre></div>
</div>
</div>
</div>
<section id="creating-a-helper-function-to-get-class-names">
<h3>5.1 Creating a helper function to get class names<a class="headerlink" href="#creating-a-helper-function-to-get-class-names" title="Permalink to this heading">#</a></h3>
<p>Let’s write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.</p>
<p>To do so, we’ll:</p>
<ol class="arabic simple">
<li><p>Get the class names using <code class="docutils literal notranslate"><span class="pre">os.scandir()</span></code> to traverse a target directory (ideally the directory is in standard image classification format).</p></li>
<li><p>Raise an error if the class names aren’t found (if this happens, there might be something wrong with the directory structure).</p></li>
<li><p>Turn the class names into a dictionary of numerical labels, one for each class.</p></li>
</ol>
<p>Let’s see a small example of step 1 before we write the full function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup path for target directory</span>
<span class="n">target_directory</span> <span class="o">=</span> <span class="n">train_dir</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target directory: </span><span class="si">{</span><span class="n">target_directory</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Get the class names from the target directory</span>
<span class="n">class_names_found</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">entry</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">scandir</span><span class="p">(</span><span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;train&quot;</span><span class="p">))])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Class names found: </span><span class="si">{</span><span class="n">class_names_found</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Target directory: data/pizza_steak_sushi/train
Class names found: [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;]
</pre></div>
</div>
</div>
</div>
<p>Excellent!</p>
<p>How about we turn it into a full function?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make function to find classes in target directory</span>
<span class="k">def</span> <span class="nf">find_classes</span><span class="p">(</span><span class="n">directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Finds the class folder names in a target directory.</span>
<span class="sd">    </span>
<span class="sd">    Assumes target directory is in standard image classification format.</span>

<span class="sd">    Args:</span>
<span class="sd">        directory (str): target directory to load classnames from.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))</span>
<span class="sd">    </span>
<span class="sd">    Example:</span>
<span class="sd">        find_classes(&quot;food_images/train&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ([&quot;class_1&quot;, &quot;class_2&quot;], {&quot;class_1&quot;: 0, ...})</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 1. Get the class names by scanning the target directory</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">entry</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">scandir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span> <span class="k">if</span> <span class="n">entry</span><span class="o">.</span><span class="n">is_dir</span><span class="p">())</span>
    
    <span class="c1"># 2. Raise an error if class names not found</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">classes</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Couldn&#39;t find any classes in </span><span class="si">{</span><span class="n">directory</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        
    <span class="c1"># 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)</span>
    <span class="n">class_to_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">cls_name</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cls_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">classes</span><span class="p">,</span> <span class="n">class_to_idx</span>
</pre></div>
</div>
</div>
</div>
<p>Looking good!</p>
<p>Now let’s test out our <code class="docutils literal notranslate"><span class="pre">find_classes()</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">find_classes</span><span class="p">(</span><span class="n">train_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>([&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;], {&#39;pizza&#39;: 0, &#39;steak&#39;: 1, &#39;sushi&#39;: 2})
</pre></div>
</div>
</div>
</div>
<p>Woohoo! Looking good!</p>
</section>
<section id="create-a-custom-dataset-to-replicate-imagefolder">
<h3>5.2 Create a custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> to replicate <code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code><a class="headerlink" href="#create-a-custom-dataset-to-replicate-imagefolder" title="Permalink to this heading">#</a></h3>
<p>Now we’re ready to build our own custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>.</p>
<p>We’ll build one to replicate the functionality of <code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder()</span></code>.</p>
<p>This will be good practice, plus, it’ll reveal a few of the required steps to make your own custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>.</p>
<p>It’ll be a fair bit of a code… but nothing we can’t handle!</p>
<p>Let’s break it down:</p>
<ol class="arabic simple">
<li><p>Subclass <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>.</p></li>
<li><p>Initialize our subclass with a <code class="docutils literal notranslate"><span class="pre">targ_dir</span></code> parameter (the target data directory) and <code class="docutils literal notranslate"><span class="pre">transform</span></code> parameter (so we have the option to transform our data if needed).</p></li>
<li><p>Create several attributes for <code class="docutils literal notranslate"><span class="pre">paths</span></code> (the paths of our target images), <code class="docutils literal notranslate"><span class="pre">transform</span></code> (the transforms we might like to use, this can be <code class="docutils literal notranslate"><span class="pre">None</span></code>), <code class="docutils literal notranslate"><span class="pre">classes</span></code> and <code class="docutils literal notranslate"><span class="pre">class_to_idx</span></code> (from our <code class="docutils literal notranslate"><span class="pre">find_classes()</span></code> function).</p></li>
<li><p>Create a function to load images from file and return them, this could be using <code class="docutils literal notranslate"><span class="pre">PIL</span></code> or <a class="reference external" href="https://pytorch.org/vision/stable/io.html#image"><code class="docutils literal notranslate"><span class="pre">torchvision.io</span></code></a> (for input/output of vision data).</p></li>
<li><p>Overwrite the <code class="docutils literal notranslate"><span class="pre">__len__</span></code> method of <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> to return the number of samples in the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, this is recommended but not required. This is so you can call <code class="docutils literal notranslate"><span class="pre">len(Dataset)</span></code>.</p></li>
<li><p>Overwrite the <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> method of <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> to return a single sample from the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, this is required.</p></li>
</ol>
<p>Let’s do it!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write a custom dataset class (inherits from torch.utils.data.Dataset)</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="c1"># 1. Subclass torch.utils.data.Dataset</span>
<span class="k">class</span> <span class="nc">ImageFolderCustom</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    
    <span class="c1"># 2. Initialize with a targ_dir and transform (optional) parameter</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">targ_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        
        <span class="c1"># 3. Create class attributes</span>
        <span class="c1"># Get all image paths</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">targ_dir</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*/*.jpg&quot;</span><span class="p">))</span> <span class="c1"># note: you&#39;d have to update this if you&#39;ve got .png&#39;s or .jpeg&#39;s</span>
        <span class="c1"># Setup transforms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>
        <span class="c1"># Create classes and class_to_idx attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_to_idx</span> <span class="o">=</span> <span class="n">find_classes</span><span class="p">(</span><span class="n">targ_dir</span><span class="p">)</span>

    <span class="c1"># 4. Make function to load images</span>
    <span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Image</span><span class="o">.</span><span class="n">Image</span><span class="p">:</span>
        <span class="s2">&quot;Opens an image via a path and returns it.&quot;</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span> 
    
    <span class="c1"># 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)</span>
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="s2">&quot;Returns the total number of samples.&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="p">)</span>
    
    <span class="c1"># 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="s2">&quot;Returns one sample of data, data and label (X, y).&quot;</span>
        <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_image</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="n">class_name</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">name</span> <span class="c1"># expects path in data_folder/class_name/image.jpeg</span>
        <span class="n">class_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_to_idx</span><span class="p">[</span><span class="n">class_name</span><span class="p">]</span>

        <span class="c1"># Transform if necessary</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="n">class_idx</span> <span class="c1"># return data, label (X, y)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">class_idx</span> <span class="c1"># return data, label (X, y)</span>
</pre></div>
</div>
</div>
</div>
<p>Woah! A whole bunch of code to load in our images.</p>
<p>This is one of the downsides of creating your own custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s.</p>
<p>However, now we’ve written it once, we could move it into a <code class="docutils literal notranslate"><span class="pre">.py</span></code> file such as <code class="docutils literal notranslate"><span class="pre">data_loader.py</span></code> along with some other helpful data functions and reuse it later on.</p>
<p>Before we test out our new <code class="docutils literal notranslate"><span class="pre">ImageFolderCustom</span></code> class, let’s create some transforms to prepare our images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Augment train data</span>
<span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">])</span>

<span class="c1"># Don&#39;t augment test data, only reshape</span>
<span class="n">test_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Now comes the moment of truth!</p>
<p>Let’s turn our training images (contained in <code class="docutils literal notranslate"><span class="pre">train_dir</span></code>) and our testing images (contained in <code class="docutils literal notranslate"><span class="pre">test_dir</span></code>) into <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s using our own <code class="docutils literal notranslate"><span class="pre">ImageFolderCustom</span></code> class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data_custom</span> <span class="o">=</span> <span class="n">ImageFolderCustom</span><span class="p">(</span><span class="n">targ_dir</span><span class="o">=</span><span class="n">train_dir</span><span class="p">,</span> 
                                      <span class="n">transform</span><span class="o">=</span><span class="n">train_transforms</span><span class="p">)</span>
<span class="n">test_data_custom</span> <span class="o">=</span> <span class="n">ImageFolderCustom</span><span class="p">(</span><span class="n">targ_dir</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span> 
                                     <span class="n">transform</span><span class="o">=</span><span class="n">test_transforms</span><span class="p">)</span>
<span class="n">train_data_custom</span><span class="p">,</span> <span class="n">test_data_custom</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;__main__.ImageFolderCustom at 0x7f5461f70c70&gt;,
 &lt;__main__.ImageFolderCustom at 0x7f5461f70c40&gt;)
</pre></div>
</div>
</div>
</div>
<p>Hmm… no errors, did it work?</p>
<p>Let’s try calling <code class="docutils literal notranslate"><span class="pre">len()</span></code> on our new <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s and find the <code class="docutils literal notranslate"><span class="pre">classes</span></code> and <code class="docutils literal notranslate"><span class="pre">class_to_idx</span></code> attributes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">train_data_custom</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data_custom</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(225, 75)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data_custom</span><span class="o">.</span><span class="n">classes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data_custom</span><span class="o">.</span><span class="n">class_to_idx</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;pizza&#39;: 0, &#39;steak&#39;: 1, &#39;sushi&#39;: 2}
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">len(test_data_custom)</span> <span class="pre">==</span> <span class="pre">len(test_data)</span></code> and <code class="docutils literal notranslate"><span class="pre">len(test_data_custom)</span> <span class="pre">==</span> <span class="pre">len(test_data)</span></code> Yes!!!</p>
<p>It looks like it worked.</p>
<p>We could check for equality with the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s made by the <code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder()</span></code> class too.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check for equality amongst our custom Dataset and ImageFolder Dataset</span>
<span class="nb">print</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data_custom</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data_custom</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_data_custom</span><span class="o">.</span><span class="n">classes</span> <span class="o">==</span> <span class="n">train_data</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_data_custom</span><span class="o">.</span><span class="n">class_to_idx</span> <span class="o">==</span> <span class="n">train_data</span><span class="o">.</span><span class="n">class_to_idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
True
True
</pre></div>
</div>
</div>
</div>
<p>Ho ho!</p>
<p>Look at us go!</p>
<p>Three <code class="docutils literal notranslate"><span class="pre">True</span></code>’s!</p>
<p>You can’t get much better than that.</p>
<p>How about we take it up a notch and plot some random images to test our <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> override?</p>
</section>
<section id="create-a-function-to-display-random-images">
<h3>5.3 Create a function to display random images<a class="headerlink" href="#create-a-function-to-display-random-images" title="Permalink to this heading">#</a></h3>
<p>You know what time it is!</p>
<p>Time to put on our data explorer’s hat and <em>visualize, visualize, visualize!</em></p>
<p>Let’s create a helper function called <code class="docutils literal notranslate"><span class="pre">display_random_images()</span></code> that helps us visualize images in our <code class="docutils literal notranslate"><span class="pre">Dataset'</span></code>s.</p>
<p>Specifically, it’ll:</p>
<ol class="arabic simple">
<li><p>Take in a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and a number of other parameters such as <code class="docutils literal notranslate"><span class="pre">classes</span></code> (the names of our target classes), the number of images to display (<code class="docutils literal notranslate"><span class="pre">n</span></code>) and a random seed.</p></li>
<li><p>To prevent the display getting out of hand, we’ll cap <code class="docutils literal notranslate"><span class="pre">n</span></code> at 10 images.</p></li>
<li><p>Set the random seed for reproducible plots (if <code class="docutils literal notranslate"><span class="pre">seed</span></code> is set).</p></li>
<li><p>Get a list of random sample indexes (we can use Python’s <code class="docutils literal notranslate"><span class="pre">random.sample()</span></code> for this) to plot.</p></li>
<li><p>Setup a <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> plot.</p></li>
<li><p>Loop through the random sample indexes found in step 4 and plot them with <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>.</p></li>
<li><p>Make sure the sample images are of shape <code class="docutils literal notranslate"><span class="pre">HWC</span></code> (height, width, color channels) so we can plot them.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Take in a Dataset as well as a list of class names</span>
<span class="k">def</span> <span class="nf">display_random_images</span><span class="p">(</span><span class="n">dataset</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">Dataset</span><span class="p">,</span>
                          <span class="n">classes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                          <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                          <span class="n">display_shape</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                          <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    
    <span class="c1"># 2. Adjust display if n too high</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="n">display_shape</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For display purposes, n shouldn&#39;t be larger than 10, setting to 10 and removing shape display.&quot;</span><span class="p">)</span>
    
    <span class="c1"># 3. Set random seed</span>
    <span class="k">if</span> <span class="n">seed</span><span class="p">:</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># 4. Get random sample indexes</span>
    <span class="n">random_samples_idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)),</span> <span class="n">k</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

    <span class="c1"># 5. Setup plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="c1"># 6. Loop through samples and display random samples </span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">targ_sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">random_samples_idx</span><span class="p">):</span>
        <span class="n">targ_image</span><span class="p">,</span> <span class="n">targ_label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">targ_sample</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="n">targ_sample</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]</span>
        <span class="n">targ_image_adjust</span> <span class="o">=</span> <span class="n">targ_image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Plot adjusted samples</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">targ_image_adjust</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">classes</span><span class="p">:</span>
            <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;class: </span><span class="si">{</span><span class="n">classes</span><span class="p">[</span><span class="n">targ_label</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">if</span> <span class="n">display_shape</span><span class="p">:</span>
                <span class="n">title</span> <span class="o">=</span> <span class="n">title</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">shape: </span><span class="si">{</span><span class="n">targ_image_adjust</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>What a good looking function!</p>
<p>Let’s test it out first with the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> we created with <code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display random images from ImageFolder created Dataset</span>
<span class="n">display_random_images</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> 
                      <span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                      <span class="n">classes</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span>
                      <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/09dc92f0d58b25ee61f5f98d68297bab6f8d743afc5e2154e2f7b7f6723424a8.png" src="../../_images/09dc92f0d58b25ee61f5f98d68297bab6f8d743afc5e2154e2f7b7f6723424a8.png" />
</div>
</div>
<p>And now with the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> we created with our own <code class="docutils literal notranslate"><span class="pre">ImageFolderCustom</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display random images from ImageFolderCustom Dataset</span>
<span class="n">display_random_images</span><span class="p">(</span><span class="n">train_data_custom</span><span class="p">,</span> 
                      <span class="n">n</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> 
                      <span class="n">classes</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span>
                      <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># Try setting the seed for reproducible images</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>For display purposes, n shouldn&#39;t be larger than 10, setting to 10 and removing shape display.
</pre></div>
</div>
<img alt="../../_images/cfa8b1e14677ab3f300f56be614b3bcb1dfe38ff122eed1d2230dfcd2d89bb6a.png" src="../../_images/cfa8b1e14677ab3f300f56be614b3bcb1dfe38ff122eed1d2230dfcd2d89bb6a.png" />
</div>
</div>
<p>Nice!!!</p>
<p>Looks like our <code class="docutils literal notranslate"><span class="pre">ImageFolderCustom</span></code> is working just as we’d like it to.</p>
</section>
<section id="turn-custom-loaded-images-into-dataloaders">
<h3>5.4 Turn custom loaded images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s<a class="headerlink" href="#turn-custom-loaded-images-into-dataloaders" title="Permalink to this heading">#</a></h3>
<p>We’ve got a way to turn our raw images into <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s (features mapped to labels or <code class="docutils literal notranslate"><span class="pre">X</span></code>’s mapped to <code class="docutils literal notranslate"><span class="pre">y</span></code>’s) through our <code class="docutils literal notranslate"><span class="pre">ImageFolderCustom</span></code> class.</p>
<p>Now how could we turn our custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s?</p>
<p>If you guessed by using <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader()</span></code>, you’d be right!</p>
<p>Because our custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s subclass <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>, we can use them directly with <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader()</span></code>.</p>
<p>And we can do using very similar steps to before except this time we’ll be using our custom created <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn train and test custom Dataset&#39;s into DataLoader&#39;s</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">train_dataloader_custom</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_data_custom</span><span class="p">,</span> <span class="c1"># use custom created train Dataset</span>
                                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># how many samples per batch?</span>
                                     <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># how many subprocesses to use for data loading? (higher = more)</span>
                                     <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># shuffle the data?</span>

<span class="n">test_dataloader_custom</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_data_custom</span><span class="p">,</span> <span class="c1"># use custom created test Dataset</span>
                                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># don&#39;t usually need to shuffle testing data</span>

<span class="n">train_dataloader_custom</span><span class="p">,</span> <span class="n">test_dataloader_custom</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ab8400&gt;,
 &lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ab8490&gt;)
</pre></div>
</div>
</div>
</div>
<p>Do the shapes of the samples look the same?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get image and label from custom DataLoader</span>
<span class="n">img_custom</span><span class="p">,</span> <span class="n">label_custom</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader_custom</span><span class="p">))</span>

<span class="c1"># Batch size will now be 1, try changing the batch_size parameter above and see what happens</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image shape: </span><span class="si">{</span><span class="n">img_custom</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, color_channels, height, width]&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Label shape: </span><span class="si">{</span><span class="n">label_custom</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]
Label shape: torch.Size([1])
</pre></div>
</div>
</div>
</div>
<p>They sure do!</p>
<p>Let’s now take a lot at some other forms of data transforms.</p>
</section>
</section>
<section id="other-forms-of-transforms-data-augmentation">
<h2>6. Other forms of transforms (data augmentation)<a class="headerlink" href="#other-forms-of-transforms-data-augmentation" title="Permalink to this heading">#</a></h2>
<p>We’ve seen a couple of transforms on our data already but there’s plenty more.</p>
<p>You can see them all in the <a class="reference external" href="https://pytorch.org/vision/stable/transforms.html"><code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> documentation</a>.</p>
<p>The purpose of tranforms is to alter your images in some way.</p>
<p>That may be turning your images into a tensor (as we’ve seen before).</p>
<p>Or cropping it or randomly erasing a portion or randomly rotating them.</p>
<p>Doing this kinds of transforms is often referred to as <strong>data augmentation</strong>.</p>
<p><strong>Data augmentation</strong> is the process of altering your data in such a way that you <em>artificially</em> increase the diversity of your training set.</p>
<p>Training a model on this <em>artificially</em> altered dataset hopefully results in a model that is capable of better <em>generalization</em> (the patterns it learns are more robust to future unseen examples).</p>
<p>You can see many different examples of data augmentation performed on images using <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> in PyTorch’s <a class="reference external" href="https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#illustration-of-transforms">Illustration of Transforms example</a>.</p>
<p>But let’s try one out ourselves.</p>
<p>Machine learning is all about harnessing the power of randomness and research shows that random transforms (like <a class="reference external" href="https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#randaugment"><code class="docutils literal notranslate"><span class="pre">transforms.RandAugment()</span></code></a> and <a class="reference external" href="https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#trivialaugmentwide"><code class="docutils literal notranslate"><span class="pre">transforms.TrivialAugmentWide()</span></code></a>) generally perform better than hand-picked transforms.</p>
<p>The idea behind <a class="reference external" href="https://arxiv.org/abs/2103.10158">TrivialAugment</a> is… well, trivial.</p>
<p>You have a set of transforms and you randomly pick a number of them to perform on an image and at a random magnitude between a given range (a higher magnitude means more instense).</p>
<p>The PyTorch team even <a class="reference external" href="https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#break-down-of-key-accuracy-improvements">used TrivialAugment it to train their latest state-of-the-art vision models</a>.</p>
<p><img alt="trivial augment data augmentation being used for PyTorch state of the art training" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-trivial-augment-being-using-in-PyTorch-resize.png" /></p>
<p><em>TrivialAugment was one of the ingredients used in a recent state of the art training upgrade to various PyTorch vision models.</em></p>
<p>How about we test it out on some of our own images?</p>
<p>The main parameter to pay attention to in <code class="docutils literal notranslate"><span class="pre">transforms.TrivialAugmentWide()</span></code> is <code class="docutils literal notranslate"><span class="pre">num_magnitude_bins=31</span></code>.</p>
<p>It defines how much of a range an intensity value will be picked to apply a certain transform, <code class="docutils literal notranslate"><span class="pre">0</span></code> being no range and <code class="docutils literal notranslate"><span class="pre">31</span></code> being maximum range (highest chance for highest intensity).</p>
<p>We can incorporate <code class="docutils literal notranslate"><span class="pre">transforms.TrivialAugmentWide()</span></code> into <code class="docutils literal notranslate"><span class="pre">transforms.Compose()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">TrivialAugmentWide</span><span class="p">(</span><span class="n">num_magnitude_bins</span><span class="o">=</span><span class="mi">31</span><span class="p">),</span> <span class="c1"># how intense </span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span> <span class="c1"># use ToTensor() last to get everything between 0 &amp; 1</span>
<span class="p">])</span>

<span class="c1"># Don&#39;t need to perform augmentation on the test data</span>
<span class="n">test_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span> 
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> You usually don’t perform data augmentation on the test set. The idea of data augmentation is to to <em>artificially</em> increase the diversity of the training set to better predict on the testing set.</p>
<p>However, you do need to make sure your test set images are transformed to tensors. We size the test images to the same size as our training images too, however, inference can be done on different size images if necessary (though this may alter performance).</p>
</div></blockquote>
<p>Beautiful, now we’ve got a training transform (with data augmentation) and test transform (without data augmentation).</p>
<p>Let’s test our data augmentation out!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get all image paths</span>
<span class="n">image_path_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">image_path</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*/*/*.jpg&quot;</span><span class="p">))</span>

<span class="c1"># Plot random images</span>
<span class="n">plot_transformed_images</span><span class="p">(</span>
    <span class="n">image_paths</span><span class="o">=</span><span class="n">image_path_list</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">train_transforms</span><span class="p">,</span>
    <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/831849a44b5d9595afa6f68b521f44aee231c045bf1463e21d48633cc59a571e.png" src="../../_images/831849a44b5d9595afa6f68b521f44aee231c045bf1463e21d48633cc59a571e.png" />
<img alt="../../_images/5a20dd46a3515d7cf53eaab09610c2a751f7c683192bcfb702e5509132b3347c.png" src="../../_images/5a20dd46a3515d7cf53eaab09610c2a751f7c683192bcfb702e5509132b3347c.png" />
<img alt="../../_images/e187e58010e37695da56dc2229e9d4a3c3233239d198861b2cf6b08dc1b1a487.png" src="../../_images/e187e58010e37695da56dc2229e9d4a3c3233239d198861b2cf6b08dc1b1a487.png" />
</div>
</div>
<p>Try running the cell above a few times and seeing how the original image changes as it goes through the transform.</p>
</section>
<section id="model-0-tinyvgg-without-data-augmentation">
<h2>7. Model 0: TinyVGG without data augmentation<a class="headerlink" href="#model-0-tinyvgg-without-data-augmentation" title="Permalink to this heading">#</a></h2>
<p>Alright, we’ve seen how to turn our data from images in folders to transformed tensors.</p>
<p>Now let’s construct a computer vision model to see if we can classify if an image is of pizza, steak or sushi.</p>
<p>To begin, we’ll start with a simple transform, only resizing the images to <code class="docutils literal notranslate"><span class="pre">(64,</span> <span class="pre">64)</span></code> and turning them into tensors.</p>
<section id="creating-transforms-and-loading-data-for-model-0">
<h3>7.1 Creating transforms and loading data for Model 0<a class="headerlink" href="#creating-transforms-and-loading-data-for-model-0" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create simple transform</span>
<span class="n">simple_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span> 
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Excellent, now we’ve got a simple transform, let’s:</p>
<ol class="arabic simple">
<li><p>Load the data, turning each of our training and test folders first into a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with <code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder()</span></code></p></li>
<li><p>Then into a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> using <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader()</span></code>.</p>
<ul class="simple">
<li><p>We’ll set the <code class="docutils literal notranslate"><span class="pre">batch_size=32</span></code> and <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> to as many CPUs on our machine (this will depend on what machine you’re using).</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Load and transform data</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">train_data_simple</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">simple_transform</span><span class="p">)</span>
<span class="n">test_data_simple</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">simple_transform</span><span class="p">)</span>

<span class="c1"># 2. Turn data into DataLoaders</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Setup batch size and number of workers </span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Creating DataLoader&#39;s with batch size </span><span class="si">{</span><span class="n">BATCH_SIZE</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">NUM_WORKERS</span><span class="si">}</span><span class="s2"> workers.&quot;</span><span class="p">)</span>

<span class="c1"># Create DataLoader&#39;s</span>
<span class="n">train_dataloader_simple</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data_simple</span><span class="p">,</span> 
                                     <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                     <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                     <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>

<span class="n">test_dataloader_simple</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data_simple</span><span class="p">,</span> 
                                    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                    <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>

<span class="n">train_dataloader_simple</span><span class="p">,</span> <span class="n">test_dataloader_simple</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Creating DataLoader&#39;s with batch size 32 and 16 workers.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ad2f70&gt;,
 &lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ad23d0&gt;)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s created!</p>
<p>Let’s build a model.</p>
</section>
<section id="create-tinyvgg-model-class">
<h3>7.2 Create TinyVGG model class<a class="headerlink" href="#create-tinyvgg-model-class" title="Permalink to this heading">#</a></h3>
<p>In <a class="reference external" href="https://www.learnpytorch.io/03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn">notebook 03</a>, we used the TinyVGG model from the <a class="reference external" href="https://poloclub.github.io/cnn-explainer/">CNN Explainer website</a>.</p>
<p>Let’s recreate the same model, except this time we’ll be using color images instead of grayscale (<code class="docutils literal notranslate"><span class="pre">in_channels=3</span></code> instead of <code class="docutils literal notranslate"><span class="pre">in_channels=1</span></code> for RGB pixels).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TinyVGG</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Model architecture copying TinyVGG from: </span>
<span class="sd">    https://poloclub.github.io/cnn-explainer/</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> 
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># how big is the square that&#39;s going over the image?</span>
                      <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># default</span>
                      <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="c1"># options = &quot;valid&quot; (no padding) or &quot;same&quot; (output has same shape as input) or int for specific number </span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                      <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                         <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># default stride value is same as kernel_size</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="c1"># Where did this in_features shape come from? </span>
            <span class="c1"># It&#39;s because each layer of our network compresses and changes the shape of our inputs data.</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="o">*</span><span class="mi">16</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span>
                      <span class="n">out_features</span><span class="o">=</span><span class="n">output_shape</span><span class="p">)</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)</span>
        <span class="k">return</span> <span class="n">x</span>
        <span class="c1"># return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model_0</span> <span class="o">=</span> <span class="n">TinyVGG</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># number of color channels (3 for RGB) </span>
                  <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                  <span class="n">output_shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">classes</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TinyVGG(
  (conv_block_1): Sequential(
    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_block_2): Sequential(
    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=2560, out_features=3, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> One of the ways to speed up deep learning models computing on a GPU is to leverage <strong>operator fusion</strong>.</p>
<p>This means in the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method in our model above, instead of calling a layer block and reassigning <code class="docutils literal notranslate"><span class="pre">x</span></code> every time, we call each block in succession (see the final line of the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method in the model above for an example).</p>
<p>This saves the time spent reassigning <code class="docutils literal notranslate"><span class="pre">x</span></code> (memory heavy) and focuses on only computing on <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
<p>See <a class="reference external" href="https://horace.io/brrr_intro.html"><em>Making Deep Learning Go Brrrr From First Principles</em></a> by Horace He for more ways on how to speed up machine learning models.</p>
</div></blockquote>
<p>Now that’s a nice looking model!</p>
<p>How about we test it out with a forward pass on a single image?</p>
</section>
<section id="try-a-forward-pass-on-a-single-image-to-test-the-model">
<h3>7.3 Try a forward pass on a single image (to test the model)<a class="headerlink" href="#try-a-forward-pass-on-a-single-image-to-test-the-model" title="Permalink to this heading">#</a></h3>
<p>A good way to test a model is to do a forward pass on a single piece of data.</p>
<p>It’s also handy way to test the input and output shapes of our different layers.</p>
<p>To do a forward pass on a single image, let’s:</p>
<ol class="arabic simple">
<li><p>Get a batch of images and labels from the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p>Get a single image from the batch and <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> the image so it has a batch size of <code class="docutils literal notranslate"><span class="pre">1</span></code> (so its shape fits the model).</p></li>
<li><p>Perform inference on a single image (making sure to send the image to the target <code class="docutils literal notranslate"><span class="pre">device</span></code>).</p></li>
<li><p>Print out what’s happening and convert the model’s raw output logits to prediction probabilities with <code class="docutils literal notranslate"><span class="pre">torch.softmax()</span></code> (since we’re working with multi-class data) and convert the prediction probabilities to prediction labels with <code class="docutils literal notranslate"><span class="pre">torch.argmax()</span></code>.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Get a batch of images and labels from the DataLoader</span>
<span class="n">img_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader_simple</span><span class="p">))</span>

<span class="c1"># 2. Get a single image from the batch and unsqueeze the image so its shape fits the model</span>
<span class="n">img_single</span><span class="p">,</span> <span class="n">label_single</span> <span class="o">=</span> <span class="n">img_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Single image shape: </span><span class="si">{</span><span class="n">img_single</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 3. Perform a forward pass on a single image</span>
<span class="n">model_0</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">(</span><span class="n">img_single</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    
<span class="c1"># 4. Print out what&#39;s happening and convert model logits -&gt; pred probs -&gt; pred label</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output logits:</span><span class="se">\n</span><span class="si">{</span><span class="n">pred</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output prediction probabilities:</span><span class="se">\n</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output prediction label:</span><span class="se">\n</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Actual label:</span><span class="se">\n</span><span class="si">{</span><span class="n">label_single</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Single image shape: torch.Size([1, 3, 64, 64])

Output logits:
tensor([[0.0578, 0.0634, 0.0352]], device=&#39;cuda:0&#39;)

Output prediction probabilities:
tensor([[0.3352, 0.3371, 0.3277]], device=&#39;cuda:0&#39;)

Output prediction label:
tensor([1], device=&#39;cuda:0&#39;)

Actual label:
2
</pre></div>
</div>
</div>
</div>
<p>Wonderful, it looks like our model is outputting what we’d expect it to output.</p>
<p>You can run the cell above a few times and each time have a different image be predicted on.</p>
<p>And you’ll probably notice the predictions are often wrong.</p>
<p>This is to be expected because the model hasn’t been trained yet and it’s essentially guessing using random weights.</p>
</section>
<section id="use-torchinfo-to-get-an-idea-of-the-shapes-going-through-our-model">
<h3>7.4 Use <code class="docutils literal notranslate"><span class="pre">torchinfo</span></code> to get an idea of the shapes going through our model<a class="headerlink" href="#use-torchinfo-to-get-an-idea-of-the-shapes-going-through-our-model" title="Permalink to this heading">#</a></h3>
<p>Printing out our model with <code class="docutils literal notranslate"><span class="pre">print(model)</span></code> gives us an idea of what’s going on with our model.</p>
<p>And we can print out the shapes of our data throughout the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
<p>However, a helpful way to get information from our model is to use <a class="reference external" href="https://github.com/TylerYep/torchinfo"><code class="docutils literal notranslate"><span class="pre">torchinfo</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">torchinfo</span></code> comes with a <code class="docutils literal notranslate"><span class="pre">summary()</span></code> method that takes a PyTorch model as well as an <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> and returns what happens as a tensor moves through your model.</p>
<blockquote>
<div><p><strong>Note:</strong> If you’re using Google Colab, you’ll need to install <code class="docutils literal notranslate"><span class="pre">torchinfo</span></code>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install torchinfo if it&#39;s not available, import it if it is</span>
<span class="k">try</span><span class="p">:</span> 
    <span class="kn">import</span> <span class="nn">torchinfo</span>
<span class="k">except</span><span class="p">:</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>torchinfo
    <span class="kn">import</span> <span class="nn">torchinfo</span>
    
<span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="n">summary</span><span class="p">(</span><span class="n">model_0</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span> <span class="c1"># do a test pass through of an example input size </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TinyVGG                                  [1, 3]                    --
├─Sequential: 1-1                        [1, 10, 32, 32]           --
│    └─Conv2d: 2-1                       [1, 10, 64, 64]           280
│    └─ReLU: 2-2                         [1, 10, 64, 64]           --
│    └─Conv2d: 2-3                       [1, 10, 64, 64]           910
│    └─ReLU: 2-4                         [1, 10, 64, 64]           --
│    └─MaxPool2d: 2-5                    [1, 10, 32, 32]           --
├─Sequential: 1-2                        [1, 10, 16, 16]           --
│    └─Conv2d: 2-6                       [1, 10, 32, 32]           910
│    └─ReLU: 2-7                         [1, 10, 32, 32]           --
│    └─Conv2d: 2-8                       [1, 10, 32, 32]           910
│    └─ReLU: 2-9                         [1, 10, 32, 32]           --
│    └─MaxPool2d: 2-10                   [1, 10, 16, 16]           --
├─Sequential: 1-3                        [1, 3]                    --
│    └─Flatten: 2-11                     [1, 2560]                 --
│    └─Linear: 2-12                      [1, 3]                    7,683
==========================================================================================
Total params: 10,693
Trainable params: 10,693
Non-trainable params: 0
Total mult-adds (M): 6.75
==========================================================================================
Input size (MB): 0.05
Forward/backward pass size (MB): 0.82
Params size (MB): 0.04
Estimated Total Size (MB): 0.91
==========================================================================================
</pre></div>
</div>
</div>
</div>
<p>Nice!</p>
<p>The output of <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code> gives us a whole bunch of information about our model.</p>
<p>Such as <code class="docutils literal notranslate"><span class="pre">Total</span> <span class="pre">params</span></code>, the total number of parameters in our model, the <code class="docutils literal notranslate"><span class="pre">Estimated</span> <span class="pre">Total</span> <span class="pre">Size</span> <span class="pre">(MB)</span></code> which is the size of our model.</p>
<p>You can also see the change in input and output shapes as data of a certain <code class="docutils literal notranslate"><span class="pre">input_size</span></code> moves through our model.</p>
<p>Right now, our parameter numbers and total model size is low.</p>
<p>This because we’re starting with a small model.</p>
<p>And if we need to increase its size later, we can.</p>
</section>
<section id="create-train-test-loop-functions">
<h3>7.5 Create train &amp; test loop functions<a class="headerlink" href="#create-train-test-loop-functions" title="Permalink to this heading">#</a></h3>
<p>We’ve got data and we’ve got a model.</p>
<p>Now let’s make some training and test loop functions to train our model on the training data and evaluate our model on the testing data.</p>
<p>And to make sure we can use these the training and testing loops again, we’ll functionize them.</p>
<p>Specifically, we’re going to make three functions:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train_step()</span></code> - takes in a model, a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>, a loss function and an optimizer and trains the model on the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_step()</span></code> - takes in a model, a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> and a loss function and evaluates the model on the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train()</span></code> - performs 1. and 2. together for a given number of epochs and returns a results dictionary.</p></li>
</ol>
<blockquote>
<div><p><strong>Note:</strong> We covered the steps in a PyTorch opimization loop in <a class="reference external" href="https://www.learnpytorch.io/01_pytorch_workflow/#creating-an-optimization-loop-in-pytorch">notebook 01</a>, as well as the<a class="reference external" href="https://youtu.be/Nutpusq_AFw"> Unofficial PyTorch Optimization Loop Song</a> and we’ve built similar functions in <a class="reference external" href="https://www.learnpytorch.io/03_pytorch_computer_vision/#62-functionizing-training-and-test-loops">notebook 03</a>.</p>
</div></blockquote>
<p>Let’s start by building <code class="docutils literal notranslate"><span class="pre">train_step()</span></code>.</p>
<p>Because we’re dealing with batches in the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s, we’ll accumulate the model loss and accuracy values during training (by adding them up for each batch) and then adjust them at the end before we return them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> 
               <span class="n">dataloader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> 
               <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> 
               <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="c1"># Put model in train mode</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    
    <span class="c1"># Setup train loss and train accuracy values</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    
    <span class="c1"># Loop through data loader data batches</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="c1"># Send data to target device</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># 1. Forward pass</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># 2. Calculate  and accumulate loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> 

        <span class="c1"># 3. Optimizer zero grad</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 4. Loss backward</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># 5. Optimizer step</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Calculate and accumulate accuracy metric across all batches</span>
        <span class="n">y_pred_class</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">train_acc</span> <span class="o">+=</span> <span class="p">(</span><span class="n">y_pred_class</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="c1"># Adjust metrics to get average loss and accuracy per batch </span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span>
</pre></div>
</div>
</div>
</div>
<p>Woohoo! <code class="docutils literal notranslate"><span class="pre">train_step()</span></code> function done.</p>
<p>Now let’s do the same for the <code class="docutils literal notranslate"><span class="pre">test_step()</span></code> function.</p>
<p>The main difference here will be the <code class="docutils literal notranslate"><span class="pre">test_step()</span></code> won’t take in an optimizer and therefore won’t perform gradient descent.</p>
<p>But since we’ll be doing inference, we’ll make sure to turn on the <code class="docutils literal notranslate"><span class="pre">torch.inference_mode()</span></code> context manager for making predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> 
              <span class="n">dataloader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> 
              <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># Put model in eval mode</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> 
    
    <span class="c1"># Setup test loss and test accuracy values</span>
    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    
    <span class="c1"># Turn on inference context manager</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="c1"># Loop through DataLoader batches</span>
        <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
            <span class="c1"># Send data to target device</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
            <span class="c1"># 1. Forward pass</span>
            <span class="n">test_pred_logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1"># 2. Calculate and accumulate loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">test_pred_logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            
            <span class="c1"># Calculate and accumulate accuracy</span>
            <span class="n">test_pred_labels</span> <span class="o">=</span> <span class="n">test_pred_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">test_acc</span> <span class="o">+=</span> <span class="p">((</span><span class="n">test_pred_labels</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_pred_labels</span><span class="p">))</span>
            
    <span class="c1"># Adjust metrics to get average loss and accuracy per batch </span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="n">test_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span>
</pre></div>
</div>
</div>
</div>
<p>Excellent!</p>
</section>
<section id="creating-a-train-function-to-combine-train-step-and-test-step">
<h3>7.6 Creating a <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to combine <code class="docutils literal notranslate"><span class="pre">train_step()</span></code> and <code class="docutils literal notranslate"><span class="pre">test_step()</span></code><a class="headerlink" href="#creating-a-train-function-to-combine-train-step-and-test-step" title="Permalink to this heading">#</a></h3>
<p>Now we need a way to put our <code class="docutils literal notranslate"><span class="pre">train_step()</span></code> and <code class="docutils literal notranslate"><span class="pre">test_step()</span></code> functions together.</p>
<p>To do so, we’ll package them up in a <code class="docutils literal notranslate"><span class="pre">train()</span></code> function.</p>
<p>This function will train the model as well as evaluate it.</p>
<p>Specificially, it’ll:</p>
<ol class="arabic simple">
<li><p>Take in a model, a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for.</p></li>
<li><p>Create an empty results dictionary for <code class="docutils literal notranslate"><span class="pre">train_loss</span></code>, <code class="docutils literal notranslate"><span class="pre">train_acc</span></code>, <code class="docutils literal notranslate"><span class="pre">test_loss</span></code> and <code class="docutils literal notranslate"><span class="pre">test_acc</span></code> values (we can fill this up as training goes on).</p></li>
<li><p>Loop through the training and test step functions for a number of epochs.</p></li>
<li><p>Print out what’s happening at the end of each epoch.</p></li>
<li><p>Update the empty results dictionary with the updated metrics each epoch.</p></li>
<li><p>Return the filled</p></li>
</ol>
<p>To keep track of the number of epochs we’ve been through, let’s import <code class="docutils literal notranslate"><span class="pre">tqdm</span></code> from <code class="docutils literal notranslate"><span class="pre">tqdm.auto</span></code> (<a class="reference external" href="https://github.com/tqdm/tqdm"><code class="docutils literal notranslate"><span class="pre">tqdm</span></code></a> is one of the most popular progress bar libraries for Python and <code class="docutils literal notranslate"><span class="pre">tqdm.auto</span></code> automatically decides what kind of progress bar is best for your computing environment, e.g. Jupyter Notebook vs. Python script).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># 1. Take in various parameters required for training and test steps</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> 
          <span class="n">train_dataloader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> 
          <span class="n">test_dataloader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> 
          <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
          <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
          <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    
    <span class="c1"># 2. Create empty results dictionary</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;train_acc&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;test_loss&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="p">[]</span>
    <span class="p">}</span>
    
    <span class="c1"># 3. Loop through training and testing steps for a number of epochs</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                                           <span class="n">dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
                                           <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
            <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">)</span>
        
        <span class="c1"># 4. Print out what&#39;s happening</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;train_loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;train_acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;test_loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;test_acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="c1"># 5. Update results dictionary</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;test_acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>

    <span class="c1"># 6. Return the filled results at the end of the epochs</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-and-evaluate-model-0">
<h3>7.7 Train and Evaluate Model 0<a class="headerlink" href="#train-and-evaluate-model-0" title="Permalink to this heading">#</a></h3>
<p>Alright, alright, alright we’ve got all of the ingredients we need to train and evaluate our model.</p>
<p>Time to put our <code class="docutils literal notranslate"><span class="pre">TinyVGG</span></code> model, <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s and <code class="docutils literal notranslate"><span class="pre">train()</span></code> function together to see if we can build a model capable of discerning between pizza, steak and sushi!</p>
<p>Let’s recreate <code class="docutils literal notranslate"><span class="pre">model_0</span></code> (we don’t need to but we will for completeness) then call our <code class="docutils literal notranslate"><span class="pre">train()</span></code> function passing in the necessary parameters.</p>
<p>To keep our experiments quick, we’ll train our model for <strong>5 epochs</strong> (though you could increase this if you want).</p>
<p>As for an <strong>optimizer</strong> and <strong>loss function</strong>, we’ll use <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss()</span></code> (since we’re working with multi-class classification data) and <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code> with a learning rate of <code class="docutils literal notranslate"><span class="pre">1e-3</span></code> respecitvely.</p>
<p>To see how long things take, we’ll import Python’s <a class="reference external" href="https://docs.python.org/3/library/timeit.html#timeit.default_timer"><code class="docutils literal notranslate"><span class="pre">timeit.default_timer()</span></code></a> method to calculate the training time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seeds</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> 
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Set number of epochs</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Recreate an instance of TinyVGG</span>
<span class="n">model_0</span> <span class="o">=</span> <span class="n">TinyVGG</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># number of color channels (3 for RGB) </span>
                  <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                  <span class="n">output_shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">classes</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Setup loss function and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_0</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Start the timer</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span> <span class="k">as</span> <span class="n">timer</span> 
<span class="n">start_time</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>

<span class="c1"># Train model_0 </span>
<span class="n">model_0_results</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_0</span><span class="p">,</span> 
                        <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader_simple</span><span class="p">,</span>
                        <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader_simple</span><span class="p">,</span>
                        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> 
                        <span class="n">epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>

<span class="c1"># End the timer and print out how long it took</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total training time: </span><span class="si">{</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b566c3ddf5bc4a8b98a6db06c8825c9d", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 1.1078 | train_acc: 0.2578 | test_loss: 1.1360 | test_acc: 0.2604
Epoch: 2 | train_loss: 1.0847 | train_acc: 0.4258 | test_loss: 1.1620 | test_acc: 0.1979
Epoch: 3 | train_loss: 1.1157 | train_acc: 0.2930 | test_loss: 1.1697 | test_acc: 0.1979
Epoch: 4 | train_loss: 1.0956 | train_acc: 0.4141 | test_loss: 1.1384 | test_acc: 0.1979
Epoch: 5 | train_loss: 1.0985 | train_acc: 0.2930 | test_loss: 1.1426 | test_acc: 0.1979
Total training time: 4.935 seconds
</pre></div>
</div>
</div>
</div>
<p>Hmm…</p>
<p>It looks like our model performed pretty poorly.</p>
<p>But that’s okay for now, we’ll keep persevering.</p>
<p>What are some ways you could potentially improve it?</p>
<blockquote>
<div><p><strong>Note:</strong> Check out the <a class="reference external" href="https://www.learnpytorch.io/02_pytorch_classification/#5-improving-a-model-from-a-model-perspective"><em>Improving a model (from a model perspective)</em> section in notebook 02</a> for ideas on improving our TinyVGG model.</p>
</div></blockquote>
</section>
<section id="plot-the-loss-curves-of-model-0">
<h3>7.8 Plot the loss curves of Model 0<a class="headerlink" href="#plot-the-loss-curves-of-model-0" title="Permalink to this heading">#</a></h3>
<p>From the print outs of our <code class="docutils literal notranslate"><span class="pre">model_0</span></code> training, it didn’t look like it did too well.</p>
<p>But we can further evaluate it by plotting the model’s <strong>loss curves</strong>.</p>
<p><strong>Loss curves</strong> show the model’s results over time.</p>
<p>And they’re a great way to see how your model performs on different datasets (e.g. training and test).</p>
<p>Let’s create a function to plot the values in our <code class="docutils literal notranslate"><span class="pre">model_0_results</span></code> dictionary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the model_0_results keys</span>
<span class="n">model_0_results</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;train_loss&#39;, &#39;train_acc&#39;, &#39;test_loss&#39;, &#39;test_acc&#39;])
</pre></div>
</div>
</div>
</div>
<p>We’ll need to extract each of these keys and turn them into a plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_loss_curves</span><span class="p">(</span><span class="n">results</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plots training curves of a results dictionary.</span>

<span class="sd">    Args:</span>
<span class="sd">        results (dict): dictionary containing list of values, e.g.</span>
<span class="sd">            {&quot;train_loss&quot;: [...],</span>
<span class="sd">             &quot;train_acc&quot;: [...],</span>
<span class="sd">             &quot;test_loss&quot;: [...],</span>
<span class="sd">             &quot;test_acc&quot;: [...]}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Get the loss values of the results dictionary (training and test)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">]</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;test_loss&#39;</span><span class="p">]</span>

    <span class="c1"># Get the accuracy values of the results dictionary (training and test)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;train_acc&#39;</span><span class="p">]</span>
    <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;test_acc&#39;</span><span class="p">]</span>

    <span class="c1"># Figure out how many epochs there were</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">]))</span>

    <span class="c1"># Setup a plot </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

    <span class="c1"># Plot loss</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train_loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test_loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Plot accuracy</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train_accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test_accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<p>Okay, let’s test our <code class="docutils literal notranslate"><span class="pre">plot_loss_curves()</span></code> function out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss_curves</span><span class="p">(</span><span class="n">model_0_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/24b828bae38c1243b1026355a96519a4047ba15568c266ad1f39cee6fdc141ad.png" src="../../_images/24b828bae38c1243b1026355a96519a4047ba15568c266ad1f39cee6fdc141ad.png" />
</div>
</div>
<p>Woah.</p>
<p>Looks like things are all over the place…</p>
<p>But we kind of knew that because our model’s print out results during training didn’t show much promise.</p>
<p>You could try training the model for longer and see what happens when you plot a loss curve over a longer time horizon.</p>
</section>
</section>
<section id="what-should-an-ideal-loss-curve-look-like">
<h2>8. What should an ideal loss curve look like?<a class="headerlink" href="#what-should-an-ideal-loss-curve-look-like" title="Permalink to this heading">#</a></h2>
<p>Looking at training and test loss curves is a great way to see if your model is <strong>overfitting</strong>.</p>
<p>An overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.</p>
<p>If your training loss is far lower than your test loss, your model is <strong>overfitting</strong>.</p>
<p>As in, it’s learning the patterns in the training too well and those patterns aren’t generalizing to the test data.</p>
<p>The other side is when your training and test loss are not as low as you’d like, this is considered <strong>underfitting</strong>.</p>
<p>The ideal position for a training and test loss curve is for them to line up closely with each other.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-loss-curves-overfitting-underfitting-ideal.jpg"><img alt="different training and test loss curves illustrating overfitting, underfitting and the ideal loss curves" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-loss-curves-overfitting-underfitting-ideal.jpg" style="width: 800px;" /></a>
<p><em>Left: If your training and test loss curves aren’t as low as you’d like, this is considered <strong>underfitting</strong>. <em>Middle:</em> When your test/validation loss is higher than your training loss this is considered <strong>overfitting</strong>. <em>Right:</em> The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Google’s <a class="reference external" href="https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic">Interpreting Loss Curves guide</a>.</em></p>
<section id="how-to-deal-with-overfitting">
<h3>8.1 How to deal with overfitting<a class="headerlink" href="#how-to-deal-with-overfitting" title="Permalink to this heading">#</a></h3>
<p>Since the main problem with overfitting is that you’re model is fitting the training data <em>too well</em>, you’ll want to use techniques to “reign it in”.</p>
<p>A common technique of preventing overfitting is known as <a class="reference external" href="https://ml-cheatsheet.readthedocs.io/en/latest/regularization.html"><strong>regularization</strong></a>.</p>
<p>I like to think of this as “making our models more regular”, as in, capable of fitting <em>more</em> kinds of data.</p>
<p>Let’s discuss a few methods to prevent overfitting.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Method to prevent overfitting</strong></p></th>
<th class="head"><p><strong>What is it?</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Get more data</strong></p></td>
<td><p>Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Simplify your model</strong></p></td>
<td><p>If the current model is already overfitting the training data, it may be too complicated of a model. This means it’s learning the patterns of the data too well and isn’t able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Use data augmentation</strong></p></td>
<td><p><a class="reference external" href="https://developers.google.com/machine-learning/glossary#data-augmentation"><strong>Data augmentation</strong></a> manipulates the training data in a way so that’s harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Use transfer learning</strong></p></td>
<td><p><a class="reference external" href="https://developers.google.com/machine-learning/glossary#transfer-learning"><strong>Transfer learning</strong></a> involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Use dropout layers</strong></p></td>
<td><p>Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Dropout()</span></code></a> for more.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Use learning rate decay</strong></p></td>
<td><p>The idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to <a class="reference external" href="https://developers.google.com/machine-learning/glossary#convergence"><strong>convergence</strong></a>, the smaller you’ll want your weight updates to be.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Use early stopping</strong></p></td>
<td><p><a class="reference external" href="https://developers.google.com/machine-learning/glossary#early_stopping"><strong>Early stopping</strong></a> stops model training <em>before</em> it begins to overfit. As in, say the model’s loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior).</p></td>
</tr>
</tbody>
</table>
</div>
<p>There are more methods for dealing with overfitting but these are some of the main ones.</p>
<p>As you start to build more and more deep models, you’ll find because deep learnings are <em>so good</em> at learning patterns in data, dealing with overfitting is one of the primary problems of deep learning.</p>
</section>
<section id="how-to-deal-with-underfitting">
<h3>8.2 How to deal with underfitting<a class="headerlink" href="#how-to-deal-with-underfitting" title="Permalink to this heading">#</a></h3>
<p>When a model is <a class="reference external" href="https://developers.google.com/machine-learning/glossary#underfitting"><strong>underfitting</strong></a> it is considered to have poor predictive power on the training and test sets.</p>
<p>In essence, an underfitting model will fail to reduce the loss values to a desired level.</p>
<p>Right now, looking at our current loss curves, I’d considered our <code class="docutils literal notranslate"><span class="pre">TinyVGG</span></code> model, <code class="docutils literal notranslate"><span class="pre">model_0</span></code>, to be underfitting the data.</p>
<p>The main idea behind dealing with underfitting is to <em>increase</em> your model’s predictive power.</p>
<p>There are several ways to do this.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Method to prevent underfitting</strong></p></th>
<th class="head"><p><strong>What is it?</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Add more layers/units to your model</strong></p></td>
<td><p>If your model is underfitting, it may not have enough capability to <em>learn</em> the required patterns/weights/representations of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden layers/units within those layers.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Tweak the learning rate</strong></p></td>
<td><p>Perhaps your model’s learning rate is too high to begin with. And it’s trying to update its weights each epoch too much, in turn not learning anything. In this case, you might lower the learning rate and see what happens.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Use transfer learning</strong></p></td>
<td><p>Transfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a previously working model and adjusting them to your own problem.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Train for longer</strong></p></td>
<td><p>Sometimes a model just needs more time to learn representations of data. If you find in your smaller experiments your model isn’t learning anything, perhaps leaving it train for a more epochs may result in better performance.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Use less regularization</strong></p></td>
<td><p>Perhaps your model is underfitting because you’re trying to prevent overfitting too much. Holding back on regularization techniques can help your model fit the data better.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="the-balance-between-overfitting-and-underfitting">
<h3>8.3 The balance between overfitting and underfitting<a class="headerlink" href="#the-balance-between-overfitting-and-underfitting" title="Permalink to this heading">#</a></h3>
<p>None of the methods discussed above are silver bullets, meaning, they don’t always work.</p>
<p>And preventing overfitting and underfitting is possibly the most active area of machine learning research.</p>
<p>Since everone wants their models to fit better (less underfitting) but not so good they don’t generalize well and perform in the real world (less overfitting).</p>
<p>There’s a fine line between overfitting and underfitting.</p>
<p>Because too much of each can cause the other.</p>
<p>Transfer learning is perhaps one of the most powerful techniques when it comes to dealing with both overfitting and underfitting on your own problems.</p>
<p>Rather than handcraft different overfitting and underfitting techniques, transfer learning enables you to take an already working model in a similar problem space to yours (say one from <a class="reference external" href="https://paperswithcode.com/sota">paperswithcode.com/sota</a> or <a class="reference external" href="https://huggingface.co/models">Hugging Face models</a>) and apply it to your own dataset.</p>
<p>We’ll see the power of transfer learning in a later notebook.</p>
</section>
</section>
<section id="model-1-tinyvgg-with-data-augmentation">
<h2>9. Model 1: TinyVGG with Data Augmentation<a class="headerlink" href="#model-1-tinyvgg-with-data-augmentation" title="Permalink to this heading">#</a></h2>
<p>Time to try out another model!</p>
<p>This time, let’s load in the data and use <strong>data augmentation</strong> to see if it improves our results in anyway.</p>
<p>First, we’ll compose a training transform to include <code class="docutils literal notranslate"><span class="pre">transforms.TrivialAugmentWide()</span></code> as well as resize and turn our images into tensors.</p>
<p>We’ll do the same for a testing transform except without the data augmentation.</p>
<section id="create-transform-with-data-augmentation">
<h3>9.1 Create transform with data augmentation<a class="headerlink" href="#create-transform-with-data-augmentation" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create training transform with TrivialAugment</span>
<span class="n">train_transform_trivial_augment</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">TrivialAugmentWide</span><span class="p">(</span><span class="n">num_magnitude_bins</span><span class="o">=</span><span class="mi">31</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span> 
<span class="p">])</span>

<span class="c1"># Create testing transform (no data augmentation)</span>
<span class="n">test_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Wonderful!</p>
<p>Now let’s turn our images into <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s using <code class="docutils literal notranslate"><span class="pre">torchvision.datasets.ImageFolder()</span></code> and then into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s with <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader()</span></code>.</p>
</section>
<section id="create-train-and-test-datasets-and-dataloaders">
<h3>9.2 Create train and test <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s<a class="headerlink" href="#create-train-and-test-datasets-and-dataloaders" title="Permalink to this heading">#</a></h3>
<p>We’ll make sure the train <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> uses the <code class="docutils literal notranslate"><span class="pre">train_transform_trivial_augment</span></code> and the test <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> uses the <code class="docutils literal notranslate"><span class="pre">test_transform</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn image folders into Datasets</span>
<span class="n">train_data_augmented</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">train_transform_trivial_augment</span><span class="p">)</span>
<span class="n">test_data_simple</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">test_transform</span><span class="p">)</span>

<span class="n">train_data_augmented</span><span class="p">,</span> <span class="n">test_data_simple</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Dataset ImageFolder
     Number of datapoints: 225
     Root location: data/pizza_steak_sushi/train
     StandardTransform
 Transform: Compose(
                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)
                TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)
                ToTensor()
            ),
 Dataset ImageFolder
     Number of datapoints: 75
     Root location: data/pizza_steak_sushi/test
     StandardTransform
 Transform: Compose(
                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)
                ToTensor()
            ))
</pre></div>
</div>
</div>
</div>
<p>And we’ll make <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s with a <code class="docutils literal notranslate"><span class="pre">batch_size=32</span></code> and with <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> set to the number of CPUs available on our machine (we can get this using Python’s <code class="docutils literal notranslate"><span class="pre">os.cpu_count()</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn Datasets into DataLoader&#39;s</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_dataloader_augmented</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data_augmented</span><span class="p">,</span> 
                                        <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                        <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>

<span class="n">test_dataloader_simple</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data_simple</span><span class="p">,</span> 
                                    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                    <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>

<span class="n">train_dataloader_augmented</span><span class="p">,</span> <span class="n">test_dataloader</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f53c6d64040&gt;,
 &lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9de50&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="construct-and-train-model-1">
<h3>9.3 Construct and train Model 1<a class="headerlink" href="#construct-and-train-model-1" title="Permalink to this heading">#</a></h3>
<p>Data loaded!</p>
<p>Now to build our next model, <code class="docutils literal notranslate"><span class="pre">model_1</span></code>, we can reuse our <code class="docutils literal notranslate"><span class="pre">TinyVGG</span></code> class from before.</p>
<p>We’ll make sure to send it to the target device.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create model_1 and send it to the target device</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="n">TinyVGG</span><span class="p">(</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">output_shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data_augmented</span><span class="o">.</span><span class="n">classes</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TinyVGG(
  (conv_block_1): Sequential(
    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_block_2): Sequential(
    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=2560, out_features=3, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<p>Model ready!</p>
<p>Time to train!</p>
<p>Since we’ve already got functions for the training loop (<code class="docutils literal notranslate"><span class="pre">train_step()</span></code>) and testing loop (<code class="docutils literal notranslate"><span class="pre">test_step()</span></code>) and a function to put them together in <code class="docutils literal notranslate"><span class="pre">train()</span></code>, let’s reuse those.</p>
<p>We’ll use the same setup as <code class="docutils literal notranslate"><span class="pre">model_0</span></code> with only the <code class="docutils literal notranslate"><span class="pre">train_dataloader</span></code> parameter varying:</p>
<ul class="simple">
<li><p>Train for 5 epochs.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">train_dataloader=train_dataloader_augmented</span></code> as the training data in <code class="docutils literal notranslate"><span class="pre">train()</span></code>.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss()</span></code> as the loss function (since we’re working with multi-class classification).</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code> with <code class="docutils literal notranslate"><span class="pre">lr=0.001</span></code> as the learning rate as the optimizer.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seeds</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> 
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Set number of epochs</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Setup loss function and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Start the timer</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span> <span class="k">as</span> <span class="n">timer</span> 
<span class="n">start_time</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>

<span class="c1"># Train model_1</span>
<span class="n">model_1_results</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span> 
                        <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader_augmented</span><span class="p">,</span>
                        <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader_simple</span><span class="p">,</span>
                        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> 
                        <span class="n">epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>

<span class="c1"># End the timer and print out how long it took</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">timer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total training time: </span><span class="si">{</span><span class="n">end_time</span><span class="o">-</span><span class="n">start_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "cf64499894624ea2a5c2da2f27c1b765", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 1.1074 | train_acc: 0.2500 | test_loss: 1.1058 | test_acc: 0.2604
Epoch: 2 | train_loss: 1.0791 | train_acc: 0.4258 | test_loss: 1.1382 | test_acc: 0.2604
Epoch: 3 | train_loss: 1.0803 | train_acc: 0.4258 | test_loss: 1.1685 | test_acc: 0.2604
Epoch: 4 | train_loss: 1.1285 | train_acc: 0.3047 | test_loss: 1.1623 | test_acc: 0.2604
Epoch: 5 | train_loss: 1.0880 | train_acc: 0.4258 | test_loss: 1.1472 | test_acc: 0.2604
Total training time: 4.924 seconds
</pre></div>
</div>
</div>
</div>
<p>Hmm…</p>
<p>It doesn’t look like our model performed very well again.</p>
<p>Let’s check out its loss curves.</p>
</section>
<section id="plot-the-loss-curves-of-model-1">
<h3>9.4 Plot the loss curves of Model 1<a class="headerlink" href="#plot-the-loss-curves-of-model-1" title="Permalink to this heading">#</a></h3>
<p>Since we’ve got the results of <code class="docutils literal notranslate"><span class="pre">model_1</span></code> saved in a results dictionary, <code class="docutils literal notranslate"><span class="pre">model_1_results</span></code>, we can plot them using <code class="docutils literal notranslate"><span class="pre">plot_loss_curves()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss_curves</span><span class="p">(</span><span class="n">model_1_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/85e4224e361bac8a554ba7feae0e32e6a99e9fab23206556c11f3ce19c0fe4d2.png" src="../../_images/85e4224e361bac8a554ba7feae0e32e6a99e9fab23206556c11f3ce19c0fe4d2.png" />
</div>
</div>
<p>Wow…</p>
<p>These don’t look very good either…</p>
<p>Is our model <strong>underfitting</strong> or <strong>overfitting</strong>?</p>
<p>Or both?</p>
<p>Ideally we’d like it have higher accuracy and lower loss right?</p>
<p>What are some methods you could try to use to achieve these?</p>
</section>
</section>
<section id="compare-model-results">
<h2>10. Compare model results<a class="headerlink" href="#compare-model-results" title="Permalink to this heading">#</a></h2>
<p>Even though our models our performing quite poorly, we can still write code to compare them.</p>
<p>Let’s first turn our model results in pandas DataFrames.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">model_0_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model_0_results</span><span class="p">)</span>
<span class="n">model_1_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model_1_results</span><span class="p">)</span>
<span class="n">model_0_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train_loss</th>
      <th>train_acc</th>
      <th>test_loss</th>
      <th>test_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.107833</td>
      <td>0.257812</td>
      <td>1.136041</td>
      <td>0.260417</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.084713</td>
      <td>0.425781</td>
      <td>1.162014</td>
      <td>0.197917</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.115697</td>
      <td>0.292969</td>
      <td>1.169704</td>
      <td>0.197917</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.095564</td>
      <td>0.414062</td>
      <td>1.138373</td>
      <td>0.197917</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.098520</td>
      <td>0.292969</td>
      <td>1.142631</td>
      <td>0.197917</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>And now we can write some plotting code using <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> to visualize the results of <code class="docutils literal notranslate"><span class="pre">model_0</span></code> and <code class="docutils literal notranslate"><span class="pre">model_1</span></code> together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup a plot </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Get number of epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_0_df</span><span class="p">))</span>

<span class="c1"># Plot train loss</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model_0_df</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Model 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model_1_df</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Model 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Train Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot test loss</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model_0_df</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Model 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model_1_df</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Model 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Test Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot train accuracy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model_0_df</span><span class="p">[</span><span class="s2">&quot;train_acc&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Model 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model_1_df</span><span class="p">[</span><span class="s2">&quot;train_acc&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Model 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Train Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Plot test accuracy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model_0_df</span><span class="p">[</span><span class="s2">&quot;test_acc&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Model 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model_1_df</span><span class="p">[</span><span class="s2">&quot;test_acc&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Model 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Test Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/431c310c3513b1925d1f22e9acbd0940a637f08878db3f5a1cb1b5f58dca3cba.png" src="../../_images/431c310c3513b1925d1f22e9acbd0940a637f08878db3f5a1cb1b5f58dca3cba.png" />
</div>
</div>
<p>It looks like our models both performed equally poorly and were kind of sporadic (the metrics go up and down sharply).</p>
<p>If you built <code class="docutils literal notranslate"><span class="pre">model_2</span></code>, what would you do differently to try and improve performance?</p>
</section>
<section id="make-a-prediction-on-a-custom-image">
<h2>11. Make a prediction on a custom image<a class="headerlink" href="#make-a-prediction-on-a-custom-image" title="Permalink to this heading">#</a></h2>
<p>If you’ve trained a model on a certain dataset, chances are you’d like to make a prediction on on your own custom data.</p>
<p>In our case, since we’ve trained a model on pizza, steak and sushi images, how could we use our model to make a prediction on one of our own images?</p>
<p>To do so, we can load an image and then <strong>preprocess it in a way that matches the type of data our model was trained on</strong>.</p>
<p>In other words, we’ll have to convert our own custom image to a tensor and make sure it’s in the right datatype before passing it to our model.</p>
<p>Let’s start by downloading a custom image.</p>
<p>Since our model predicts whether an image contains pizza, steak or sushi, let’s download a photo of <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg">my Dad giving two thumbs up to a big pizza from the Learn PyTorch for Deep Learning GitHub</a>.</p>
<p>We download the image using Python’s <code class="docutils literal notranslate"><span class="pre">requests</span></code> module.</p>
<blockquote>
<div><p><strong>Note:</strong> If you’re using Google Colab, you can also upload an image to the current session by going to the left hand side menu -&gt; Files -&gt; Upload to session storage. Beware though, this image will delete when your Google Colab session ends.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download custom image</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="c1"># Setup custom image path</span>
<span class="n">custom_image_path</span> <span class="o">=</span> <span class="n">data_path</span> <span class="o">/</span> <span class="s2">&quot;04-pizza-dad.jpeg&quot;</span>

<span class="c1"># Download the image if it doesn&#39;t already exist</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">custom_image_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">custom_image_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="c1"># When downloading from GitHub, need to use the &quot;raw&quot; file link</span>
        <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-pizza-dad.jpeg&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Downloading </span><span class="si">{</span><span class="n">custom_image_path</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">custom_image_path</span><span class="si">}</span><span class="s2"> already exists, skipping download.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>data/04-pizza-dad.jpeg already exists, skipping download.
</pre></div>
</div>
</div>
</div>
<section id="loading-in-a-custom-image-with-pytorch">
<h3>11.1 Loading in a custom image with PyTorch<a class="headerlink" href="#loading-in-a-custom-image-with-pytorch" title="Permalink to this heading">#</a></h3>
<p>Excellent!</p>
<p>Looks like we’ve got a custom image downloaded and ready to go at <code class="docutils literal notranslate"><span class="pre">data/04-pizza-dad.jpeg</span></code>.</p>
<p>Time to load it in.</p>
<p>PyTorch’s <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> has several input and output (“IO” or “io” for short) methods for reading and writing images and video in <a class="reference external" href="https://pytorch.org/vision/stable/io.html"><code class="docutils literal notranslate"><span class="pre">torchvision.io</span></code></a>.</p>
<p>Since we want to load in an image, we’ll use <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image"><code class="docutils literal notranslate"><span class="pre">torchvision.io.read_image()</span></code></a>.</p>
<p>This method will read a JPEG or PNG image and turn it into a 3 dimensional RGB or grayscale <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> with values of datatype <code class="docutils literal notranslate"><span class="pre">uint8</span></code> in range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code>.</p>
<p>Let’s try it out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span>

<span class="c1"># Read in custom image</span>
<span class="n">custom_image_uint8</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read_image</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">custom_image_path</span><span class="p">))</span>

<span class="c1"># Print out image data</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Custom image tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">custom_image_uint8</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Custom image shape: </span><span class="si">{</span><span class="n">custom_image_uint8</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Custom image dtype: </span><span class="si">{</span><span class="n">custom_image_uint8</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Custom image tensor:
tensor([[[154, 173, 181,  ...,  21,  18,  14],
         [146, 165, 181,  ...,  21,  18,  15],
         [124, 146, 172,  ...,  18,  17,  15],
         ...,
         [ 72,  59,  45,  ..., 152, 150, 148],
         [ 64,  55,  41,  ..., 150, 147, 144],
         [ 64,  60,  46,  ..., 149, 146, 143]],

        [[171, 190, 193,  ...,  22,  19,  15],
         [163, 182, 193,  ...,  22,  19,  16],
         [141, 163, 184,  ...,  19,  18,  16],
         ...,
         [ 55,  42,  28,  ..., 107, 104, 103],
         [ 47,  38,  24,  ..., 108, 104, 102],
         [ 47,  43,  29,  ..., 107, 104, 101]],

        [[119, 138, 147,  ...,  17,  14,  10],
         [111, 130, 145,  ...,  17,  14,  11],
         [ 87, 111, 136,  ...,  14,  13,  11],
         ...,
         [ 35,  22,   8,  ...,  52,  52,  48],
         [ 27,  18,   4,  ...,  50,  49,  44],
         [ 27,  23,   9,  ...,  49,  46,  43]]], dtype=torch.uint8)

Custom image shape: torch.Size([3, 4032, 3024])

Custom image dtype: torch.uint8
</pre></div>
</div>
</div>
</div>
<p>Nice! Looks like our image is in tensor format, however, is this image format compatible with our model?</p>
<p>Our <code class="docutils literal notranslate"><span class="pre">custom_image</span></code> tensor is of datatype <code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code> and its values are between <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code>.</p>
<p>But our model takes image tensors of datatype <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> and with values between <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>.</p>
<p>So before we use our custom image with our model, <strong>we’ll need to convert it to the same format as the data our model is trained on</strong>.</p>
<p>If we don’t do this, our model will error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Try to make a prediction on image in uint8 format (this will error)</span>
<span class="n">model_1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">model_1</span><span class="p">(</span><span class="n">custom_image_uint8</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="nn">Input In [61],</span> in <span class="ni">&lt;cell line: 3&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">model_1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
<span class="ne">----&gt; </span><span class="mi">4</span>     <span class="n">model_1</span><span class="p">(</span><span class="n">custom_image_uint8</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">Input In [41],</span> in <span class="ni">TinyVGG.forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">39</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">40</span>     <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">41</span>     <span class="c1"># print(x.shape)</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span>     <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139,</span> in <span class="ni">Sequential.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">137</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">138</span>     <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">139</span>         <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>     <span class="k">return</span> <span class="nb">input</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:457,</span> in <span class="ni">Conv2d.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">456</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">457</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:453,</span> in <span class="ni">Conv2d._conv_forward</span><span class="nt">(self, input, weight, bias)</span>
<span class="g g-Whitespace">    </span><span class="mi">449</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span>     <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">451</span>                     <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">452</span>                     <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">453</span> <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">454</span>                 <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

<span class="ne">RuntimeError</span>: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same
</pre></div>
</div>
</div>
</div>
<p>If we try to make a prediction on an image in a different datatype to what our model was trained on, we get an error like the following:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Input</span> <span class="pre">type</span> <span class="pre">(torch.cuda.ByteTensor)</span> <span class="pre">and</span> <span class="pre">weight</span> <span class="pre">type</span> <span class="pre">(torch.cuda.FloatTensor)</span> <span class="pre">should</span> <span class="pre">be</span> <span class="pre">the</span> <span class="pre">same</span></code></p>
</div></blockquote>
<p>Let’s fix this by converting our custom image to the same datatype as what our model was trained on (<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load in custom image and convert the tensor values to float32</span>
<span class="n">custom_image</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read_image</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">custom_image_path</span><span class="p">))</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Divide the image pixel values by 255 to get them between [0, 1]</span>
<span class="n">custom_image</span> <span class="o">=</span> <span class="n">custom_image</span> <span class="o">/</span> <span class="mf">255.</span> 

<span class="c1"># Print out image data</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Custom image tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">custom_image</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Custom image shape: </span><span class="si">{</span><span class="n">custom_image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Custom image dtype: </span><span class="si">{</span><span class="n">custom_image</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Custom image tensor:
tensor([[[0.6039, 0.6784, 0.7098,  ..., 0.0824, 0.0706, 0.0549],
         [0.5725, 0.6471, 0.7098,  ..., 0.0824, 0.0706, 0.0588],
         [0.4863, 0.5725, 0.6745,  ..., 0.0706, 0.0667, 0.0588],
         ...,
         [0.2824, 0.2314, 0.1765,  ..., 0.5961, 0.5882, 0.5804],
         [0.2510, 0.2157, 0.1608,  ..., 0.5882, 0.5765, 0.5647],
         [0.2510, 0.2353, 0.1804,  ..., 0.5843, 0.5725, 0.5608]],

        [[0.6706, 0.7451, 0.7569,  ..., 0.0863, 0.0745, 0.0588],
         [0.6392, 0.7137, 0.7569,  ..., 0.0863, 0.0745, 0.0627],
         [0.5529, 0.6392, 0.7216,  ..., 0.0745, 0.0706, 0.0627],
         ...,
         [0.2157, 0.1647, 0.1098,  ..., 0.4196, 0.4078, 0.4039],
         [0.1843, 0.1490, 0.0941,  ..., 0.4235, 0.4078, 0.4000],
         [0.1843, 0.1686, 0.1137,  ..., 0.4196, 0.4078, 0.3961]],

        [[0.4667, 0.5412, 0.5765,  ..., 0.0667, 0.0549, 0.0392],
         [0.4353, 0.5098, 0.5686,  ..., 0.0667, 0.0549, 0.0431],
         [0.3412, 0.4353, 0.5333,  ..., 0.0549, 0.0510, 0.0431],
         ...,
         [0.1373, 0.0863, 0.0314,  ..., 0.2039, 0.2039, 0.1882],
         [0.1059, 0.0706, 0.0157,  ..., 0.1961, 0.1922, 0.1725],
         [0.1059, 0.0902, 0.0353,  ..., 0.1922, 0.1804, 0.1686]]])

Custom image shape: torch.Size([3, 4032, 3024])

Custom image dtype: torch.float32
</pre></div>
</div>
</div>
</div>
</section>
<section id="predicting-on-custom-images-with-a-trained-pytorch-model">
<h3>11.2 Predicting on custom images with a trained PyTorch model<a class="headerlink" href="#predicting-on-custom-images-with-a-trained-pytorch-model" title="Permalink to this heading">#</a></h3>
<p>Beautiful, it looks like our image data is now in the same format our model was trained on.</p>
<p>Except for one thing…</p>
<p>It’s <code class="docutils literal notranslate"><span class="pre">shape</span></code>.</p>
<p>Our model was trained on images with shape <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">64,</span> <span class="pre">64]</span></code>, whereas our custom image is currently <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">4032,</span> <span class="pre">3024]</span></code>.</p>
<p>How could we make sure our custom image is the same shape as the images our model was trained on?</p>
<p>Are there any <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> that could help?</p>
<p>Before we answer that question, let’s plot the image with <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> to make sure it looks okay, remember we’ll have to permute the dimensions from <code class="docutils literal notranslate"><span class="pre">CHW</span></code> to <code class="docutils literal notranslate"><span class="pre">HWC</span></code> to suit <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>’s requirements.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot custom image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">custom_image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image shape: </span><span class="si">{</span><span class="n">custom_image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/19cde66d2f29da69cc18a87b6965ee4c866e33b7660eac03a7581c1e8f9b62c0.png" src="../../_images/19cde66d2f29da69cc18a87b6965ee4c866e33b7660eac03a7581c1e8f9b62c0.png" />
</div>
</div>
<p>Two thumbs up!</p>
<p>Now how could we get our image to be the same size as the images our model was trained on?</p>
<p>One way to do so is with <code class="docutils literal notranslate"><span class="pre">torchvision.transforms.Resize()</span></code>.</p>
<p>Let’s compose a transform pipeline to do so.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create transform pipleine to resize image</span>
<span class="n">custom_image_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
<span class="p">])</span>

<span class="c1"># Transform target image</span>
<span class="n">custom_image_transformed</span> <span class="o">=</span> <span class="n">custom_image_transform</span><span class="p">(</span><span class="n">custom_image</span><span class="p">)</span>

<span class="c1"># Print out original shape and new shape</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original shape: </span><span class="si">{</span><span class="n">custom_image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;New shape: </span><span class="si">{</span><span class="n">custom_image_transformed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original shape: torch.Size([3, 4032, 3024])
New shape: torch.Size([3, 64, 64])
</pre></div>
</div>
</div>
</div>
<p>Woohoo!</p>
<p>Let’s finally make a prediction on our own custom image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">custom_image_pred</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">(</span><span class="n">custom_image_transformed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="nn">Input In [65],</span> in <span class="ni">&lt;cell line: 2&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">model_1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
<span class="ne">----&gt; </span><span class="mi">3</span>     <span class="n">custom_image_pred</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">(</span><span class="n">custom_image_transformed</span><span class="p">)</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">Input In [41],</span> in <span class="ni">TinyVGG.forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">39</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">40</span>     <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">41</span>     <span class="c1"># print(x.shape)</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span>     <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139,</span> in <span class="ni">Sequential.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">137</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">138</span>     <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">139</span>         <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>     <span class="k">return</span> <span class="nb">input</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:457,</span> in <span class="ni">Conv2d.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">456</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">457</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:453,</span> in <span class="ni">Conv2d._conv_forward</span><span class="nt">(self, input, weight, bias)</span>
<span class="g g-Whitespace">    </span><span class="mi">449</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span>     <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">451</span>                     <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">452</span>                     <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">453</span> <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">454</span>                 <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

<span class="ne">RuntimeError</span>: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)
</pre></div>
</div>
</div>
</div>
<p>Oh my goodness…</p>
<p>Despite our preparations our custom image and model are on different devices.</p>
<p>And we get the error:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Expected</span> <span class="pre">all</span> <span class="pre">tensors</span> <span class="pre">to</span> <span class="pre">be</span> <span class="pre">on</span> <span class="pre">the</span> <span class="pre">same</span> <span class="pre">device,</span> <span class="pre">but</span> <span class="pre">found</span> <span class="pre">at</span> <span class="pre">least</span> <span class="pre">two</span> <span class="pre">devices,</span> <span class="pre">cpu</span> <span class="pre">and</span> <span class="pre">cuda:0!</span> <span class="pre">(when</span> <span class="pre">checking</span> <span class="pre">argument</span> <span class="pre">for</span> <span class="pre">argument</span> <span class="pre">weight</span> <span class="pre">in</span> <span class="pre">method</span> <span class="pre">wrapper___slow_conv2d_forward)</span></code></p>
</div></blockquote>
<p>Let’s fix that by putting our <code class="docutils literal notranslate"><span class="pre">custom_image_transformed</span></code> on the target device.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">custom_image_pred</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">(</span><span class="n">custom_image_transformed</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="nn">Input In [66],</span> in <span class="ni">&lt;cell line: 2&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">model_1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
<span class="ne">----&gt; </span><span class="mi">3</span>     <span class="n">custom_image_pred</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">(</span><span class="n">custom_image_transformed</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">Input In [41],</span> in <span class="ni">TinyVGG.forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span> <span class="c1"># print(x.shape)</span>
<span class="ne">---&gt; </span><span class="mi">44</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">45</span> <span class="c1"># print(x.shape)</span>
<span class="g g-Whitespace">     </span><span class="mi">46</span> <span class="k">return</span> <span class="n">x</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139,</span> in <span class="ni">Sequential.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">137</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">138</span>     <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">139</span>         <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>     <span class="k">return</span> <span class="nb">input</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1126</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1127</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1128</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1129</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1130</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1131</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1132</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">File ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114,</span> in <span class="ni">Linear.forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">113</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">114</span>     <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="ne">RuntimeError</span>: mat1 and mat2 shapes cannot be multiplied (10x256 and 2560x3)
</pre></div>
</div>
</div>
</div>
<p>What now?</p>
<p>It looks like we’re getting a shape error.</p>
<p>Why might this be?</p>
<p>We converted our custom image to be the same size as the images our model was trained on…</p>
<p>Oh wait…</p>
<p>There’s one dimension we forgot about.</p>
<p>The batch size.</p>
<p>Our model expects image tensors with a batch size dimension at the start (<code class="docutils literal notranslate"><span class="pre">NCHW</span></code> where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the batch size).</p>
<p>Except our custom image is currently only <code class="docutils literal notranslate"><span class="pre">CHW</span></code>.</p>
<p>We can add a batch size dimension using <code class="docutils literal notranslate"><span class="pre">torch.unsqueeze(dim=0)</span></code> to add an extra dimension our image and <em>finally</em> make a prediction.</p>
<p>Essentially we’ll be telling our model to predict on a single image (an image with a <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> of 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="c1"># Add an extra dimension to image</span>
    <span class="n">custom_image_transformed_with_batch_size</span> <span class="o">=</span> <span class="n">custom_image_transformed</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># Print out different shapes</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Custom image transformed shape: </span><span class="si">{</span><span class="n">custom_image_transformed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsqueezed custom image shape: </span><span class="si">{</span><span class="n">custom_image_transformed_with_batch_size</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Make a prediction on image with an extra dimension</span>
    <span class="n">custom_image_pred</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">(</span><span class="n">custom_image_transformed</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Custom image transformed shape: torch.Size([3, 64, 64])
Unsqueezed custom image shape: torch.Size([1, 3, 64, 64])
</pre></div>
</div>
</div>
</div>
<p>Yes!!!</p>
<p>It looks like it worked!</p>
<blockquote>
<div><p><strong>Note:</strong> What we’ve just gone through are three of the classical and most common deep learning and PyTorch issues:</p>
<ol class="arabic simple">
<li><p><strong>Wrong datatypes</strong> - our model expects <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> where our original custom image was <code class="docutils literal notranslate"><span class="pre">uint8</span></code>.</p></li>
<li><p><strong>Wrong device</strong> - our model was on the target <code class="docutils literal notranslate"><span class="pre">device</span></code> (in our case, the GPU) whereas our target data hadn’t been moved to the target <code class="docutils literal notranslate"><span class="pre">device</span></code> yet.</p></li>
<li><p><strong>Wrong shapes</strong> - our model expected an input image of shape <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">C,</span> <span class="pre">H,</span> <span class="pre">W]</span></code> or <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">color_channels,</span> <span class="pre">height,</span> <span class="pre">width]</span></code> whereas our custom image tensor was of shape <code class="docutils literal notranslate"><span class="pre">[color_channels,</span> <span class="pre">height,</span> <span class="pre">width]</span></code>.</p></li>
</ol>
<p>Keep in mind, these errors aren’t just for predicting on custom images.</p>
<p>They will be present with almost every kind of data type (text, audio, structured data) and problem you work with.</p>
</div></blockquote>
<p>Now let’s take a look at our model’s predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">custom_image_pred</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.1172,  0.0160, -0.1425]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<p>Alright, these are still in <em>logit form</em> (the raw outputs of a model are called logits).</p>
<p>Let’s convert them from logits -&gt; prediction probabilities -&gt; prediction labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print out prediction logits</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction logits: </span><span class="si">{</span><span class="n">custom_image_pred</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)</span>
<span class="n">custom_image_pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">custom_image_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction probabilities: </span><span class="si">{</span><span class="n">custom_image_pred_probs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Convert prediction probabilities -&gt; prediction labels</span>
<span class="n">custom_image_pred_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">custom_image_pred_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction label: </span><span class="si">{</span><span class="n">custom_image_pred_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prediction logits: tensor([[ 0.1172,  0.0160, -0.1425]], device=&#39;cuda:0&#39;)
Prediction probabilities: tensor([[0.3738, 0.3378, 0.2883]], device=&#39;cuda:0&#39;)
Prediction label: tensor([0], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<p>Alright!</p>
<p>Looking good.</p>
<p>But of course our prediction label is still in index/tensor form.</p>
<p>We can convert it to a string class name prediction by indexing on the <code class="docutils literal notranslate"><span class="pre">class_names</span></code> list.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find the predicted label</span>
<span class="n">custom_image_pred_class</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">[</span><span class="n">custom_image_pred_label</span><span class="o">.</span><span class="n">cpu</span><span class="p">()]</span> <span class="c1"># put pred label to CPU, otherwise will error</span>
<span class="n">custom_image_pred_class</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;pizza&#39;
</pre></div>
</div>
</div>
</div>
<p>Wow.</p>
<p>It looks like the model gets the prediction right, even though it was performing poorly based on our evaluation metrics.</p>
<blockquote>
<div><p><strong>Note:</strong> The model in its current form will predict “pizza”, “steak” or “sushi” no matter what image it’s given. If you wanted your model to predict on a different class, you’d have to train it to do so.</p>
</div></blockquote>
<p>But if we check the <code class="docutils literal notranslate"><span class="pre">custom_image_pred_probs</span></code>, we’ll notice that the model gives almost equal weight (the values are similar) to every class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The values of the prediction probabilities are quite similar</span>
<span class="n">custom_image_pred_probs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.3738, 0.3378, 0.2883]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<p>Having prediction probabilities this similar could mean a couple of things:</p>
<ol class="arabic simple">
<li><p>The model is trying to predict all three classes at the same time (there may be an image containing pizza, steak and sushi).</p></li>
<li><p>The model doesn’t really know what it wants to predict and is in turn just assigning similar values to each of the classes.</p></li>
</ol>
<p>Our case is number 2, since our model is poorly trained, it is basically <em>guessing</em> the prediction.</p>
</section>
<section id="putting-custom-image-prediction-together-building-a-function">
<h3>11.3 Putting custom image prediction together: building a function<a class="headerlink" href="#putting-custom-image-prediction-together-building-a-function" title="Permalink to this heading">#</a></h3>
<p>Doing all of the above steps every time you’d like to make a prediction on a custom image would quickly become tedious.</p>
<p>So let’s put them all together in a function we can easily use over and over again.</p>
<p>Specifically, let’s make a function that:</p>
<ol class="arabic simple">
<li><p>Takes in a target image path and converts to the right datatype for our model (<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>).</p></li>
<li><p>Makes sure the target image pixel values are in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>.</p></li>
<li><p>Transforms the target image if necessary.</p></li>
<li><p>Makes sure the model is on the target device.</p></li>
<li><p>Makes a prediction on the target image with a trained model (ensuring the image is the right size and on the same device as the model).</p></li>
<li><p>Converts the model’s output logits to prediction probabilities.</p></li>
<li><p>Converts the prediction probabilities to prediction labels.</p></li>
<li><p>Plots the target image alongside the model prediction and prediction probability.</p></li>
</ol>
<p>A fair few steps but we’ve got this!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pred_and_plot_image</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> 
                        <span class="n">image_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                        <span class="n">class_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                        <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Makes a prediction on a target image and plots the image with its prediction.&quot;&quot;&quot;</span>
    
    <span class="c1"># 1. Load in image and convert the tensor values to float32</span>
    <span class="n">target_image</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read_image</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">image_path</span><span class="p">))</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    
    <span class="c1"># 2. Divide the image pixel values by 255 to get them between [0, 1]</span>
    <span class="n">target_image</span> <span class="o">=</span> <span class="n">target_image</span> <span class="o">/</span> <span class="mf">255.</span> 
    
    <span class="c1"># 3. Transform if necessary</span>
    <span class="k">if</span> <span class="n">transform</span><span class="p">:</span>
        <span class="n">target_image</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">target_image</span><span class="p">)</span>
    
    <span class="c1"># 4. Make sure the model is on the target device</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># 5. Turn on model evaluation mode and inference mode</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="c1"># Add an extra dimension to the image</span>
        <span class="n">target_image</span> <span class="o">=</span> <span class="n">target_image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
        <span class="c1"># Make a prediction on image with an extra dimension and send it to the target device</span>
        <span class="n">target_image_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">target_image</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        
    <span class="c1"># 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)</span>
    <span class="n">target_image_pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">target_image_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 7. Convert prediction probabilities -&gt; prediction labels</span>
    <span class="n">target_image_pred_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">target_image_pred_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># 8. Plot the image alongside the prediction and prediction probability</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">target_image</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># make sure it&#39;s the right size for matplotlib</span>
    <span class="k">if</span> <span class="n">class_names</span><span class="p">:</span>
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Pred: </span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">target_image_pred_label</span><span class="o">.</span><span class="n">cpu</span><span class="p">()]</span><span class="si">}</span><span class="s2"> | Prob: </span><span class="si">{</span><span class="n">target_image_pred_probs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">else</span><span class="p">:</span> 
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Pred: </span><span class="si">{</span><span class="n">target_image_pred_label</span><span class="si">}</span><span class="s2"> | Prob: </span><span class="si">{</span><span class="n">target_image_pred_probs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>What a nice looking function, let’s test it out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pred on our custom image</span>
<span class="n">pred_and_plot_image</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span>
                    <span class="n">image_path</span><span class="o">=</span><span class="n">custom_image_path</span><span class="p">,</span>
                    <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">custom_image_transform</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2cd42f6b86441ba293d46db6a5e74d0c2274b9d22c4bfcd250b6cdab1007efb8.png" src="../../_images/2cd42f6b86441ba293d46db6a5e74d0c2274b9d22c4bfcd250b6cdab1007efb8.png" />
</div>
</div>
<p>Two thumbs up again!</p>
<p>Looks like our model got the prediction right just by guessing.</p>
<p>This won’t always be the case with other images though…</p>
<p>The image is pixelated too because we resized it to <code class="docutils literal notranslate"><span class="pre">[64,</span> <span class="pre">64]</span></code> using <code class="docutils literal notranslate"><span class="pre">custom_image_transform</span></code>.</p>
<blockquote>
<div><p><strong>Exercise:</strong> Try making a prediction with one of your own images of pizza, steak or sushi and see what happens.</p>
</div></blockquote>
</section>
</section>
<section id="main-takeaways">
<h2>Main takeaways<a class="headerlink" href="#main-takeaways" title="Permalink to this heading">#</a></h2>
<p>We’ve covered a fair bit in this module.</p>
<p>Let’s summarise it with a few dot points.</p>
<ul class="simple">
<li><p>PyTorch has many in-built functions to deal with all kinds of data, from vision to text to audio to recommendation systems.</p></li>
<li><p>If PyTorch’s built-in data loading functions don’t suit your requirements, you can write code to create your own custom datasets by subclassing <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>’s in PyTorch help turn your <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s into iterables that can be used when training and testing a model.</p></li>
<li><p>A lot of machine learning is dealing with the balance between <strong>overfitting</strong> and <strong>underfitting</strong> (we discussed different methods for each above, so a good exercise would be to research more and writing code to try out the different techniques).</p></li>
<li><p>Predicting on your own custom data with a trained model is possible, as long as you format the data into a similar format to what the model was trained on. Make sure you take care of the three big PyTorch and deep learning errors:</p>
<ol class="arabic simple">
<li><p><strong>Wrong datatypes</strong> - Your model expected <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> when your data is <code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code>.</p></li>
<li><p><strong>Wrong data shapes</strong> - Your model expected <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">color_channels,</span> <span class="pre">height,</span> <span class="pre">width]</span></code> when your data is <code class="docutils literal notranslate"><span class="pre">[color_channels,</span> <span class="pre">height,</span> <span class="pre">width]</span></code>.</p></li>
<li><p><strong>Wrong devices</strong> - Your model is on the GPU but your data is on the CPU.</p></li>
</ol>
</li>
</ul>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<p>All of the exercises are focused on practicing the code in the sections above.</p>
<p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p>
<p>All exercises should be completed using <a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">device-agnostic code</a>.</p>
<p><strong>Resources:</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/exercises/04_pytorch_custom_datasets_exercises.ipynb">Exercise template notebook for 04</a></p></li>
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/solutions/04_pytorch_custom_datasets_exercise_solutions.ipynb">Example solutions notebook for 04</a> (try the exercises <em>before</em> looking at this)</p></li>
</ul>
<ol class="arabic simple">
<li><p>Our models are underperforming (not fitting the data well). What are 3 methods for preventing underfitting? Write them down and explain each with a sentence.</p></li>
<li><p>Recreate the data loading functions we built in sections 1, 2, 3 and 4. You should have train and test <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s ready to use.</p></li>
<li><p>Recreate <code class="docutils literal notranslate"><span class="pre">model_0</span></code> we built in section 7.</p></li>
<li><p>Create training and testing functions for <code class="docutils literal notranslate"><span class="pre">model_0</span></code>.</p></li>
<li><p>Try training the model you made in exercise 3 for 5, 20 and 50 epochs, what happens to the results?</p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code> with a learning rate of 0.001 as the optimizer.</p></li>
</ul>
</li>
<li><p>Double the number of hidden units in your model and train it for 20 epochs, what happens to the results?</p></li>
<li><p>Double the data you’re using with your model and train it for 20 epochs, what happens to the results?</p>
<ul class="simple">
<li><p><strong>Note:</strong> You can use the <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb">custom data creation notebook</a> to scale up your Food101 dataset.</p></li>
<li><p>You can also find the <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip">already formatted double data (20% instead of 10% subset) dataset on GitHub</a>, you will need to write download code like in exercise 2 to get it into this notebook.</p></li>
</ul>
</li>
<li><p>Make a prediction on your own custom image of pizza/steak/sushi (you could even download one from the internet) and share your prediction.</p>
<ul class="simple">
<li><p>Does the model you trained in exercise 7 get it right?</p></li>
<li><p>If not, what do you think you could do to improve it?</p></li>
</ul>
</li>
</ol>
</section>
<section id="extra-curriculum">
<h2>Extra-curriculum<a class="headerlink" href="#extra-curriculum" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>To practice your knowledge of PyTorch <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s through PyTorch <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">datasets and dataloaders tutorial notebook</a>.</p></li>
<li><p>Spend 10-minutes reading the <a class="reference external" href="https://pytorch.org/vision/stable/transforms.html">PyTorch <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> documentation</a>.</p>
<ul>
<li><p>You can see demos of transforms in action in the <a class="reference external" href="https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#illustration-of-transforms">illustrations of transforms tutorial</a>.</p></li>
</ul>
</li>
<li><p>Spend 10-minutes reading the PyTorch <a class="reference external" href="https://pytorch.org/vision/stable/datasets.html"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets</span></code> documentation</a>.</p>
<ul>
<li><p>What are some datasets that stand out to you?</p></li>
<li><p>How could you try building a model on these?</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://pytorch.org/data/beta/index.html">TorchData is currently in beta</a> (as of April 2022), it’ll be a future way of loading data in PyTorch, but you can start to check it out now.</p></li>
<li><p>To speed up deep learning models, you can do a few tricks to improve compute, memory and overhead computations, for more read the post <a class="reference external" href="https://horace.io/brrr_intro.html"><em>Making Deep Learning Go Brrrr From First Principles</em></a> by Horace He.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook/pytorch_deep_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_pytorch_computer_vision.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">03. PyTorch Computer Vision</p>
      </div>
    </a>
    <a class="right-next"
       href="05_pytorch_going_modular.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">05. PyTorch Going Modular</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-custom-dataset">What is a custom dataset?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-were-going-to-cover">What we’re going to cover</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-can-can-you-get-help">Where can can you get help?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-pytorch-and-setting-up-device-agnostic-code">0. Importing PyTorch and setting up device-agnostic code</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-data">1. Get data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#become-one-with-the-data-data-preparation">2. Become one with the data (data preparation)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-an-image">2.1 Visualize an image</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-data">3. Transforming data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-data-with-torchvision-transforms">3.1 Transforming data with <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#option-1-loading-image-data-using-imagefolder">4. Option 1: Loading Image Data Using <code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-loaded-images-into-dataloaders">4.1 Turn loaded images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#option-2-loading-image-data-with-a-custom-dataset">5. Option 2: Loading Image Data with a Custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-helper-function-to-get-class-names">5.1 Creating a helper function to get class names</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-custom-dataset-to-replicate-imagefolder">5.2 Create a custom <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> to replicate <code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-function-to-display-random-images">5.3 Create a function to display random images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-custom-loaded-images-into-dataloaders">5.4 Turn custom loaded images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-forms-of-transforms-data-augmentation">6. Other forms of transforms (data augmentation)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-0-tinyvgg-without-data-augmentation">7. Model 0: TinyVGG without data augmentation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-transforms-and-loading-data-for-model-0">7.1 Creating transforms and loading data for Model 0</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-tinyvgg-model-class">7.2 Create TinyVGG model class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#try-a-forward-pass-on-a-single-image-to-test-the-model">7.3 Try a forward pass on a single image (to test the model)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-torchinfo-to-get-an-idea-of-the-shapes-going-through-our-model">7.4 Use <code class="docutils literal notranslate"><span class="pre">torchinfo</span></code> to get an idea of the shapes going through our model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-train-test-loop-functions">7.5 Create train &amp; test loop functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-train-function-to-combine-train-step-and-test-step">7.6 Creating a <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to combine <code class="docutils literal notranslate"><span class="pre">train_step()</span></code> and <code class="docutils literal notranslate"><span class="pre">test_step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-and-evaluate-model-0">7.7 Train and Evaluate Model 0</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss-curves-of-model-0">7.8 Plot the loss curves of Model 0</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-should-an-ideal-loss-curve-look-like">8. What should an ideal loss curve look like?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-overfitting">8.1 How to deal with overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-deal-with-underfitting">8.2 How to deal with underfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-balance-between-overfitting-and-underfitting">8.3 The balance between overfitting and underfitting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1-tinyvgg-with-data-augmentation">9. Model 1: TinyVGG with Data Augmentation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-transform-with-data-augmentation">9.1 Create transform with data augmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-train-and-test-datasets-and-dataloaders">9.2 Create train and test <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>’s and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#construct-and-train-model-1">9.3 Construct and train Model 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss-curves-of-model-1">9.4 Plot the loss curves of Model 1</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#compare-model-results">10. Compare model results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-a-prediction-on-a-custom-image">11. Make a prediction on a custom image</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-in-a-custom-image-with-pytorch">11.1 Loading in a custom image with PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-on-custom-images-with-a-trained-pytorch-model">11.2 Predicting on custom images with a trained PyTorch model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-custom-image-prediction-together-building-a-function">11.3 Putting custom image prediction together: building a function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-takeaways">Main takeaways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extra-curriculum">Extra-curriculum</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By thangckt
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>