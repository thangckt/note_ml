

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>07. PyTorch Experiment Tracking</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebook/pytorch_deep_learning/07_pytorch_experiment_tracking';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="08. PyTorch Paper Replicating" href="08_pytorch_paper_replicating.html" />
    <link rel="prev" title="06. PyTorch Transfer Learning" href="06_pytorch_transfer_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
  
    <p class="title logo__title"></p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic of ML &amp; DL</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../0_basic_MLDL/1_0_ml_overview.html">Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/1_1_ml_supervised_unsuppersives.html">Supervised vs. Unsuppervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/1_2_regression.html">Regression &amp; Model Assessment</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../0_basic_MLDL/2_0_dl_overview.html">Deep Learning Overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/2_1_dl_neural_network.html">What is a neural network?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/2_2_layers.html">Standard Layers</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../0_basic_MLDL/3_1_workflow.html">Workflow in ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_basic_MLDL/3_2_Model_template.html">Core Ml templates</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PyTorch for Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="00_pytorch_fundamentals.html">00. PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_pytorch_workflow.html">01. PyTorch Workflow Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_pytorch_classification.html">02. PyTorch Neural Network Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_pytorch_computer_vision.html">03. PyTorch Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_pytorch_custom_datasets.html">04. PyTorch Custom Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_pytorch_going_modular.html">05. PyTorch Going Modular</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_pytorch_transfer_learning.html">06. PyTorch Transfer Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">07. PyTorch Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_pytorch_paper_replicating.html">08. PyTorch Paper Replicating</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_pytorch_model_deployment.html">09. PyTorch Model Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_extra_resources.html">PyTorch Extra Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_cheatsheet.html">PyTorch Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_most_common_errors.html">The Three Most Common Errors in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_setup.html">Setup to code PyTorch</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Zero to Mastery Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../zero_to_mastery_ml/README.html">Zero to Mastery Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Practices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1_Practices/1_PT_Linear_Regression.html">Linear Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_Practices/2_PT_Logistic_Regression.html">Logistic Regression</a></li>





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/thangckt/note_ml/edit/main/notebook/pytorch_deep_learning/07_pytorch_experiment_tracking.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button"
   title="Suggest edit"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>

</a>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>07. PyTorch Experiment Tracking</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-experiment-tracking">What is experiment tracking?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-track-experiments">Why track experiments?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-ways-to-track-machine-learning-experiments">Different ways to track machine learning experiments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-were-going-to-cover">What we’re going to cover</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-can-you-get-help">Where can you get help?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-setup">0. Getting setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-helper-function-to-set-seeds">Create a helper function to set seeds</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-data">1. Get data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-datasets-and-dataloaders">2. Create Datasets and DataLoaders</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataloaders-using-manually-created-transforms">2.1 Create DataLoaders using manually created transforms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataloaders-using-automatically-created-transforms">2.2 Create DataLoaders using automatically created transforms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head">3. Getting a pretrained model, freezing the base layers and changing the classifier head</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-model-and-track-results">4. Train model and track results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjust-train-function-to-track-results-with-summarywriter">Adjust <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to track results with <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#view-our-models-results-in-tensorboard">5. View our model’s results in TensorBoard</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-helper-function-to-build-summarywriter-instances">6. Create a helper function to build <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> instances</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-the-train-function-to-include-a-writer-parameter">6.1 Update the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to include a <code class="docutils literal notranslate"><span class="pre">writer</span></code> parameter</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-a-series-of-modelling-experiments">7. Setting up a series of modelling experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-kind-of-experiments-should-you-run">7.1 What kind of experiments should you run?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-experiments-are-we-going-to-run">7.2 What experiments are we going to run?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-different-datasets">7.3 Download different datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transform-datasets-and-create-dataloaders">7.4 Transform Datasets and create DataLoaders</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-feature-extractor-models">7.5 Create feature extractor models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-experiments-and-set-up-training-code">7.6 Create experiments and set up training code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#view-experiments-in-tensorboard">8. View experiments in TensorBoard</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-in-the-best-model-and-make-predictions-with-it">9. Load in the best model and make predictions with it</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-on-a-custom-image-with-the-best-model">9.1 Predict on a custom image with the best model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-takeaways">Main takeaways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extra-curriculum">Extra-curriculum</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/thangckt/pytorch-deep-learning/blob/main/07_pytorch_experiment_tracking.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p><a class="reference external" href="https://thangckt.github.io/pytorch_deep_learning/slides/07_pytorch_experiment_tracking.pdf">View Slides</a></p>
<section class="tex2jax_ignore mathjax_ignore" id="pytorch-experiment-tracking">
<h1>07. PyTorch Experiment Tracking<a class="headerlink" href="#pytorch-experiment-tracking" title="Permalink to this heading">#</a></h1>
<blockquote>
<div><p><strong>Note:</strong> This notebook uses <code class="docutils literal notranslate"><span class="pre">torchvision</span></code>’s new <a class="reference external" href="https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/">multi-weight support API (available in <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> v0.13+)</a>.</p>
</div></blockquote>
<p>We’ve trained a fair few models now on the journey to making FoodVision Mini (an image classification model to classify images of pizza, steak or sushi).</p>
<p>And so far we’ve keep track of them via Python dictionaries.</p>
<p>Or just comparing them by the metric print outs during training.</p>
<p>What if you wanted to run a dozen (or more) different models at once?</p>
<p>Surely there’s a better way…</p>
<p>There is.</p>
<p><strong>Experiment tracking.</strong></p>
<p>And since experiment tracking is so important and integral to machine learning, you can consider this notebook your first milestone project.</p>
<p>So welcome to Milestone Project 1: FoodVision Mini Experiment Tracking.</p>
<p>We’re going to answer the question: <strong>how do I track my machine learning experiments?</strong></p>
<section id="what-is-experiment-tracking">
<h2>What is experiment tracking?<a class="headerlink" href="#what-is-experiment-tracking" title="Permalink to this heading">#</a></h2>
<p>Machine learning and deep learning are very experimental.</p>
<p>You have to put on your artist’s beret/chef’s hat to cook up lots of different models.</p>
<p>And you have to put on your scientist’s coat to track the results of various combinations of data, model architectures and training regimes.</p>
<p>That’s where <strong>experiment tracking</strong> comes in.</p>
<p>If you’re running lots of different experiments, <strong>experiment tracking helps you figure out what works and what doesn’t</strong>.</p>
</section>
<section id="why-track-experiments">
<h2>Why track experiments?<a class="headerlink" href="#why-track-experiments" title="Permalink to this heading">#</a></h2>
<p>If you’re only running a handful of models (like we’ve done so far), it might be okay just to track their results in print outs and a few dictionaries.</p>
<p>However, as the number of experiments you run starts to increase, this naive way of tracking could get out of hand.</p>
<p>So if you’re following the machine learning practitioner’s motto of <em>experiment, experiment, experiment!</em>, you’ll want a way to track them.</p>
<img alt="experiment tracking can get out of hand, many different experiments with different names" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/07-experiment-tracking-can-get-out-of-hand.png" />
<p><em>After building a few models and tracking their results, you’ll start to notice how quickly it can get out of hand.</em></p>
</section>
<section id="different-ways-to-track-machine-learning-experiments">
<h2>Different ways to track machine learning experiments<a class="headerlink" href="#different-ways-to-track-machine-learning-experiments" title="Permalink to this heading">#</a></h2>
<p>There are as many different ways to track machine learning experiments as there is experiments to run.</p>
<p>This table covers a few.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Method</strong></p></th>
<th class="head"><p><strong>Setup</strong></p></th>
<th class="head"><p><strong>Pros</strong></p></th>
<th class="head"><p><strong>Cons</strong></p></th>
<th class="head"><p><strong>Cost</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Python dictionaries, CSV files, print outs</p></td>
<td><p>None</p></td>
<td><p>Easy to setup, runs in pure Python</p></td>
<td><p>Hard to keep track of large numbers of experiments</p></td>
<td><p>Free</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.tensorflow.org/tensorboard/get_started">TensorBoard</a></p></td>
<td><p>Minimal, install <a class="reference external" href="https://pypi.org/project/tensorboard/"><code class="docutils literal notranslate"><span class="pre">tensorboard</span></code></a></p></td>
<td><p>Extensions built into PyTorch, widely recognized and used, easily scales.</p></td>
<td><p>User-experience not as nice as other options.</p></td>
<td><p>Free</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://wandb.ai/site/experiment-tracking">Weights &amp; Biases Experiment Tracking</a></p></td>
<td><p>Minimal, install <a class="reference external" href="https://docs.wandb.ai/quickstart"><code class="docutils literal notranslate"><span class="pre">wandb</span></code></a>, make an account</p></td>
<td><p>Incredible user experience, make experiments public, tracks almost anything.</p></td>
<td><p>Requires external resource outside of PyTorch.</p></td>
<td><p>Free for personal use</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://mlflow.org/">MLFlow</a></p></td>
<td><p>Minimal, install <code class="docutils literal notranslate"><span class="pre">mlflow</span></code> and starting tracking</p></td>
<td><p>Fully open-source MLOps lifecycle management, many integrations.</p></td>
<td><p>Little bit harder to setup a remote tracking server than other services.</p></td>
<td><p>Free</p></td>
</tr>
</tbody>
</table>
</div>
<img alt="various places to track machine learning experiments" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/07-different-places-to-track-experiments.png" />
<p><em>Various places and techniques you can use to track your machine learning experiments. <strong>Note:</strong> There are various other options similar to Weights &amp; Biases and open-source options similar to MLflow but I’ve left them out for brevity. You can find more by searching “machine learning experiment tracking”.</em></p>
</section>
<section id="what-were-going-to-cover">
<h2>What we’re going to cover<a class="headerlink" href="#what-were-going-to-cover" title="Permalink to this heading">#</a></h2>
<p>We’re going to be running several different modelling experiments with various levels of data, model size and training time to try and improve on FoodVision Mini.</p>
<p>And due to its tight integration with PyTorch and widespread use, this notebook focuses on using TensorBoard to track our experiments.</p>
<p>However, the principles we’re going to cover are similar across all of the other tools for experiment tracking.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Topic</strong></p></th>
<th class="head"><p><strong>Contents</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>0. Getting setup</strong></p></td>
<td><p>We’ve written a fair bit of useful code over the past few sections, let’s download it and make sure we can use it again.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>1. Get data</strong></p></td>
<td><p>Let’s get the pizza, steak and sushi image classification dataset we’ve been using to try and improve our FoodVision Mini model’s results.</p></td>
</tr>
<tr class="row-even"><td><p><strong>2. Create Datasets and DataLoaders</strong></p></td>
<td><p>We’ll use the <code class="docutils literal notranslate"><span class="pre">data_setup.py</span></code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>3. Get and customise a pretrained model</strong></p></td>
<td><p>Just like the last section, 06. PyTorch Transfer Learning we’ll download a pretrained model from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> and customise it to our own problem.</p></td>
</tr>
<tr class="row-even"><td><p><strong>4. Train model amd track results</strong></p></td>
<td><p>Let’s see what it’s like to train and track the training results of a single model using TensorBoard.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>5. View our model’s results in TensorBoard</strong></p></td>
<td><p>Previously we visualized our model’s loss curves with a helper function, now let’s see what they look like in TensorBoard.</p></td>
</tr>
<tr class="row-even"><td><p><strong>6. Creating a helper function to track experiments</strong></p></td>
<td><p>If we’re going to be adhering to the machine learner practitioner’s motto of <em>experiment, experiment, experiment!</em>, we best create a function that will help us save our modelling experiment results.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>7. Setting up a series of modelling experiments</strong></p></td>
<td><p>Instead of running experiments one by one, how about we write some code to run several experiments at once, with different models, different amounts of data and different training times.</p></td>
</tr>
<tr class="row-even"><td><p><strong>8. View modelling experiments in TensorBoard</strong></p></td>
<td><p>By this stage we’ll have run eight modelling experiments in one go, a fair bit to keep track of, let’s what their results look like in TensorBoard.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>9. Load in the best model and make predictions with it</strong></p></td>
<td><p>The point of experiment tracking is to figure out which model performs the best, let’s load in the best performing model and make some predictions with it to <em>visualize, visualize, visualize!</em>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="where-can-you-get-help">
<h2>Where can you get help?<a class="headerlink" href="#where-can-you-get-help" title="Permalink to this heading">#</a></h2>
<p>All of the materials for this course <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning">are available on GitHub</a>.</p>
<p>If you run into trouble, you can ask a question on the course <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/discussions">GitHub Discussions page</a>.</p>
<p>And of course, there’s the <a class="reference external" href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a> and <a class="reference external" href="https://discuss.pytorch.org/">PyTorch developer forums</a>, a very helpful place for all things PyTorch.</p>
</section>
<section id="getting-setup">
<h2>0. Getting setup<a class="headerlink" href="#getting-setup" title="Permalink to this heading">#</a></h2>
<p>Let’s start by downloading all of the modules we’ll need for this section.</p>
<p>To save us writing extra code, we’re going to be leveraging some of the Python scripts (such as <code class="docutils literal notranslate"><span class="pre">data_setup.py</span></code> and <code class="docutils literal notranslate"><span class="pre">engine.py</span></code>) we created in section, <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/">05. PyTorch Going Modular</a>.</p>
<p>Specifically, we’re going to download the <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/tree/main/going_modular"><code class="docutils literal notranslate"><span class="pre">going_modular</span></code></a> directory from the <code class="docutils literal notranslate"><span class="pre">pytorch-deep-learning</span></code> repository (if we don’t already have it).</p>
<p>We’ll also get the <a class="reference external" href="https://github.com/TylerYep/torchinfo"><code class="docutils literal notranslate"><span class="pre">torchinfo</span></code></a> package if it’s not available.</p>
<p><code class="docutils literal notranslate"><span class="pre">torchinfo</span></code> will help later on to give us visual summaries of our model(s).</p>
<p>And since we’re using a newer version of the <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> package (v0.13 as of June 2022), we’ll make sure we’ve got the latest versions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">import</span> <span class="nn">torchvision</span>
    <span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">12</span><span class="p">,</span> <span class="s2">&quot;torch version should be 1.12+&quot;</span>
    <span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">13</span><span class="p">,</span> <span class="s2">&quot;torchvision version should be 0.13+&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torchvision version: </span><span class="si">{</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] torch/torchvision versions not as required, installing nightly versions.&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip3<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--extra-index-url<span class="w"> </span>https://download.pytorch.org/whl/cu113
    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">import</span> <span class="nn">torchvision</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torchvision version: </span><span class="si">{</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch version: 1.13.0.dev20220620+cu113
torchvision version: 0.14.0.dev20220620+cu113
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> If you’re using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you’ve got the right versions of <code class="docutils literal notranslate"><span class="pre">torch</span></code> (0.12+) and <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> (0.13+).</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Continue with regular imports</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="c1"># Try to get torchinfo, install it if it doesn&#39;t work</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO] Couldn&#39;t find torchinfo... installing it.&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-q<span class="w"> </span>torchinfo
    <span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="c1"># Try to import the going_modular directory, download it from GitHub if it doesn&#39;t work</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">going_modular.going_modular</span> <span class="kn">import</span> <span class="n">data_setup</span><span class="p">,</span> <span class="n">engine</span>
<span class="k">except</span><span class="p">:</span>
    <span class="c1"># Get the going_modular scripts</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO] Couldn&#39;t find going_modular scripts... downloading them from GitHub.&quot;</span><span class="p">)</span>
    <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/thangckt/pytorch-deep-learning
    <span class="o">!</span>mv<span class="w"> </span>pytorch-deep-learning/going_modular<span class="w"> </span>.
    <span class="o">!</span>rm<span class="w"> </span>-rf<span class="w"> </span>pytorch-deep-learning
    <span class="kn">from</span> <span class="nn">going_modular.going_modular</span> <span class="kn">import</span> <span class="n">data_setup</span><span class="p">,</span> <span class="n">engine</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s setup device agnostic code.</p>
<blockquote>
<div><p><strong>Note:</strong> If you’re using Google Colab, and you don’t have a GPU turned on yet, it’s now time to turn one on via <code class="docutils literal notranslate"><span class="pre">Runtime</span> <span class="pre">-&gt;</span> <span class="pre">Change</span> <span class="pre">runtime</span> <span class="pre">type</span> <span class="pre">-&gt;</span> <span class="pre">Hardware</span> <span class="pre">accelerator</span> <span class="pre">-&gt;</span> <span class="pre">GPU</span></code>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
<section id="create-a-helper-function-to-set-seeds">
<h3>Create a helper function to set seeds<a class="headerlink" href="#create-a-helper-function-to-set-seeds" title="Permalink to this heading">#</a></h3>
<p>Since we’ve been setting random seeds a whole bunch throughout previous sections, how about we functionize it?</p>
<p>Let’s create a function to “set the seeds” called <code class="docutils literal notranslate"><span class="pre">set_seeds()</span></code>.</p>
<blockquote>
<div><p><strong>Note:</strong> Recall a <a class="reference external" href="https://en.wikipedia.org/wiki/Random_seed">random seed</a> is a way of flavouring the randomness generated by a computer. They aren’t necessary to always set when running machine learning code, however, they help ensure there’s an element of reproducibility (the numbers I get with my code are similar to the numbers you get with your code). Outside of an education or experimental setting, random seeds generally aren’t required.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set seeds</span>
<span class="k">def</span> <span class="nf">set_seeds</span><span class="p">(</span><span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets random sets for torch operations.</span>

<span class="sd">    Args:</span>
<span class="sd">        seed (int, optional): Random seed to set. Defaults to 42.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set the seed for general torch operations</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1"># Set the seed for CUDA torch operations (ones that happen on the GPU)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="get-data">
<h2>1. Get data<a class="headerlink" href="#get-data" title="Permalink to this heading">#</a></h2>
<p>As always, before we can run machine learning experiments, we’ll need a dataset.</p>
<p>We’re going to continue trying to improve upon the results we’ve been getting on FoodVision Mini.</p>
<p>In the previous section, <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/">06. PyTorch Transfer Learning</a>, we saw how powerful using a pretrained model and transfer learning could be when classifying images of pizza, steak and sushi.</p>
<p>So how about we run some experiments and try to further improve our results?</p>
<p>To do so, we’ll use similar code to the previous section to download the <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip"><code class="docutils literal notranslate"><span class="pre">pizza_steak_sushi.zip</span></code></a> (if the data doesn’t already exist) except this time its been functionised.</p>
<p>This will allow us to use it again later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">zipfile</span>

<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">download_data</span><span class="p">(</span><span class="n">source</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                  <span class="n">destination</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                  <span class="n">remove_source</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Downloads a zipped dataset from source and unzips to destination.</span>

<span class="sd">    Args:</span>
<span class="sd">        source (str): A link to a zipped file containing data.</span>
<span class="sd">        destination (str): A target directory to unzip data to.</span>
<span class="sd">        remove_source (bool): Whether to remove the source after downloading and extracting.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        pathlib.Path to downloaded data.</span>
<span class="sd">    </span>
<span class="sd">    Example usage:</span>
<span class="sd">        download_data(source=&quot;https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip&quot;,</span>
<span class="sd">                      destination=&quot;pizza_steak_sushi&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Setup path to data folder</span>
    <span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;data/&quot;</span><span class="p">)</span>
    <span class="n">image_path</span> <span class="o">=</span> <span class="n">data_path</span> <span class="o">/</span> <span class="n">destination</span>

    <span class="c1"># If the image folder doesn&#39;t exist, download it and prepare it... </span>
    <span class="k">if</span> <span class="n">image_path</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] </span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="s2"> directory exists, skipping download.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Did not find </span><span class="si">{</span><span class="n">image_path</span><span class="si">}</span><span class="s2"> directory, creating one...&quot;</span><span class="p">)</span>
        <span class="n">image_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Download pizza, steak, sushi data</span>
        <span class="n">target_file</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">source</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span> <span class="o">/</span> <span class="n">target_file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Downloading </span><span class="si">{</span><span class="n">target_file</span><span class="si">}</span><span class="s2"> from </span><span class="si">{</span><span class="n">source</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

        <span class="c1"># Unzip pizza, steak, sushi data</span>
        <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">data_path</span> <span class="o">/</span> <span class="n">target_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_ref</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Unzipping </span><span class="si">{</span><span class="n">target_file</span><span class="si">}</span><span class="s2"> data...&quot;</span><span class="p">)</span> 
            <span class="n">zip_ref</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>

        <span class="c1"># Remove .zip file</span>
        <span class="k">if</span> <span class="n">remove_source</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">data_path</span> <span class="o">/</span> <span class="n">target_file</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">image_path</span>

<span class="n">image_path</span> <span class="o">=</span> <span class="n">download_data</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s2">&quot;https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip&quot;</span><span class="p">,</span>
                           <span class="n">destination</span><span class="o">=</span><span class="s2">&quot;pizza_steak_sushi&quot;</span><span class="p">)</span>
<span class="n">image_path</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] data/pizza_steak_sushi directory exists, skipping download.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PosixPath(&#39;data/pizza_steak_sushi&#39;)
</pre></div>
</div>
</div>
</div>
<p>Excellent! Looks like we’ve got our pizza, steak and sushi images in standard image classification format ready to go.</p>
</section>
<section id="create-datasets-and-dataloaders">
<h2>2. Create Datasets and DataLoaders<a class="headerlink" href="#create-datasets-and-dataloaders" title="Permalink to this heading">#</a></h2>
<p>Now we’ve got some data, let’s turn it into PyTorch DataLoaders.</p>
<p>We can do so using the <code class="docutils literal notranslate"><span class="pre">create_dataloaders()</span></code> function we created in <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy">05. PyTorch Going Modular part 2</a>.</p>
<p>And since we’ll be using transfer learning and specifically pretrained models from <a class="reference external" href="https://pytorch.org/vision/stable/models.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a>, we’ll create a transform to prepare our images correctly.</p>
<p>To transform our images in tensors, we can use:</p>
<ol class="arabic simple">
<li><p>Manually created transforms using <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code>.</p></li>
<li><p>Automatically created transforms using <code class="docutils literal notranslate"><span class="pre">torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms()</span></code>.</p>
<ul class="simple">
<li><p>Where <code class="docutils literal notranslate"><span class="pre">MODEL_NAME</span></code> is a specific <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> architecture, <code class="docutils literal notranslate"><span class="pre">MODEL_WEIGHTS</span></code> is a specific set of pretrained weights and <code class="docutils literal notranslate"><span class="pre">DEFAULT</span></code> means the “best available weights”.</p></li>
</ul>
</li>
</ol>
<p>We saw an example of each of these in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#2-create-datasets-and-dataloaders">06. PyTorch Transfer Learning section 2</a>.</p>
<p>Let’s see first an example of manually creating a <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> pipeline (creating a transforms pipeline this way gives the most customization but can potentially result in performance degradation if the transforms don’t match the pretrained model).</p>
<p>The main manual transformation we need to be sure of is that all of our images are normalized in ImageNet format (this is because pretrained <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> are all pretrained on <a class="reference external" href="https://www.image-net.org/">ImageNet</a>).</p>
<p>We can do this with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                 <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</pre></div>
</div>
<section id="create-dataloaders-using-manually-created-transforms">
<h3>2.1 Create DataLoaders using manually created transforms<a class="headerlink" href="#create-dataloaders-using-manually-created-transforms" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup directories</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;train&quot;</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;test&quot;</span>

<span class="c1"># Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)</span>
<span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                 <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>

<span class="c1"># Create transform pipeline manually</span>
<span class="n">manual_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">normalize</span>
<span class="p">])</span>           
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Manually created transforms: </span><span class="si">{</span><span class="n">manual_transforms</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Create data loaders</span>
<span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span> <span class="o">=</span> <span class="n">data_setup</span><span class="o">.</span><span class="n">create_dataloaders</span><span class="p">(</span>
    <span class="n">train_dir</span><span class="o">=</span><span class="n">train_dir</span><span class="p">,</span>
    <span class="n">test_dir</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">manual_transforms</span><span class="p">,</span> <span class="c1"># use manually created transforms</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span>
<span class="p">)</span>

<span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Manually created transforms: Compose(
    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d218e0&gt;,
 &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d216a0&gt;,
 [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;])
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-dataloaders-using-automatically-created-transforms">
<h3>2.2 Create DataLoaders using automatically created transforms<a class="headerlink" href="#create-dataloaders-using-automatically-created-transforms" title="Permalink to this heading">#</a></h3>
<p>Data transformed and DataLoaders created!</p>
<p>Let’s now see what the same transformation pipeline looks like but this time by using automatic transforms.</p>
<p>We can do this by first instantiating a set of pretrained weights (for example <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">torchvision.models.EfficientNet_B0_Weights.DEFAULT</span></code>)  we’d like to use and calling the <code class="docutils literal notranslate"><span class="pre">transforms()</span></code> method on it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup dirs</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;train&quot;</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;test&quot;</span>

<span class="c1"># Setup pretrained weights (plenty of these available in torchvision.models)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">EfficientNet_B0_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>

<span class="c1"># Get transforms from weights (these are the transforms that were used to obtain the weights)</span>
<span class="n">automatic_transforms</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Automatically created transforms: </span><span class="si">{</span><span class="n">automatic_transforms</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Create data loaders</span>
<span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span> <span class="o">=</span> <span class="n">data_setup</span><span class="o">.</span><span class="n">create_dataloaders</span><span class="p">(</span>
    <span class="n">train_dir</span><span class="o">=</span><span class="n">train_dir</span><span class="p">,</span>
    <span class="n">test_dir</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">automatic_transforms</span><span class="p">,</span> <span class="c1"># use automatic created transforms</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span>
<span class="p">)</span>

<span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Automatically created transforms: ImageClassification(
    crop_size=[224]
    resize_size=[256]
    mean=[0.485, 0.456, 0.406]
    std=[0.229, 0.224, 0.225]
    interpolation=InterpolationMode.BICUBIC
)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d213a0&gt;,
 &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d21490&gt;,
 [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head">
<h2>3. Getting a pretrained model, freezing the base layers and changing the classifier head<a class="headerlink" href="#getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head" title="Permalink to this heading">#</a></h2>
<p>Before we run and track multiple modelling experiments, let’s see what it’s like to run and track a single one.</p>
<p>And since our data is ready, the next thing we’ll need is a model.</p>
<p>Let’s download the pretrained weights for a <code class="docutils literal notranslate"><span class="pre">torchvision.models.efficientnet_b0()</span></code> model and prepare it for use with our own data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions.</span>
<span class="c1"># model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD </span>

<span class="c1"># Download the pretrained weights for EfficientNet_B0</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">EfficientNet_B0_Weights</span><span class="o">.</span><span class="n">DEFAULT</span> <span class="c1"># NEW in torchvision 0.13, &quot;DEFAULT&quot; means &quot;best weights available&quot;</span>

<span class="c1"># Setup the model with the pretrained weights and send it to the target device</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b0</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># View the output of the model</span>
<span class="c1"># model</span>
</pre></div>
</div>
</div>
</div>
<p>Wonderful!</p>
<p>Now we’ve got a pretrained model let’s turn into a feature extractor model.</p>
<p>In essence, we’ll freeze the base layers of the model (we’ll use these to extract features from our input images) and we’ll change the classifier head (output layer) to suit the number of classes we’re working with (we’ve got 3 classes: pizza, steak, sushi).</p>
<blockquote>
<div><p><strong>Note:</strong> The idea of creating a feature extractor model (what we’re doing here) was covered in more depth in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#32-setting-up-a-pretrained-model">06. PyTorch Transfer Learning section 3.2: Setting up a pretrained model</a>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Freeze all base layers by setting requires_grad attribute to False</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    
<span class="c1"># Since we&#39;re creating a new layer with random weights (torch.nn.Linear), </span>
<span class="c1"># let&#39;s set the seeds</span>
<span class="n">set_seeds</span><span class="p">()</span> 

<span class="c1"># Update the classifier head to suit our problem</span>
<span class="n">model</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span> 
              <span class="n">out_features</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">),</span>
              <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Base layers frozen, classifier head changed, let’s get a summary of our model with <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="c1"># # Get a summary of the model (uncomment for full output)</span>
<span class="c1"># summary(model, </span>
<span class="c1">#         input_size=(32, 3, 224, 224), # make sure this is &quot;input_size&quot;, not &quot;input_shape&quot; (batch_size, color_channels, height, width)</span>
<span class="c1">#         verbose=0,</span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;]</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
</div>
<img alt="output of torchinfo.summary() when passed our model when base layers are frozen and classifier head is updated" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/07-output-of-torchinfo-summary.png" />
<p><em>Output of <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code> with our feature extractor EffNetB0 model, notice how the base layers are frozen (not trainable) and the output layers are customized to our own problem.</em></p>
</section>
<section id="train-model-and-track-results">
<h2>4. Train model and track results<a class="headerlink" href="#train-model-and-track-results" title="Permalink to this heading">#</a></h2>
<p>Model ready to go!</p>
<p>Let’s get ready to train it by creating a loss function and an optimizer.</p>
<p>Since we’re working with multiple classes, we’ll use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss()</span></code></a> as the loss function.</p>
<p>And we’ll stick with <a class="reference external" href="https://pytorch.org/docs/stable/optim.html"><code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code></a> with learning rate of <code class="docutils literal notranslate"><span class="pre">0.001</span></code> for the optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define loss and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="adjust-train-function-to-track-results-with-summarywriter">
<h3>Adjust <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to track results with <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code><a class="headerlink" href="#adjust-train-function-to-track-results-with-summarywriter" title="Permalink to this heading">#</a></h3>
<p>Beautiful!</p>
<p>All of the pieces of our training code are starting to come together.</p>
<p>Let’s now add the final piece to track our experiments.</p>
<p>Previously, we’ve tracked our modelling experiments using multiple Python dictionaries (one for each model).</p>
<p>But you can imagine this could get out of hand if we were running anything more than a few experiments.</p>
<p>Not to worry, there’s a better option!</p>
<p>We can use PyTorch’s <a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html"><code class="docutils literal notranslate"><span class="pre">torch.utils.tensorboard.SummaryWriter()</span></code></a> class to save various parts of our model’s training progress to file.</p>
<p>By default, the <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> class saves various information about our model to a file set by the <code class="docutils literal notranslate"><span class="pre">log_dir</span></code> parameter.</p>
<p>The default location for <code class="docutils literal notranslate"><span class="pre">log_dir</span></code> is under <code class="docutils literal notranslate"><span class="pre">runs/CURRENT_DATETIME_HOSTNAME</span></code>, where the <code class="docutils literal notranslate"><span class="pre">HOSTNAME</span></code> is the name of your computer.</p>
<p>But of course, you can change where your experiments are tracked (the filename is as customisable as you’d like).</p>
<p>The outputs of the <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> are saved in <a class="reference external" href="https://www.tensorflow.org/tensorboard/">TensorBoard format</a>.</p>
<p>TensorBoard is a part of the TensorFlow deep learning library and is an excellent way to visualize different parts of your model.</p>
<p>To start tracking our modelling experiments, let’s create a default <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> instance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>

<span class="c1"># Create a writer with all default settings</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now to use the writer, we could write a new training loop or we could adjust the existing <code class="docutils literal notranslate"><span class="pre">train()</span></code> function we created in <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them">05. PyTorch Going Modular section 4</a>.</p>
<p>Let’s take the latter option.</p>
<p>We’ll get the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function from <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py"><code class="docutils literal notranslate"><span class="pre">engine.py</span></code></a> and adjust it to use <code class="docutils literal notranslate"><span class="pre">writer</span></code>.</p>
<p>Specifically, we’ll add the ability for our <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to log our model’s training and test loss and accuracy values.</p>
<p>We can do this with <a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars"><code class="docutils literal notranslate"><span class="pre">writer.add_scalars(main_tag,</span> <span class="pre">tag_scalar_dict)</span></code></a>, where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">main_tag</span></code> (string) - the name for the scalars being tracked (e.g. “Accuracy”)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tag_scalar_dict</span></code> (dict) - a dictionary of the values being tracked (e.g. <code class="docutils literal notranslate"><span class="pre">{&quot;train_loss&quot;:</span> <span class="pre">0.3454}</span></code>)</p>
<ul>
<li><blockquote>
<div><p><strong>Note:</strong> The method is called <code class="docutils literal notranslate"><span class="pre">add_scalars()</span></code> because our loss and accuracy values are generally scalars (single values).</p>
</div></blockquote>
</li>
</ul>
</li>
</ul>
<p>Once we’ve finished tracking values, we’ll call <code class="docutils literal notranslate"><span class="pre">writer.close()</span></code> to tell the <code class="docutils literal notranslate"><span class="pre">writer</span></code> to stop looking for values to track.</p>
<p>To start modifying <code class="docutils literal notranslate"><span class="pre">train()</span></code> we’ll also import <code class="docutils literal notranslate"><span class="pre">train_step()</span></code> and <code class="docutils literal notranslate"><span class="pre">test_step()</span></code> from <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py"><code class="docutils literal notranslate"><span class="pre">engine.py</span></code></a>.</p>
<blockquote>
<div><p><strong>Note:</strong> You can track information about your model almost anywhere in your code. But quite often experiments will be tracked <em>while</em> a model is training (inside a training/testing loop).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">torch.utils.tensorboard.SummaryWriter()</span></code> class also has many different methods to track different things about your model/data, such as <a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_graph"><code class="docutils literal notranslate"><span class="pre">add_graph()</span></code></a> which tracks the computation graph of your model. For more options, <a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter">check the <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> documentation</a>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">going_modular.going_modular.engine</span> <span class="kn">import</span> <span class="n">train_step</span><span class="p">,</span> <span class="n">test_step</span>

<span class="c1"># Import train() function from: </span>
<span class="c1"># https://github.com/thangckt/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> 
          <span class="n">train_dataloader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> 
          <span class="n">test_dataloader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> 
          <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
          <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
          <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Trains and tests a PyTorch model.</span>

<span class="sd">    Passes a target PyTorch models through train_step() and test_step()</span>
<span class="sd">    functions for a number of epochs, training and testing the model</span>
<span class="sd">    in the same epoch loop.</span>

<span class="sd">    Calculates, prints and stores evaluation metrics throughout.</span>

<span class="sd">    Args:</span>
<span class="sd">      model: A PyTorch model to be trained and tested.</span>
<span class="sd">      train_dataloader: A DataLoader instance for the model to be trained on.</span>
<span class="sd">      test_dataloader: A DataLoader instance for the model to be tested on.</span>
<span class="sd">      optimizer: A PyTorch optimizer to help minimize the loss function.</span>
<span class="sd">      loss_fn: A PyTorch loss function to calculate loss on both datasets.</span>
<span class="sd">      epochs: An integer indicating how many epochs to train for.</span>
<span class="sd">      device: A target device to compute on (e.g. &quot;cuda&quot; or &quot;cpu&quot;).</span>
<span class="sd">      </span>
<span class="sd">    Returns:</span>
<span class="sd">      A dictionary of training and testing loss as well as training and</span>
<span class="sd">      testing accuracy metrics. Each metric has a value in a list for </span>
<span class="sd">      each epoch.</span>
<span class="sd">      In the form: {train_loss: [...],</span>
<span class="sd">                train_acc: [...],</span>
<span class="sd">                test_loss: [...],</span>
<span class="sd">                test_acc: [...]} </span>
<span class="sd">      For example if training for epochs=2: </span>
<span class="sd">              {train_loss: [2.0616, 1.0537],</span>
<span class="sd">                train_acc: [0.3945, 0.3945],</span>
<span class="sd">                test_loss: [1.2641, 1.5706],</span>
<span class="sd">                test_acc: [0.3400, 0.2973]} </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create empty results dictionary</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="p">[],</span>
               <span class="s2">&quot;train_acc&quot;</span><span class="p">:</span> <span class="p">[],</span>
               <span class="s2">&quot;test_loss&quot;</span><span class="p">:</span> <span class="p">[],</span>
               <span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="p">[]</span>
    <span class="p">}</span>

    <span class="c1"># Loop through training and testing steps for a number of epochs</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                                           <span class="n">dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
                                           <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                                           <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                           <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                                        <span class="n">dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
                                        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                                        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Print out what&#39;s happening</span>
        <span class="nb">print</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;train_loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;train_acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;test_loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;test_acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Update results dictionary</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;test_acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>

        <span class="c1">### New: Experiment tracking ###</span>
        <span class="c1"># Add loss results to SummaryWriter</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">add_scalars</span><span class="p">(</span><span class="n">main_tag</span><span class="o">=</span><span class="s2">&quot;Loss&quot;</span><span class="p">,</span> 
                           <span class="n">tag_scalar_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">,</span>
                                            <span class="s2">&quot;test_loss&quot;</span><span class="p">:</span> <span class="n">test_loss</span><span class="p">},</span>
                           <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

        <span class="c1"># Add accuracy results to SummaryWriter</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">add_scalars</span><span class="p">(</span><span class="n">main_tag</span><span class="o">=</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> 
                           <span class="n">tag_scalar_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train_acc&quot;</span><span class="p">:</span> <span class="n">train_acc</span><span class="p">,</span>
                                            <span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">},</span> 
                           <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
        
        <span class="c1"># Track the PyTorch model architecture</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">add_graph</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> 
                         <span class="c1"># Pass in an example input</span>
                         <span class="n">input_to_model</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    
    <span class="c1"># Close the writer</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    
    <span class="c1">### End new ###</span>

    <span class="c1"># Return the filled results at the end of the epochs</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<p>Woohoo!</p>
<p>Our <code class="docutils literal notranslate"><span class="pre">train()</span></code> function is now updated to use a <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> instance to track our model’s results.</p>
<p>How about we try it out for 5 epochs?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train model</span>
<span class="c1"># Note: Not using engine.train() since the original script isn&#39;t updated to use writer</span>
<span class="n">set_seeds</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
                <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
                <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "bf70c256625142c283475bdf9af948a1", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398
Epoch: 2 | train_loss: 0.8975 | train_acc: 0.6562 | test_loss: 0.7838 | test_acc: 0.8561
Epoch: 3 | train_loss: 0.8037 | train_acc: 0.7461 | test_loss: 0.6723 | test_acc: 0.8864
Epoch: 4 | train_loss: 0.6769 | train_acc: 0.8516 | test_loss: 0.6698 | test_acc: 0.8049
Epoch: 5 | train_loss: 0.7065 | train_acc: 0.7188 | test_loss: 0.6746 | test_acc: 0.7737
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> You might notice the results here are slightly different to what our model got in 06. PyTorch Transfer Learning. The difference comes from using the <code class="docutils literal notranslate"><span class="pre">engine.train()</span></code> and our modified <code class="docutils literal notranslate"><span class="pre">train()</span></code> function. Can you guess why? The <a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch documentation on randomness</a> may help more.</p>
</div></blockquote>
<p>Running the cell above we get similar outputs we got in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#4-train-model">06. PyTorch Transfer Learning section 4: Train model</a> but the difference is behind the scenes our <code class="docutils literal notranslate"><span class="pre">writer</span></code> instance has created a <code class="docutils literal notranslate"><span class="pre">runs/</span></code> directory storing our model’s results.</p>
<p>For example, the save location might look like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">runs</span><span class="o">/</span><span class="n">Jun21_00</span><span class="o">-</span><span class="mi">46</span><span class="o">-</span><span class="mi">03</span><span class="n">_daniels_macbook_pro</span>
</pre></div>
</div>
<p>Where the <a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter">default format</a> is <code class="docutils literal notranslate"><span class="pre">runs/CURRENT_DATETIME_HOSTNAME</span></code>.</p>
<p>We’ll check these out in a second but just as a reminder, we were previously tracking our model’s results in a dictionary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check out the model results</span>
<span class="n">results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train_loss&#39;: [1.0923754647374153,
  0.8974628075957298,
  0.803724929690361,
  0.6769256368279457,
  0.7064960040152073],
 &#39;train_acc&#39;: [0.3984375, 0.65625, 0.74609375, 0.8515625, 0.71875],
 &#39;test_loss&#39;: [0.9132757981618246,
  0.7837507526079813,
  0.6722926497459412,
  0.6698453426361084,
  0.6746167540550232],
 &#39;test_acc&#39;: [0.5397727272727273,
  0.8560606060606061,
  0.8863636363636364,
  0.8049242424242425,
  0.7736742424242425]}
</pre></div>
</div>
</div>
</div>
<p>Hmmm, we could format this to be a nice plot but could you imagine keeping track of a bunch of these dictionaries?</p>
<p>There has to be a better way…</p>
</section>
</section>
<section id="view-our-models-results-in-tensorboard">
<h2>5. View our model’s results in TensorBoard<a class="headerlink" href="#view-our-models-results-in-tensorboard" title="Permalink to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> class stores our model’s results in a directory called <code class="docutils literal notranslate"><span class="pre">runs/</span></code> in TensorBoard format by default.</p>
<p>TensorBoard is a visualization program created by the TensorFlow team to view and inspect information about models and data.</p>
<p>You know what that means?</p>
<p>It’s time to follow the data visualizer’s motto and <em>visualize, visualize, visualize!</em></p>
<p>You can view TensorBoard in a number of ways:</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Code environment</p></th>
<th class="head"><p>How to view TensorBoard</p></th>
<th class="head"><p>Resource</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>VS Code (notebooks or Python scripts)</p></td>
<td><p>Press <code class="docutils literal notranslate"><span class="pre">SHIFT</span> <span class="pre">+</span> <span class="pre">CMD</span> <span class="pre">+</span> <span class="pre">P</span></code> to open the Command Palette and search for the command “Python: Launch TensorBoard”.</p></td>
<td><p><a class="reference external" href="https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration">VS Code Guide on TensorBoard and PyTorch</a></p></td>
</tr>
<tr class="row-odd"><td><p>Jupyter and Colab Notebooks</p></td>
<td><p>Make sure <a class="reference external" href="https://pypi.org/project/tensorboard/">TensorBoard is installed</a>, load it with <code class="docutils literal notranslate"><span class="pre">%load_ext</span> <span class="pre">tensorboard</span></code> and then view your results with <code class="docutils literal notranslate"><span class="pre">%tensorboard</span> <span class="pre">--logdir</span> <span class="pre">DIR_WITH_LOGS</span></code>.</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/tensorboard.html"><code class="docutils literal notranslate"><span class="pre">torch.utils.tensorboard</span></code></a> and <a class="reference external" href="https://www.tensorflow.org/tensorboard/get_started">Get started with TensorBoard</a></p></td>
</tr>
</tbody>
</table>
</div>
<p>You can also upload your experiments to <a class="reference external" href="https://tensorboard.dev/">tensorboard.dev</a> to share them publicly with others.</p>
<p>Running the following code in a Google Colab or Jupyter Notebook will start an interactive TensorBoard session to view TensorBoard files in the <code class="docutils literal notranslate"><span class="pre">runs/</span></code> directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">load_ext</span> <span class="n">tensorboard</span> <span class="c1"># line magic to load TensorBoard</span>
<span class="o">%</span><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span> <span class="n">runs</span> <span class="c1"># run TensorBoard session with the &quot;runs/&quot; directory</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out)</span>
<span class="c1"># %load_ext tensorboard</span>
<span class="c1"># %tensorboard --logdir runs</span>
</pre></div>
</div>
</div>
</div>
<p>If all went correctly, you should see something like the following:</p>
<img alt="output of viewing a single experiment in tensorboard" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/07-tensorboard-single-experiment.png" />
<p><em>Viewing a single modelling experiment’s results for accuracy and loss in TensorBoard.</em></p>
<blockquote>
<div><p><strong>Note:</strong> For more information on running TensorBoard in notebooks or in other locations, see the following:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks">Using TensorBoard in Notebooks guide by TensorFlow</a></p></li>
<li><p><a class="reference external" href="https://tensorboard.dev/#get-started">Get started with TensorBoard.dev</a> (helpful for uploading your TensorBoard logs to a shareable link)</p></li>
</ul>
</div></blockquote>
</section>
<section id="create-a-helper-function-to-build-summarywriter-instances">
<h2>6. Create a helper function to build <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> instances<a class="headerlink" href="#create-a-helper-function-to-build-summarywriter-instances" title="Permalink to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> class logs various information to a directory specified by the <code class="docutils literal notranslate"><span class="pre">log_dir</span></code> parameter.</p>
<p>How about we make a helper function to create a custom directory per experiment?</p>
<p>In essence, each experiment gets its own logs directory.</p>
<p>For example, say we’d like to track things like:</p>
<ul class="simple">
<li><p><strong>Experiment date/timestamp</strong> - when did the experiment take place?</p></li>
<li><p><strong>Experiment name</strong> - is there something we’d like to call the experiment?</p></li>
<li><p><strong>Model name</strong> - what model was used?</p></li>
<li><p><strong>Extra</strong> - should anything else be tracked?</p></li>
</ul>
<p>You could track almost anything here and be as creative as you want but these should be enough to start.</p>
<p>Let’s create a helper function called <code class="docutils literal notranslate"><span class="pre">create_writer()</span></code> that produces a <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> instance tracking to a custom <code class="docutils literal notranslate"><span class="pre">log_dir</span></code>.</p>
<p>Ideally, we’d like the <code class="docutils literal notranslate"><span class="pre">log_dir</span></code> to be something like:</p>
<p><code class="docutils literal notranslate"><span class="pre">runs/YYYY-MM-DD/experiment_name/model_name/extra</span></code></p>
<p>Where <code class="docutils literal notranslate"><span class="pre">YYYY-MM-DD</span></code> is the date the experiment was run (you could add the time if you wanted to as well).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_writer</span><span class="p">(</span><span class="n">experiment_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                  <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                  <span class="n">extra</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">tensorboard</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">SummaryWriter</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.</span>

<span class="sd">    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.</span>

<span class="sd">    Where timestamp is the current date in YYYY-MM-DD format.</span>

<span class="sd">    Args:</span>
<span class="sd">        experiment_name (str): Name of experiment.</span>
<span class="sd">        model_name (str): Name of model.</span>
<span class="sd">        extra (str, optional): Anything extra to add to the directory. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.</span>

<span class="sd">    Example usage:</span>
<span class="sd">        # Create a writer saving to &quot;runs/2022-06-04/data_10_percent/effnetb2/5_epochs/&quot;</span>
<span class="sd">        writer = create_writer(experiment_name=&quot;data_10_percent&quot;,</span>
<span class="sd">                               model_name=&quot;effnetb2&quot;,</span>
<span class="sd">                               extra=&quot;5_epochs&quot;)</span>
<span class="sd">        # The above is the same as:</span>
<span class="sd">        writer = SummaryWriter(log_dir=&quot;runs/2022-06-04/data_10_percent/effnetb2/5_epochs/&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
    <span class="kn">import</span> <span class="nn">os</span>

    <span class="c1"># Get timestamp of current date (all experiments on certain day live in same folder)</span>
    <span class="n">timestamp</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># returns current date in YYYY-MM-DD format</span>

    <span class="k">if</span> <span class="n">extra</span><span class="p">:</span>
        <span class="c1"># Create log directory path</span>
        <span class="n">log_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;runs&quot;</span><span class="p">,</span> <span class="n">timestamp</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">extra</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">log_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;runs&quot;</span><span class="p">,</span> <span class="n">timestamp</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Created SummaryWriter, saving to: </span><span class="si">{</span><span class="n">log_dir</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Beautiful!</p>
<p>Now we’ve got a <code class="docutils literal notranslate"><span class="pre">create_writer()</span></code> function, let’s try it out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an example writer</span>
<span class="n">example_writer</span> <span class="o">=</span> <span class="n">create_writer</span><span class="p">(</span><span class="n">experiment_name</span><span class="o">=</span><span class="s2">&quot;data_10_percent&quot;</span><span class="p">,</span>
                               <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;effnetb0&quot;</span><span class="p">,</span>
                               <span class="n">extra</span><span class="o">=</span><span class="s2">&quot;5_epochs&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...
</pre></div>
</div>
</div>
</div>
<p>Looking good, now we’ve got a way to log and trace back our various experiments.</p>
<section id="update-the-train-function-to-include-a-writer-parameter">
<h3>6.1 Update the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to include a <code class="docutils literal notranslate"><span class="pre">writer</span></code> parameter<a class="headerlink" href="#update-the-train-function-to-include-a-writer-parameter" title="Permalink to this heading">#</a></h3>
<p>Our <code class="docutils literal notranslate"><span class="pre">create_writer()</span></code> function works fantastic.</p>
<p>How about we give our <code class="docutils literal notranslate"><span class="pre">train()</span></code> function the ability to take in a <code class="docutils literal notranslate"><span class="pre">writer</span></code> parameter so we actively update the <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> instance we’re using each time we call <code class="docutils literal notranslate"><span class="pre">train()</span></code>.</p>
<p>For example, say we’re running a series of experiments, calling <code class="docutils literal notranslate"><span class="pre">train()</span></code> multiple times for multiple different models, it would be good if each experiment used a different <code class="docutils literal notranslate"><span class="pre">writer</span></code>.</p>
<p>One <code class="docutils literal notranslate"><span class="pre">writer</span></code> per experiment = one logs directory per experiment.</p>
<p>To adjust the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function we’ll add a <code class="docutils literal notranslate"><span class="pre">writer</span></code> parameter to the function and then we’ll add some code to see if there’s a <code class="docutils literal notranslate"><span class="pre">writer</span></code> and if so, we’ll track our information there.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Add writer parameter to train()</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> 
          <span class="n">train_dataloader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> 
          <span class="n">test_dataloader</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span> 
          <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
          <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
          <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> 
          <span class="n">writer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">tensorboard</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">SummaryWriter</span> <span class="c1"># new parameter to take in a writer</span>
          <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Trains and tests a PyTorch model.</span>

<span class="sd">    Passes a target PyTorch models through train_step() and test_step()</span>
<span class="sd">    functions for a number of epochs, training and testing the model</span>
<span class="sd">    in the same epoch loop.</span>

<span class="sd">    Calculates, prints and stores evaluation metrics throughout.</span>

<span class="sd">    Stores metrics to specified writer log_dir if present.</span>

<span class="sd">    Args:</span>
<span class="sd">      model: A PyTorch model to be trained and tested.</span>
<span class="sd">      train_dataloader: A DataLoader instance for the model to be trained on.</span>
<span class="sd">      test_dataloader: A DataLoader instance for the model to be tested on.</span>
<span class="sd">      optimizer: A PyTorch optimizer to help minimize the loss function.</span>
<span class="sd">      loss_fn: A PyTorch loss function to calculate loss on both datasets.</span>
<span class="sd">      epochs: An integer indicating how many epochs to train for.</span>
<span class="sd">      device: A target device to compute on (e.g. &quot;cuda&quot; or &quot;cpu&quot;).</span>
<span class="sd">      writer: A SummaryWriter() instance to log model results to.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A dictionary of training and testing loss as well as training and</span>
<span class="sd">      testing accuracy metrics. Each metric has a value in a list for </span>
<span class="sd">      each epoch.</span>
<span class="sd">      In the form: {train_loss: [...],</span>
<span class="sd">                train_acc: [...],</span>
<span class="sd">                test_loss: [...],</span>
<span class="sd">                test_acc: [...]} </span>
<span class="sd">      For example if training for epochs=2: </span>
<span class="sd">              {train_loss: [2.0616, 1.0537],</span>
<span class="sd">                train_acc: [0.3945, 0.3945],</span>
<span class="sd">                test_loss: [1.2641, 1.5706],</span>
<span class="sd">                test_acc: [0.3400, 0.2973]} </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create empty results dictionary</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="p">[],</span>
               <span class="s2">&quot;train_acc&quot;</span><span class="p">:</span> <span class="p">[],</span>
               <span class="s2">&quot;test_loss&quot;</span><span class="p">:</span> <span class="p">[],</span>
               <span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="p">[]</span>
    <span class="p">}</span>

    <span class="c1"># Loop through training and testing steps for a number of epochs</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                                          <span class="n">dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
                                          <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                                          <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                          <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
          <span class="n">dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
          <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
          <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Print out what&#39;s happening</span>
        <span class="nb">print</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;train_loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;train_acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;test_loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;test_acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Update results dictionary</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;train_acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;test_acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>


        <span class="c1">### New: Use the writer parameter to track experiments ###</span>
        <span class="c1"># See if there&#39;s a writer, if so, log to it</span>
        <span class="k">if</span> <span class="n">writer</span><span class="p">:</span>
            <span class="c1"># Add results to SummaryWriter</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">add_scalars</span><span class="p">(</span><span class="n">main_tag</span><span class="o">=</span><span class="s2">&quot;Loss&quot;</span><span class="p">,</span> 
                               <span class="n">tag_scalar_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">,</span>
                                                <span class="s2">&quot;test_loss&quot;</span><span class="p">:</span> <span class="n">test_loss</span><span class="p">},</span>
                               <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">add_scalars</span><span class="p">(</span><span class="n">main_tag</span><span class="o">=</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> 
                               <span class="n">tag_scalar_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;train_acc&quot;</span><span class="p">:</span> <span class="n">train_acc</span><span class="p">,</span>
                                                <span class="s2">&quot;test_acc&quot;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">},</span> 
                               <span class="n">global_step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

            <span class="c1"># Close the writer</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="c1">### End new ###</span>

    <span class="c1"># Return the filled results at the end of the epochs</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="setting-up-a-series-of-modelling-experiments">
<h2>7. Setting up a series of modelling experiments<a class="headerlink" href="#setting-up-a-series-of-modelling-experiments" title="Permalink to this heading">#</a></h2>
<p>It’s to step things up a notch.</p>
<p>Previously we’ve been running various experiments and inspecting the results one by one.</p>
<p>But what if we could run multiple experiments and then inspect the results all together?</p>
<p>You in?</p>
<p>C’mon, let’s go.</p>
<section id="what-kind-of-experiments-should-you-run">
<h3>7.1 What kind of experiments should you run?<a class="headerlink" href="#what-kind-of-experiments-should-you-run" title="Permalink to this heading">#</a></h3>
<p>That’s the million dollar question in machine learning.</p>
<p>Because there’s really no limit to the experiments you can run.</p>
<p>Such a freedom is why machine learning is so exciting and terrifying at the same time.</p>
<p>This is where you’ll have to put on your scientist coat and remember the machine learning practitioner’s motto: <em>experiment, experiment, experiment!</em></p>
<p>Every hyperparameter stands as a starting point for a different experiment:</p>
<ul class="simple">
<li><p>Change the number of <strong>epochs</strong>.</p></li>
<li><p>Change the number of <strong>layers/hidden units</strong>.</p></li>
<li><p>Change the amount of <strong>data</strong>.</p></li>
<li><p>Change the <strong>learning rate</strong>.</p></li>
<li><p>Try different kinds of <strong>data augmentation</strong>.</p></li>
<li><p>Choose a different <strong>model architecture</strong>.</p></li>
</ul>
<p>With practice and running many different experiments, you’ll start to build an intuition of what <em>might</em> help your model.</p>
<p>I say <em>might</em> on purpose because there’s no guarantees.</p>
<p>But generally, in light of <a class="reference external" href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><em>The Bitter Lesson</em></a> (I’ve mentioned this twice now because it’s an important essay in the world of AI), generally the bigger your model (more learnable parameters) and the more data you have (more opportunities to learn), the better the performance.</p>
<p>However, when you’re first approaching a machine learning problem: start small and if something works, scale it up.</p>
<p>Your first batch of experiments should take no longer than a few seconds to a few minutes to run.</p>
<p>The quicker you can experiment, the faster you can work out what <em>doesn’t</em> work, in turn, the faster you can work out what <em>does</em> work.</p>
</section>
<section id="what-experiments-are-we-going-to-run">
<h3>7.2 What experiments are we going to run?<a class="headerlink" href="#what-experiments-are-we-going-to-run" title="Permalink to this heading">#</a></h3>
<p>Our goal is to improve the model powering FoodVision Mini without it getting too big.</p>
<p>In essence, our ideal model achieves a high level of test set accuracy (90%+) but doesn’t take too long to train/perform inference (make predictions).</p>
<p>We’ve got plenty of options but how about we keep things simple?</p>
<p>Let’s try a combination of:</p>
<ol class="arabic simple">
<li><p>A different amount of data (10% of Pizza, Steak, Sushi vs. 20%)</p></li>
<li><p>A different model (<a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.models.efficientnet_b0.html#torchvision.models.efficientnet_b0"><code class="docutils literal notranslate"><span class="pre">torchvision.models.efficientnet_b0</span></code></a> vs. <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2"><code class="docutils literal notranslate"><span class="pre">torchvision.models.efficientnet_b2</span></code></a>)</p></li>
<li><p>A different training time (5 epochs vs. 10 epochs)</p></li>
</ol>
<p>Breaking these down we get:</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Experiment number</p></th>
<th class="head"><p>Training Dataset</p></th>
<th class="head"><p>Model (pretrained on ImageNet)</p></th>
<th class="head"><p>Number of epochs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Pizza, Steak, Sushi 10% percent</p></td>
<td><p>EfficientNetB0</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Pizza, Steak, Sushi 10% percent</p></td>
<td><p>EfficientNetB2</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Pizza, Steak, Sushi 10% percent</p></td>
<td><p>EfficientNetB0</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>Pizza, Steak, Sushi 10% percent</p></td>
<td><p>EfficientNetB2</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Pizza, Steak, Sushi 20% percent</p></td>
<td><p>EfficientNetB0</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>Pizza, Steak, Sushi 20% percent</p></td>
<td><p>EfficientNetB2</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>Pizza, Steak, Sushi 20% percent</p></td>
<td><p>EfficientNetB0</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>Pizza, Steak, Sushi 20% percent</p></td>
<td><p>EfficientNetB2</p></td>
<td><p>10</p></td>
</tr>
</tbody>
</table>
</div>
<p>Notice how we’re slowly scaling things up.</p>
<p>With each experiment we slowly increase the amount of data, the model size and the length of training.</p>
<p>By the end, experiment 8 will be using double the data, double the model size and double the length of training compared to experiment 1.</p>
<blockquote>
<div><p><strong>Note:</strong> I want to be clear that there truly is no limit to amount of experiments you can run. What we’ve designed here is only a very small subset of options. However, you can’t test <em>everything</em> so best to try a few things to begin with and then follow the ones which work the best.</p>
<p>And as a reminder, the datasets we’re using are a subset of the <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision.datasets.Food101">Food101 dataset</a> (3 classes, pizza, steak, suhsi, instead of 101) and 10% and 20% of the images rather than 100%. If our experiments work, we could start to run more on more data (though this will take longer to compute). You can see how the datasets were created via the <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb"><code class="docutils literal notranslate"><span class="pre">04_custom_data_creation.ipynb</span></code> notebook</a>.</p>
</div></blockquote>
</section>
<section id="download-different-datasets">
<h3>7.3 Download different datasets<a class="headerlink" href="#download-different-datasets" title="Permalink to this heading">#</a></h3>
<p>Before we start running our series of experiments, we need to make sure our datasets are ready.</p>
<p>We’ll need two forms of a training set:</p>
<ol class="arabic simple">
<li><p>A training set with <strong>10% of the data</strong> of Food101 pizza, steak, sushi images (we’ve already created this above but we’ll do it again for completeness).</p></li>
<li><p>A training set with <strong>20% of the data</strong> of Food101 pizza, steak, sushi images.</p></li>
</ol>
<p>For consistency, all experiments will use the same testing dataset (the one from the 10% data split).</p>
<p>We’ll start by downloading the various datasets we need using the <code class="docutils literal notranslate"><span class="pre">download_data()</span></code> function we created earlier.</p>
<p>Both datasets are available from the course GitHub:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip">Pizza, steak, sushi 10% training data</a>.</p></li>
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip">Pizza, steak, sushi 20% training data</a>.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download 10 percent and 20 percent training data (if necessary)</span>
<span class="n">data_10_percent_path</span> <span class="o">=</span> <span class="n">download_data</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s2">&quot;https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip&quot;</span><span class="p">,</span>
                                     <span class="n">destination</span><span class="o">=</span><span class="s2">&quot;pizza_steak_sushi&quot;</span><span class="p">)</span>

<span class="n">data_20_percent_path</span> <span class="o">=</span> <span class="n">download_data</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s2">&quot;https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip&quot;</span><span class="p">,</span>
                                     <span class="n">destination</span><span class="o">=</span><span class="s2">&quot;pizza_steak_sushi_20_percent&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] data/pizza_steak_sushi directory exists, skipping download.
[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.
</pre></div>
</div>
</div>
</div>
<p>Data downloaded!</p>
<p>Now let’s setup the filepaths to data we’ll be using for the different experiments.</p>
<p>We’ll create different training directory paths but we’ll only need one testing directory path since all experiments will be using the same test dataset (the test dataset from pizza, steak, sushi 10%).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup training directory paths</span>
<span class="n">train_dir_10_percent</span> <span class="o">=</span> <span class="n">data_10_percent_path</span> <span class="o">/</span> <span class="s2">&quot;train&quot;</span>
<span class="n">train_dir_20_percent</span> <span class="o">=</span> <span class="n">data_20_percent_path</span> <span class="o">/</span> <span class="s2">&quot;train&quot;</span>

<span class="c1"># Setup testing directory paths (note: use the same test dataset for both to compare the results)</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">data_10_percent_path</span> <span class="o">/</span> <span class="s2">&quot;test&quot;</span>

<span class="c1"># Check the directories</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training directory 10%: </span><span class="si">{</span><span class="n">train_dir_10_percent</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training directory 20%: </span><span class="si">{</span><span class="n">train_dir_20_percent</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Testing directory: </span><span class="si">{</span><span class="n">test_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training directory 10%: data/pizza_steak_sushi/train
Training directory 20%: data/pizza_steak_sushi_20_percent/train
Testing directory: data/pizza_steak_sushi/test
</pre></div>
</div>
</div>
</div>
</section>
<section id="transform-datasets-and-create-dataloaders">
<h3>7.4 Transform Datasets and create DataLoaders<a class="headerlink" href="#transform-datasets-and-create-dataloaders" title="Permalink to this heading">#</a></h3>
<p>Next we’ll create a series of transforms to prepare our images for our model(s).</p>
<p>To keep things consistent, we’ll manually create a transform (just like we did above) and use the same transform across all of the datasets.</p>
<p>The transform will:</p>
<ol class="arabic simple">
<li><p>Resize all the images (we’ll start with 224, 224 but this could be changed).</p></li>
<li><p>Turn them into tensors with values between 0 &amp; 1.</p></li>
<li><p>Normalize them in way so their distributions are inline with the ImageNet dataset (we do this because our models from <a class="reference external" href="https://pytorch.org/vision/stable/models.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a> have been pretrained on ImageNet).</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="c1"># Create a transform to normalize data distribution to be inline with ImageNet</span>
<span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="c1"># values per colour channel [red, green, blue]</span>
                                 <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span> <span class="c1"># values per colour channel [red, green, blue]</span>

<span class="c1"># Compose transforms into a pipeline</span>
<span class="n">simple_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span> <span class="c1"># 1. Resize the images</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="c1"># 2. Turn the images into tensors with values between 0 &amp; 1</span>
    <span class="n">normalize</span> <span class="c1"># 3. Normalize the images so their distributions match the ImageNet dataset </span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Transform ready!</p>
<p>Now let’s create our DataLoaders using the <code class="docutils literal notranslate"><span class="pre">create_dataloaders()</span></code> function from <code class="docutils literal notranslate"><span class="pre">data_setup.py</span></code> we created in <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy">05. PyTorch Going Modular section 2</a>.</p>
<p>We’ll create the DataLoaders with a batch size of 32.</p>
<p>For all of our experiments we’ll be using the same <code class="docutils literal notranslate"><span class="pre">test_dataloader</span></code> (to keep comparisons consistent).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Create 10% training and test DataLoaders</span>
<span class="n">train_dataloader_10_percent</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span> <span class="o">=</span> <span class="n">data_setup</span><span class="o">.</span><span class="n">create_dataloaders</span><span class="p">(</span><span class="n">train_dir</span><span class="o">=</span><span class="n">train_dir_10_percent</span><span class="p">,</span>
    <span class="n">test_dir</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span> 
    <span class="n">transform</span><span class="o">=</span><span class="n">simple_transform</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span>
<span class="p">)</span>

<span class="c1"># Create 20% training and test data DataLoders</span>
<span class="n">train_dataloader_20_percent</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span> <span class="o">=</span> <span class="n">data_setup</span><span class="o">.</span><span class="n">create_dataloaders</span><span class="p">(</span><span class="n">train_dir</span><span class="o">=</span><span class="n">train_dir_20_percent</span><span class="p">,</span>
    <span class="n">test_dir</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">simple_transform</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span>
<span class="p">)</span>

<span class="c1"># Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of batches of size </span><span class="si">{</span><span class="n">BATCH_SIZE</span><span class="si">}</span><span class="s2"> in 10 percent training data: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader_10_percent</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of batches of size </span><span class="si">{</span><span class="n">BATCH_SIZE</span><span class="si">}</span><span class="s2"> in 20 percent training data: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader_20_percent</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of batches of size </span><span class="si">{</span><span class="n">BATCH_SIZE</span><span class="si">}</span><span class="s2"> in testing data: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader_10_percent</span><span class="p">)</span><span class="si">}</span><span class="s2"> (all experiments will use the same test set)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of classes: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span><span class="si">}</span><span class="s2">, class names: </span><span class="si">{</span><span class="n">class_names</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of batches of size 32 in 10 percent training data: 8
Number of batches of size 32 in 20 percent training data: 15
Number of batches of size 32 in testing data: 8 (all experiments will use the same test set)
Number of classes: 3, class names: [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;]
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-feature-extractor-models">
<h3>7.5 Create feature extractor models<a class="headerlink" href="#create-feature-extractor-models" title="Permalink to this heading">#</a></h3>
<p>Time to start building our models.</p>
<p>We’re going to create two feature extractor models:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models.efficientnet_b0()</span></code></a> pretrained backbone + custom classifier head (EffNetB0 for short).</p></li>
<li><p><a class="reference external" href="https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b2.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models.efficientnet_b2()</span></code></a> pretrained backbone + custom classifier head (EffNetB2 for short).</p></li>
</ol>
<p>To do this, we’ll freeze the base layers (the feature layers) and update the model’s classifier heads (output layers) to suit our problem just like we did in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs">06. PyTorch Transfer Learning section 3.4</a>.</p>
<p>We saw in the previous chapter the <code class="docutils literal notranslate"><span class="pre">in_features</span></code> parameter to the classifier head of EffNetB0 is <code class="docutils literal notranslate"><span class="pre">1280</span></code> (the backbone turns the input image into a feature vector of size <code class="docutils literal notranslate"><span class="pre">1280</span></code>).</p>
<p>Since EffNetB2 has a different number of layers and parameters, we’ll need to adapt it accordingly.</p>
<blockquote>
<div><p><strong>Note:</strong> Whenever you use a different model, one of the first things you should inspect is the input and output shapes. That way you’ll know how you’ll have to prepare your input data/update the model to have the correct output shape.</p>
</div></blockquote>
<p>We can find the input and output shapes of EffNetB2 using <a class="reference external" href="https://github.com/TylerYep/torchinfo"><code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code></a> and passing in the <code class="docutils literal notranslate"><span class="pre">input_size=(32,</span> <span class="pre">3,</span> <span class="pre">224,</span> <span class="pre">224)</span></code> parameter (<code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">3,</span> <span class="pre">224,</span> <span class="pre">224)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">color_channels,</span> <span class="pre">height,</span> <span class="pre">width)</span></code>, i.e we pass in an example of what a single batch of data would be to our model).</p>
<blockquote>
<div><p><strong>Note:</strong> Many modern models can handle input images of varying sizes thanks to <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.AdaptiveAvgPool2d()</span></code></a> layer, this layer adaptively adjusts the <code class="docutils literal notranslate"><span class="pre">output_size</span></code> of a given input as required. You can try this out by passing different size input images to <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code> or to your own models using the layer.</p>
</div></blockquote>
<p>To find the required input shape to the final layer of EffNetB2, let’s:</p>
<ol class="arabic simple">
<li><p>Create an instance of <code class="docutils literal notranslate"><span class="pre">torchvision.models.efficientnet_b2(pretrained=True)</span></code>.</p></li>
<li><p>See the various input and output shapes by running <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code>.</p></li>
<li><p>Print out the number of <code class="docutils literal notranslate"><span class="pre">in_features</span></code> by inspecting <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> of the classifier portion of EffNetB2 and printing the length of the weight matrix.</p>
<ul class="simple">
<li><p><strong>Note:</strong> You could also just inspect the output of <code class="docutils literal notranslate"><span class="pre">effnetb2.classifier</span></code>.</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="c1"># 1. Create an instance of EffNetB2 with pretrained weights</span>
<span class="n">effnetb2_weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">EfficientNet_B2_Weights</span><span class="o">.</span><span class="n">DEFAULT</span> <span class="c1"># &quot;DEFAULT&quot; means best available weights</span>
<span class="n">effnetb2</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b2</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">effnetb2_weights</span><span class="p">)</span>

<span class="c1"># # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)</span>
<span class="c1"># summary(model=effnetb2, </span>
<span class="c1">#         input_size=(32, 3, 224, 224), # make sure this is &quot;input_size&quot;, not &quot;input_shape&quot;</span>
<span class="c1">#         # col_names=[&quot;input_size&quot;], # uncomment for smaller output</span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;]</span>
<span class="c1"># ) </span>

<span class="c1"># 3. Get the number of in_features of the EfficientNetB2 classifier layer</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of in_features to final layer of EfficientNetB2: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">effnetb2</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s1">&#39;1.weight&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of in_features to final layer of EfficientNetB2: 1408
</pre></div>
</div>
</div>
</div>
<img alt="output of torchinfo.summary() when passed our effnetb2 model with all layers trainable and default classifier head" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/07-effnetb2-unfrozen-summary-output.png" />
<p><em>Model summary of EffNetB2 feature extractor model with all layers unfrozen (trainable) and default classifier head from ImageNet pretraining.</em></p>
<p>Now we know the required number of <code class="docutils literal notranslate"><span class="pre">in_features</span></code> for the EffNetB2 model, let’s create a couple of helper functions to setup our EffNetB0 and EffNetB2 feature extractor models.</p>
<p>We want these functions to:</p>
<ol class="arabic simple">
<li><p>Get the base model from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></p></li>
<li><p>Freeze the base layers in the model (set <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>)</p></li>
<li><p>Set the random seeds (we don’t <em>need</em> to do this but since we’re running a series of experiments and initalizing a new layer with random weights, we want the randomness to be similar for each experiment)</p></li>
<li><p>Change the classifier head (to suit our problem)</p></li>
<li><p>Give the model a name (e.g. “effnetb0” for EffNetB0)</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Get num out features (one for each class pizza, steak, sushi)</span>
<span class="n">OUT_FEATURES</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span>

<span class="c1"># Create an EffNetB0 feature extractor</span>
<span class="k">def</span> <span class="nf">create_effnetb0</span><span class="p">():</span>
    <span class="c1"># 1. Get the base mdoel with pretrained weights and send to target device</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">EfficientNet_B0_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b0</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 2. Freeze the base model layers</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># 3. Set the seeds</span>
    <span class="n">set_seeds</span><span class="p">()</span>

    <span class="c1"># 4. Change the classifier head</span>
    <span class="n">model</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">OUT_FEATURES</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 5. Give the model a name</span>
    <span class="n">model</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;effnetb0&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Created new </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> model.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Create an EffNetB2 feature extractor</span>
<span class="k">def</span> <span class="nf">create_effnetb2</span><span class="p">():</span>
    <span class="c1"># 1. Get the base model with pretrained weights and send to target device</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">EfficientNet_B2_Weights</span><span class="o">.</span><span class="n">DEFAULT</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">efficientnet_b2</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 2. Freeze the base model layers</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># 3. Set the seeds</span>
    <span class="n">set_seeds</span><span class="p">()</span>

    <span class="c1"># 4. Change the classifier head</span>
    <span class="n">model</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1408</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">OUT_FEATURES</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 5. Give the model a name</span>
    <span class="n">model</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;effnetb2&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Created new </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> model.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<p>Those are some nice looking functions!</p>
<p>Let’s test them out by creating an instance of EffNetB0 and EffNetB2 and checking out their <code class="docutils literal notranslate"><span class="pre">summary()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">effnetb0</span> <span class="o">=</span> <span class="n">create_effnetb0</span><span class="p">()</span> 

<span class="c1"># Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)</span>
<span class="c1"># summary(model=effnetb0, </span>
<span class="c1">#         input_size=(32, 3, 224, 224), # make sure this is &quot;input_size&quot;, not &quot;input_shape&quot;</span>
<span class="c1">#         # col_names=[&quot;input_size&quot;], # uncomment for smaller output</span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;]</span>
<span class="c1"># ) </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Created new effnetb0 model.
</pre></div>
</div>
</div>
</div>
<img alt="output of torchinfo.summary() when passed our effnetb0 model with base layers are frozen and classifier head is updated" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/07-effnetb0-frozen-summary-output.png" />
<p><em>Model summary of EffNetB0 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">effnetb2</span> <span class="o">=</span> <span class="n">create_effnetb2</span><span class="p">()</span>

<span class="c1"># Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)</span>
<span class="c1"># summary(model=effnetb2, </span>
<span class="c1">#         input_size=(32, 3, 224, 224), # make sure this is &quot;input_size&quot;, not &quot;input_shape&quot;</span>
<span class="c1">#         # col_names=[&quot;input_size&quot;], # uncomment for smaller output</span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;]</span>
<span class="c1"># ) </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Created new effnetb2 model.
</pre></div>
</div>
</div>
</div>
<img alt="output of torchinfo.summary() when passed our effnetb2 model with base layers are frozen and classifier head is updated" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/07-effnetb2-frozen-summary-output.png" />
<p><em>Model summary of EffNetB2 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).</em></p>
<p>Looking at the outputs of the summaries, it seems the EffNetB2 backbone has nearly double the amount of parameters as EffNetB0.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Total parameters (before freezing/changing head)</p></th>
<th class="head"><p>Total parameters (after freezing/changing head)</p></th>
<th class="head"><p>Total trainable parameters (after freezing/changing head)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>EfficientNetB0</p></td>
<td><p>5,288,548</p></td>
<td><p>4,011,391</p></td>
<td><p>3,843</p></td>
</tr>
<tr class="row-odd"><td><p>EfficientNetB2</p></td>
<td><p>9,109,994</p></td>
<td><p>7,705,221</p></td>
<td><p>4,227</p></td>
</tr>
</tbody>
</table>
</div>
<p>This gives the backbone of the EffNetB2 model more opportunities to form a representation of our pizza, steak and sushi data.</p>
<p>However, the trainable parameters for each model (the classifier heads) aren’t very different.</p>
<p>Will these extra parameters lead to better results?</p>
<p>We’ll have to wait and see…</p>
<blockquote>
<div><p><strong>Note:</strong> In the spirit of experimenting, you really could try almost any model from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> in a similar fashion to what we’re doing here. I’ve only chosen EffNetB0 and EffNetB2 as examples. Perhaps you might want to throw something like <code class="docutils literal notranslate"><span class="pre">torchvision.models.convnext_tiny()</span></code> or <code class="docutils literal notranslate"><span class="pre">torchvision.models.convnext_small()</span></code> into the mix.</p>
</div></blockquote>
</section>
<section id="create-experiments-and-set-up-training-code">
<h3>7.6 Create experiments and set up training code<a class="headerlink" href="#create-experiments-and-set-up-training-code" title="Permalink to this heading">#</a></h3>
<p>We’ve prepared our data and prepared our models, the time has come to setup some experiments!</p>
<p>We’ll start by creating two lists and a dictionary:</p>
<ol class="arabic simple">
<li><p>A list of the number of epochs we’d like to test (<code class="docutils literal notranslate"><span class="pre">[5,</span> <span class="pre">10]</span></code>)</p></li>
<li><p>A list of the models we’d like to test (<code class="docutils literal notranslate"><span class="pre">[&quot;effnetb0&quot;,</span> <span class="pre">&quot;effnetb2&quot;]</span></code>)</p></li>
<li><p>A dictionary of the different training DataLoaders</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create epochs list</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="c1"># 2. Create models list (need to create a new model for each experiment)</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;effnetb0&quot;</span><span class="p">,</span> <span class="s2">&quot;effnetb2&quot;</span><span class="p">]</span>

<span class="c1"># 3. Create dataloaders dictionary for various dataloaders</span>
<span class="n">train_dataloaders</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;data_10_percent&quot;</span><span class="p">:</span> <span class="n">train_dataloader_10_percent</span><span class="p">,</span>
                     <span class="s2">&quot;data_20_percent&quot;</span><span class="p">:</span> <span class="n">train_dataloader_20_percent</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Lists and dictionary created!</p>
<p>Now we can write code to iterate through each of the different options and try out each of the different combinations.</p>
<p>We’ll also save the model at the end of each experiment so later on we can load back in the best model and use it for making predictions.</p>
<p>Specifically, let’s go through the following steps:</p>
<ol class="arabic simple">
<li><p>Set the random seeds (so our experiment results are reproducible, in practice, you might run the same experiment across ~3 different seeds and average the results).</p></li>
<li><p>Keep track of different experiment numbers (this is mostly for pretty print outs).</p></li>
<li><p>Loop through the <code class="docutils literal notranslate"><span class="pre">train_dataloaders</span></code> dictionary items for each of the different training DataLoaders.</p></li>
<li><p>Loop through the list of epoch numbers.</p></li>
<li><p>Loop through the list of different model names.</p></li>
<li><p>Create information print outs for the current running experiment (so we know what’s happening).</p></li>
<li><p>Check which model is the target model and create a new EffNetB0 or EffNetB2 instance (we create a new model instance each experiment so all models start from the same standpoint).</p></li>
<li><p>Create a new loss function (<code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss()</span></code>) and optimizer (<code class="docutils literal notranslate"><span class="pre">torch.optim.Adam(params=model.parameters(),</span> <span class="pre">lr=0.001)</span></code>) for each new experiment.</p></li>
<li><p>Train the model with the modified <code class="docutils literal notranslate"><span class="pre">train()</span></code> function passing the appropriate details to the <code class="docutils literal notranslate"><span class="pre">writer</span></code> parameter.</p></li>
<li><p>Save the trained model with an appropriate file name to file with <code class="docutils literal notranslate"><span class="pre">save_model()</span></code> from <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/going_modular/going_modular/utils.py"><code class="docutils literal notranslate"><span class="pre">utils.py</span></code></a>.</p></li>
</ol>
<p>We can also use the <code class="docutils literal notranslate"><span class="pre">%%time</span></code> magic to see how long all of our experiments take together in a single Jupyter/Google Colab cell.</p>
<p>Let’s do it!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="kn">from</span> <span class="nn">going_modular.going_modular.utils</span> <span class="kn">import</span> <span class="n">save_model</span>

<span class="c1"># 1. Set the random seeds</span>
<span class="n">set_seeds</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 2. Keep track of experiment numbers</span>
<span class="n">experiment_number</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># 3. Loop through each DataLoader</span>
<span class="k">for</span> <span class="n">dataloader_name</span><span class="p">,</span> <span class="n">train_dataloader</span> <span class="ow">in</span> <span class="n">train_dataloaders</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

    <span class="c1"># 4. Loop through each number of epochs</span>
    <span class="k">for</span> <span class="n">epochs</span> <span class="ow">in</span> <span class="n">num_epochs</span><span class="p">:</span> 

        <span class="c1"># 5. Loop through each model name and create a new model based on the name</span>
        <span class="k">for</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>

            <span class="c1"># 6. Create information print outs</span>
            <span class="n">experiment_number</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Experiment number: </span><span class="si">{</span><span class="n">experiment_number</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] DataLoader: </span><span class="si">{</span><span class="n">dataloader_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Number of epochs: </span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  

            <span class="c1"># 7. Select the model</span>
            <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;effnetb0&quot;</span><span class="p">:</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">create_effnetb0</span><span class="p">()</span> <span class="c1"># creates a new model each time (important because we want each experiment to start from scratch)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">create_effnetb2</span><span class="p">()</span> <span class="c1"># creates a new model each time (important because we want each experiment to start from scratch)</span>
            
            <span class="c1"># 8. Create a new loss and optimizer for every model</span>
            <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

            <span class="c1"># 9. Train target model with target dataloaders and track experiments</span>
            <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                  <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
                  <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span> 
                  <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                  <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                  <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                  <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                  <span class="n">writer</span><span class="o">=</span><span class="n">create_writer</span><span class="p">(</span><span class="n">experiment_name</span><span class="o">=</span><span class="n">dataloader_name</span><span class="p">,</span>
                                       <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
                                       <span class="n">extra</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">_epochs&quot;</span><span class="p">))</span>
            
            <span class="c1"># 10. Save the model to file so we can get back the best model</span>
            <span class="n">save_filepath</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;07_</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">dataloader_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">_epochs.pth&quot;</span>
            <span class="n">save_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                       <span class="n">target_dir</span><span class="o">=</span><span class="s2">&quot;models&quot;</span><span class="p">,</span>
                       <span class="n">model_name</span><span class="o">=</span><span class="n">save_filepath</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">50</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Experiment number: 1
[INFO] Model: effnetb0
[INFO] DataLoader: data_10_percent
[INFO] Number of epochs: 5
[INFO] Created new effnetb0 model.
[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "7f724e8d22604328b6f2c69ab0b3948f", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678
Epoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203
Epoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258
Epoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864
Epoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864
[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth
--------------------------------------------------

[INFO] Experiment number: 2
[INFO] Model: effnetb2
[INFO] DataLoader: data_10_percent
[INFO] Number of epochs: 5
[INFO] Created new effnetb2 model.
[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/5_epochs...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "36ca9faf96d443b38c6e3f71c427c567", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610
Epoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144
Epoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176
Epoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081
Epoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873
[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth
--------------------------------------------------

[INFO] Experiment number: 3
[INFO] Model: effnetb0
[INFO] DataLoader: data_10_percent
[INFO] Number of epochs: 10
[INFO] Created new effnetb0 model.
[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/10_epochs...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "85b88ac8b65a41139edf9ef59763f6cc", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678
Epoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203
Epoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258
Epoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864
Epoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864
Epoch: 6 | train_loss: 0.5611 | train_acc: 0.8984 | test_loss: 0.5949 | test_acc: 0.8864
Epoch: 7 | train_loss: 0.5573 | train_acc: 0.7930 | test_loss: 0.5566 | test_acc: 0.8864
Epoch: 8 | train_loss: 0.4702 | train_acc: 0.9492 | test_loss: 0.5176 | test_acc: 0.8759
Epoch: 9 | train_loss: 0.5728 | train_acc: 0.7773 | test_loss: 0.5095 | test_acc: 0.8873
Epoch: 10 | train_loss: 0.4794 | train_acc: 0.8242 | test_loss: 0.4640 | test_acc: 0.9072
[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth
--------------------------------------------------

[INFO] Experiment number: 4
[INFO] Model: effnetb2
[INFO] DataLoader: data_10_percent
[INFO] Number of epochs: 10
[INFO] Created new effnetb2 model.
[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/10_epochs...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b4df178559d448539d3e159fb9a3b0fb", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610
Epoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144
Epoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176
Epoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081
Epoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873
Epoch: 6 | train_loss: 0.6111 | train_acc: 0.7812 | test_loss: 0.6325 | test_acc: 0.9280
Epoch: 7 | train_loss: 0.6127 | train_acc: 0.8008 | test_loss: 0.6404 | test_acc: 0.8769
Epoch: 8 | train_loss: 0.5202 | train_acc: 0.9336 | test_loss: 0.6200 | test_acc: 0.8977
Epoch: 9 | train_loss: 0.5425 | train_acc: 0.8008 | test_loss: 0.6227 | test_acc: 0.8466
Epoch: 10 | train_loss: 0.4908 | train_acc: 0.8125 | test_loss: 0.5870 | test_acc: 0.8873
[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth
--------------------------------------------------

[INFO] Experiment number: 5
[INFO] Model: effnetb0
[INFO] DataLoader: data_20_percent
[INFO] Number of epochs: 5
[INFO] Created new effnetb0 model.
[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/5_epochs...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "067d7002a70443edb72e0dc5f61b60d1", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655
Epoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176
Epoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176
Epoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176
Epoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176
[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth
--------------------------------------------------

[INFO] Experiment number: 6
[INFO] Model: effnetb2
[INFO] DataLoader: data_20_percent
[INFO] Number of epochs: 5
[INFO] Created new effnetb2 model.
[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/5_epochs...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "50eee46e57ec47948c11b0b51a2460e3", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153
Epoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873
Epoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280
Epoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186
Epoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489
[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth
--------------------------------------------------

[INFO] Experiment number: 7
[INFO] Model: effnetb0
[INFO] DataLoader: data_20_percent
[INFO] Number of epochs: 10
[INFO] Created new effnetb0 model.
[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/10_epochs...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "564c35143a874dd1ad829e03034101f2", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655
Epoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176
Epoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176
Epoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176
Epoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176
Epoch: 6 | train_loss: 0.3705 | train_acc: 0.8854 | test_loss: 0.3568 | test_acc: 0.9072
Epoch: 7 | train_loss: 0.3551 | train_acc: 0.9250 | test_loss: 0.3187 | test_acc: 0.9072
Epoch: 8 | train_loss: 0.3745 | train_acc: 0.8938 | test_loss: 0.3349 | test_acc: 0.8873
Epoch: 9 | train_loss: 0.2972 | train_acc: 0.9396 | test_loss: 0.3092 | test_acc: 0.9280
Epoch: 10 | train_loss: 0.3620 | train_acc: 0.8479 | test_loss: 0.2780 | test_acc: 0.9072
[INFO] Saving model to: models/07_effnetb0_data_20_percent_10_epochs.pth
--------------------------------------------------

[INFO] Experiment number: 8
[INFO] Model: effnetb2
[INFO] DataLoader: data_20_percent
[INFO] Number of epochs: 10
[INFO] Created new effnetb2 model.
[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/10_epochs...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "c53f44132ccf45d4aeb1e8cc18383798", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153
Epoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873
Epoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280
Epoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186
Epoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489
Epoch: 6 | train_loss: 0.3889 | train_acc: 0.9104 | test_loss: 0.4555 | test_acc: 0.8977
Epoch: 7 | train_loss: 0.3483 | train_acc: 0.9271 | test_loss: 0.4227 | test_acc: 0.9384
Epoch: 8 | train_loss: 0.3862 | train_acc: 0.8771 | test_loss: 0.4344 | test_acc: 0.9280
Epoch: 9 | train_loss: 0.3308 | train_acc: 0.8979 | test_loss: 0.4242 | test_acc: 0.9384
Epoch: 10 | train_loss: 0.3383 | train_acc: 0.8896 | test_loss: 0.3906 | test_acc: 0.9384
[INFO] Saving model to: models/07_effnetb2_data_20_percent_10_epochs.pth
--------------------------------------------------

CPU times: user 29.5 s, sys: 1min 28s, total: 1min 58s
Wall time: 2min 33s
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="view-experiments-in-tensorboard">
<h2>8. View experiments in TensorBoard<a class="headerlink" href="#view-experiments-in-tensorboard" title="Permalink to this heading">#</a></h2>
<p>Ho, ho!</p>
<p>Look at us go!</p>
<p>Training eight models in one go?</p>
<p>Now that’s living up to the motto!</p>
<p><em>Experiment, experiment, experiment!</em></p>
<p>How about we check out the results in TensorBoard?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)</span>
<span class="c1"># %load_ext tensorboard</span>
<span class="c1"># %tensorboard --logdir runs</span>
</pre></div>
</div>
</div>
</div>
<p>Running the cell above we should get an output similar to the following.</p>
<blockquote>
<div><p><strong>Note:</strong> Depending on the random seeds you used/hardware you used there’s a chance your numbers aren’t exactly the same as what’s here. This is okay. It’s due to the inheret randomness of deep learning. What matters most is the trend. Where your numbers are heading. If they’re off by a large amount, perhaps there’s something wrong and best to go back and check the code. But if they’re off by a small amount (say a couple of decimal places or so), that’s okay.</p>
</div></blockquote>
<img alt="various modelling experiments visualized on tensorboard with model that has the lowest test loss highlighted" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/07-tensorboard-lowest-test-loss.png" />
<p><em>Visualizing the test loss values for the different modelling experiments in TensorBoard, you can see that the EffNetB2 model trained for 10 epochs and with 20% of the data achieves the lowest loss. This sticks with the overall trend of the experiments that: more data, larger model and longer training time is generally better.</em></p>
<p>You can also upload your TensorBoard experiment results to <a class="reference external" href="https://tensorboard.dev">tensorboard.dev</a> to host them publically for free.</p>
<p>For example, running code similiar to the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Upload the results to TensorBoard.dev (uncomment to try it out)</span>
<span class="c1"># !tensorboard dev upload --logdir runs \</span>
<span class="c1">#     --name &quot;07. PyTorch Experiment Tracking: FoodVision Mini model results&quot; \</span>
<span class="c1">#     --description &quot;Comparing results of different model size, training data amount and training time.&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Running the cell above results in the experiments from this notebook being publically viewable at: <a class="reference external" href="https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/">https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/</a></p>
<blockquote>
<div><p><strong>Note:</strong> Beware that anything you upload to tensorboard.dev is publically available for anyone to see. So if you do upload your experiments, be careful they don’t contain sensitive information.</p>
</div></blockquote>
</section>
<section id="load-in-the-best-model-and-make-predictions-with-it">
<h2>9. Load in the best model and make predictions with it<a class="headerlink" href="#load-in-the-best-model-and-make-predictions-with-it" title="Permalink to this heading">#</a></h2>
<p>Looking at the TensorBoard logs for our eight experiments, it seems experiment number eight achieved the best overall results (highest test accuracy, second lowest test loss).</p>
<p>This is the experiment that used:</p>
<ul class="simple">
<li><p>EffNetB2 (double the parameters of EffNetB0)</p></li>
<li><p>20% pizza, steak, sushi training data (double the original training data)</p></li>
<li><p>10 epochs (double the original training time)</p></li>
</ul>
<p>In essence, our biggest model achieved the best results.</p>
<p>Though it wasn’t as if these results were far better than the other models.</p>
<p>The same model on the same data achieved similar results in half the training time (experiment number 6).</p>
<p>This suggests that potentially the most influential parts of our experiments were the number of parameters and the amount of data.</p>
<p>Inspecting the results further it seems that generally a model with more parameters (EffNetB2) and more data (20% pizza, steak, sushi training data) performs better (lower test loss and higher test accuracy).</p>
<p>More experiments could be done to further test this but for now, let’s import our best performing model from experiment eight (saved to: <code class="docutils literal notranslate"><span class="pre">models/07_effnetb2_data_20_percent_10_epochs.pth</span></code>, you can <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/models/07_effnetb2_data_20_percent_10_epochs.pth">download this model from the course GitHub</a>) and perform some qualitative evaluations.</p>
<p>In other words, let’s <em>visualize, visualize, visualize!</em></p>
<p>We can import the best saved model by creating a new instance of EffNetB2 using the <code class="docutils literal notranslate"><span class="pre">create_effnetb2()</span></code> function and then load in the saved <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> with <code class="docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup the best model filepath</span>
<span class="n">best_model_path</span> <span class="o">=</span> <span class="s2">&quot;models/07_effnetb2_data_20_percent_10_epochs.pth&quot;</span>

<span class="c1"># Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">create_effnetb2</span><span class="p">()</span>

<span class="c1"># Load the saved best model state_dict()</span>
<span class="n">best_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">best_model_path</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Created new effnetb2 model.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;All keys matched successfully&gt;
</pre></div>
</div>
</div>
</div>
<p>Best model loaded!</p>
<p>While we’re here, let’s check its filesize.</p>
<p>This is an important consideration later on when deploying the model (incorporating it in an app).</p>
<p>If the model is too large, it can be hard to deploy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the model file size</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># Get the model size in bytes then convert to megabytes</span>
<span class="n">effnetb2_model_size</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">best_model_path</span><span class="p">)</span><span class="o">.</span><span class="n">stat</span><span class="p">()</span><span class="o">.</span><span class="n">st_size</span> <span class="o">//</span> <span class="p">(</span><span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;EfficientNetB2 feature extractor model size: </span><span class="si">{</span><span class="n">effnetb2_model_size</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>EfficientNetB2 feature extractor model size: 29 MB
</pre></div>
</div>
</div>
</div>
<p>Looks like our best model so far is 29 MB in size. We’ll keep this in mind if we wanted to deploy it later on.</p>
<p>Time to make and visualize some predictions.</p>
<p>We created a <code class="docutils literal notranslate"><span class="pre">pred_and_plot_image()</span></code> function use a trained model to make predictions on an image in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set">06. PyTorch Transfer Learning section 6</a>.</p>
<p>And we can reuse this function by importing it from <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py"><code class="docutils literal notranslate"><span class="pre">going_modular.going_modular.predictions.py</span></code></a> (I put the <code class="docutils literal notranslate"><span class="pre">pred_and_plot_image()</span></code> function in a script so we could reuse it).</p>
<p>So to make predictions on various images the model hasn’t seen before, we’ll first get a list of all the image filepaths from the 20% pizza, steak, sushi testing dataset and then we’ll randomly select a subset of these filepaths to pass to our <code class="docutils literal notranslate"><span class="pre">pred_and_plot_image()</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import function to make predictions on images and plot them </span>
<span class="c1"># See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set</span>
<span class="kn">from</span> <span class="nn">going_modular.going_modular.predictions</span> <span class="kn">import</span> <span class="n">pred_and_plot_image</span>

<span class="c1"># Get a random list of 3 images from 20% test set</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">num_images_to_plot</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">test_image_path_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">data_20_percent_path</span> <span class="o">/</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*/*.jpg&quot;</span><span class="p">))</span> <span class="c1"># get all test image paths from 20% dataset</span>
<span class="n">test_image_path_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">population</span><span class="o">=</span><span class="n">test_image_path_list</span><span class="p">,</span>
                                       <span class="n">k</span><span class="o">=</span><span class="n">num_images_to_plot</span><span class="p">)</span> <span class="c1"># randomly select k number of images</span>

<span class="c1"># Iterate through random test image paths, make predictions on them and plot them</span>
<span class="k">for</span> <span class="n">image_path</span> <span class="ow">in</span> <span class="n">test_image_path_sample</span><span class="p">:</span>
    <span class="n">pred_and_plot_image</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">best_model</span><span class="p">,</span>
                        <span class="n">image_path</span><span class="o">=</span><span class="n">image_path</span><span class="p">,</span>
                        <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span>
                        <span class="n">image_size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/482a6f513095a0c85933773661974e3998305176c5bc48c36141d4ce0927ac55.png" src="../../_images/482a6f513095a0c85933773661974e3998305176c5bc48c36141d4ce0927ac55.png" />
<img alt="../../_images/43c9c1d3a31cf81dce8df8dd41c1797445f1ab21c87cf3d2a044aac5715c044b.png" src="../../_images/43c9c1d3a31cf81dce8df8dd41c1797445f1ab21c87cf3d2a044aac5715c044b.png" />
<img alt="../../_images/0bbc42dbfefdbb6cdbe815cfd564d0f543278e0a18114e76d360b1b7718e0783.png" src="../../_images/0bbc42dbfefdbb6cdbe815cfd564d0f543278e0a18114e76d360b1b7718e0783.png" />
</div>
</div>
<p>Nice!</p>
<p>Running the cell above a few times we can see our model performs quite well and often has higher prediction probabilities than previous models we’ve built.</p>
<p>This suggests the model is more confident in the decisions it’s making.</p>
<section id="predict-on-a-custom-image-with-the-best-model">
<h3>9.1 Predict on a custom image with the best model<a class="headerlink" href="#predict-on-a-custom-image-with-the-best-model" title="Permalink to this heading">#</a></h3>
<p>Making predictions on the test dataset is cool but the real magic of machine learning is making predictions on custom images of your own.</p>
<p>So let’s import the trusty <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg">pizza dad image</a> (a photo of my dad in front of a pizza) we’ve been using for the past couple of sections and see how our model performs on it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download custom image</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="c1"># Setup custom image path</span>
<span class="n">custom_image_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;data/04-pizza-dad.jpeg&quot;</span><span class="p">)</span>

<span class="c1"># Download the image if it doesn&#39;t already exist</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">custom_image_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">custom_image_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="c1"># When downloading from GitHub, need to use the &quot;raw&quot; file link</span>
        <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-pizza-dad.jpeg&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Downloading </span><span class="si">{</span><span class="n">custom_image_path</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">custom_image_path</span><span class="si">}</span><span class="s2"> already exists, skipping download.&quot;</span><span class="p">)</span>

<span class="c1"># Predict on custom image</span>
<span class="n">pred_and_plot_image</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                    <span class="n">image_path</span><span class="o">=</span><span class="n">custom_image_path</span><span class="p">,</span>
                    <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>data/04-pizza-dad.jpeg already exists, skipping download.
</pre></div>
</div>
<img alt="../../_images/2b9a7c42052f121b0bc1f7a5c29f87d98e8e851831c1ac35b741eab675159c38.png" src="../../_images/2b9a7c42052f121b0bc1f7a5c29f87d98e8e851831c1ac35b741eab675159c38.png" />
</div>
</div>
<p>Woah!</p>
<p>Two thumbs again!</p>
<p>Our best model predicts “pizza” correctly and this time with an even higher prediction probability (0.978) than the first feature extraction model we trained and used in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#61-making-predictions-on-a-custom-image">06. PyTorch Transfer Learning section 6.1</a>.</p>
<p>This again suggests our current best model (EffNetB2 feature extractor trained on 20% of the pizza, steak, sushi training data and for 10 epochs) has learned patterns to make it more confident of its decision to predict pizza.</p>
<p>I wonder what could improve our model’s performance even further?</p>
<p>I’ll leave that as a challenge for you to investigate.</p>
</section>
</section>
<section id="main-takeaways">
<h2>Main takeaways<a class="headerlink" href="#main-takeaways" title="Permalink to this heading">#</a></h2>
<p>We’ve now gone full circle on the PyTorch workflow introduced in <a class="reference external" href="https://www.learnpytorch.io/01_pytorch_workflow/">01. PyTorch Workflow Fundamentals</a>, we’ve gotten data ready, we’ve built and picked a pretrained model, we’ve used our various helper functions to train and evaluate the model and in this notebook we’ve improved our FoodVision Mini model by running and tracking a series of experiments.</p>
<img alt="a pytorch workflow flowchat" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png" />
<p>You should be proud of yourself, this is no small feat!</p>
<p>The main ideas you should take away from this Milestone Project 1 are:</p>
<ul class="simple">
<li><p>The machine learning practioner’s motto: <em>experiment, experiment, experiment!</em> (though we’ve been doing plenty of this already).</p></li>
<li><p>In the beginning, keep your experiments small so you can work fast, your first few experiments shouldn’t take more than a few seconds to a few minutes to run.</p></li>
<li><p>The more experiments you do, the quicker you can figure out what <em>doesn’t</em> work.</p></li>
<li><p>Scale up when you find something that works. For example, since we’ve found a pretty good performing model with EffNetB2 as a feature extractor, perhaps you’d now like to see what happens when you scale it up to the whole <a class="reference external" href="https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html">Food101 dataset</a> from <code class="docutils literal notranslate"><span class="pre">torchvision.datasets</span></code>.</p></li>
<li><p>Programmatically tracking your experiments takes a few steps to set up but it’s worth it in the long run so you can figure out what works and what doesn’t.</p>
<ul>
<li><p>There are many different machine learning experiment trackers out there so explore a few and try them out.</p></li>
</ul>
</li>
</ul>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p><strong>Note:</strong> These exercises expect the use of <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> v0.13+ (released July 2022), previous versions may work but will likely have errors.</p>
</div></blockquote>
<p>All of the exercises are focused on practicing the code above.</p>
<p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p>
<p>All exercises should be completed using <a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">device-agnostic code</a>.</p>
<p><strong>Resources:</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/exercises/07_pytorch_experiment_tracking_exercise_template.ipynb">Exercise template notebook for 07</a></p></li>
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/solutions/07_pytorch_experiment_tracking_exercise_solutions.ipynb">Example solutions notebook for 07</a> (try the exercises <em>before</em> looking at this)</p>
<ul>
<li><p>See a live <a class="reference external" href="https://youtu.be/cO_r2FYcAjU">video walkthrough of the solutions on YouTube</a> (errors and all)</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple">
<li><p>Pick a larger model from <a class="reference external" href="https://pytorch.org/vision/main/models.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a> to add to the list of experiments (for example, EffNetB3 or higher).</p>
<ul class="simple">
<li><p>How does it perform compared to our existing models?</p></li>
</ul>
</li>
<li><p>Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?</p>
<ul class="simple">
<li><p>For example, you could have one training DataLoader that uses data augmentation (e.g. <code class="docutils literal notranslate"><span class="pre">train_dataloader_20_percent_aug</span></code> and <code class="docutils literal notranslate"><span class="pre">train_dataloader_20_percent_no_aug</span></code>) and then compare the results of two of the same model types training on these two DataLoaders.</p></li>
<li><p><strong>Note:</strong> You may need to alter the <code class="docutils literal notranslate"><span class="pre">create_dataloaders()</span></code> function to be able to take a transform for the training data and the testing data (because you don’t need to perform data augmentation on the test data). See <a class="reference external" href="https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation">04. PyTorch Custom Datasets section 6</a> for examples of using data augmentation or the script below for an example:</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: Data augmentation transform like this should only be performed on training data</span>
<span class="n">train_transform_data_aug</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">TrivialAugmentWide</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">normalize</span>
<span class="p">])</span>

<span class="c1"># Helper function to view images in a DataLoader (works with data augmentation transforms or not) </span>
<span class="k">def</span> <span class="nf">view_dataloader_images</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Having n higher than 10 will create messy plots, lowering to 10.&quot;</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># Min max scale the image for display purposes</span>
        <span class="n">targ_image</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">sample_min</span><span class="p">,</span> <span class="n">sample_max</span> <span class="o">=</span> <span class="n">targ_image</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">targ_image</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">sample_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">targ_image</span> <span class="o">-</span> <span class="n">sample_min</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">sample_max</span> <span class="o">-</span> <span class="n">sample_min</span><span class="p">)</span>

        <span class="c1"># Plot images with appropriate axes information</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sample_scaled</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># resize for Matplotlib requirements</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Have to update `create_dataloaders()` to handle different augmentations</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="c1"># use maximum number of CPUs for workers to load data </span>

<span class="c1"># Note: this is an update version of data_setup.create_dataloaders to handle</span>
<span class="c1"># differnt train and test transforms.</span>
<span class="k">def</span> <span class="nf">create_dataloaders</span><span class="p">(</span>
    <span class="n">train_dir</span><span class="p">,</span> 
    <span class="n">test_dir</span><span class="p">,</span> 
    <span class="n">train_transform</span><span class="p">,</span> <span class="c1"># add parameter for train transform (transforms on train dataset)</span>
    <span class="n">test_transform</span><span class="p">,</span>  <span class="c1"># add parameter for test transform (transforms on test dataset)</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span>
<span class="p">):</span>
    <span class="c1"># Use ImageFolder to create dataset(s)</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">train_transform</span><span class="p">)</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">test_transform</span><span class="p">)</span>

    <span class="c1"># Get class names</span>
    <span class="n">class_names</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">classes</span>

    <span class="c1"># Turn images into data loaders</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">train_data</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">test_data</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision.datasets.Food101">Food101 dataset from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a></p>
<ul class="simple">
<li><p>You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.</p></li>
<li><p>If you try more than one model, it would be good to have the model’s results tracked.</p></li>
<li><p>If you load the Food101 dataset from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code>, you’ll have to create PyTorch DataLoaders to use it in training.</p></li>
<li><p><strong>Note:</strong> Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train.</p></li>
</ul>
</li>
</ol>
</section>
<section id="extra-curriculum">
<h2>Extra-curriculum<a class="headerlink" href="#extra-curriculum" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Read <a class="reference external" href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a> blog post by Richard Sutton to get an idea of how many of the latest advancements in AI have come from increased scale (bigger datasets and bigger models) and more general (less meticulously crafted) methods.</p></li>
<li><p>Go through the <a class="reference external" href="https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html">PyTorch YouTube/code tutorial</a> for TensorBoard for 20-minutes and see how it compares to the code we’ve written in this notebook.</p></li>
<li><p>Perhaps you may want to view and rearrange your model’s TensorBoard logs with a DataFrame (so you can sort the results by lowest loss or highest accuracy), there’s a guide for this <a class="reference external" href="https://www.tensorflow.org/tensorboard/dataframe_api">in the TensorBoard documentation</a>.</p></li>
<li><p>If you like to use VSCode for development using scripts or notebooks (VSCode can now use Jupyter Notebooks natively), you can setup TensorBoard right within VSCode using the  <a class="reference external" href="https://code.visualstudio.com/docs/datascience/pytorch-support">PyTorch Development in VSCode guide</a>.</p></li>
<li><p>To go further with experiment tracking and see how your PyTorch model is performing from a speed perspective (are there any bottlenecks that could be improved to speed up training?), see the <a class="reference external" href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/">PyTorch documentation for the PyTorch profiler</a>.</p></li>
<li><p>Made With ML is an outstanding resource for all things machine learning by Goku Mohandas and their <a class="reference external" href="https://madewithml.com/courses/mlops/experiment-tracking/">guide on experiment tracking</a> contains a fantastic introduction to tracking machine learning experiments with MLflow.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook/pytorch_deep_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="06_pytorch_transfer_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">06. PyTorch Transfer Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="08_pytorch_paper_replicating.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">08. PyTorch Paper Replicating</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-experiment-tracking">What is experiment tracking?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-track-experiments">Why track experiments?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-ways-to-track-machine-learning-experiments">Different ways to track machine learning experiments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-were-going-to-cover">What we’re going to cover</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-can-you-get-help">Where can you get help?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-setup">0. Getting setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-helper-function-to-set-seeds">Create a helper function to set seeds</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-data">1. Get data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-datasets-and-dataloaders">2. Create Datasets and DataLoaders</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataloaders-using-manually-created-transforms">2.1 Create DataLoaders using manually created transforms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-dataloaders-using-automatically-created-transforms">2.2 Create DataLoaders using automatically created transforms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head">3. Getting a pretrained model, freezing the base layers and changing the classifier head</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-model-and-track-results">4. Train model and track results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjust-train-function-to-track-results-with-summarywriter">Adjust <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to track results with <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#view-our-models-results-in-tensorboard">5. View our model’s results in TensorBoard</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-helper-function-to-build-summarywriter-instances">6. Create a helper function to build <code class="docutils literal notranslate"><span class="pre">SummaryWriter()</span></code> instances</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#update-the-train-function-to-include-a-writer-parameter">6.1 Update the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function to include a <code class="docutils literal notranslate"><span class="pre">writer</span></code> parameter</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-a-series-of-modelling-experiments">7. Setting up a series of modelling experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-kind-of-experiments-should-you-run">7.1 What kind of experiments should you run?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-experiments-are-we-going-to-run">7.2 What experiments are we going to run?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-different-datasets">7.3 Download different datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transform-datasets-and-create-dataloaders">7.4 Transform Datasets and create DataLoaders</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-feature-extractor-models">7.5 Create feature extractor models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-experiments-and-set-up-training-code">7.6 Create experiments and set up training code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#view-experiments-in-tensorboard">8. View experiments in TensorBoard</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-in-the-best-model-and-make-predictions-with-it">9. Load in the best model and make predictions with it</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-on-a-custom-image-with-the-best-model">9.1 Predict on a custom image with the best model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-takeaways">Main takeaways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extra-curriculum">Extra-curriculum</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By thangckt
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>