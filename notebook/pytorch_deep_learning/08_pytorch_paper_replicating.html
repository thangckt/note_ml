

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>08. PyTorch Paper Replicating</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebook/pytorch_deep_learning/08_pytorch_paper_replicating';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="09. PyTorch Model Deployment" href="09_pytorch_model_deployment.html" />
    <link rel="prev" title="07. PyTorch Experiment Tracking" href="07_pytorch_experiment_tracking.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
  
    <p class="title logo__title"></p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Basic of ML &amp; DL</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../0_basic_MLDL/1_0_ml_overview.html">Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/1_1_ml_supervised_unsuppersives.html">Supervised vs. Unsuppervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/1_2_regression.html">Regression &amp; Model Assessment</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../0_basic_MLDL/2_0_dl_overview.html">Deep Learning Overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/2_1_dl_neural_network.html">What is a neural network?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../0_basic_MLDL/2_2_layers.html">Standard Layers</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../0_basic_MLDL/3_1_workflow.html">Workflow in ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_basic_MLDL/3_2_Model_template.html">Core Ml templates</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">PyTorch for Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="00_pytorch_fundamentals.html">00. PyTorch Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_pytorch_workflow.html">01. PyTorch Workflow Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_pytorch_classification.html">02. PyTorch Neural Network Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_pytorch_computer_vision.html">03. PyTorch Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_pytorch_custom_datasets.html">04. PyTorch Custom Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_pytorch_going_modular.html">05. PyTorch Going Modular</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_pytorch_transfer_learning.html">06. PyTorch Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_pytorch_experiment_tracking.html">07. PyTorch Experiment Tracking</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">08. PyTorch Paper Replicating</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_pytorch_model_deployment.html">09. PyTorch Model Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_extra_resources.html">PyTorch Extra Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_cheatsheet.html">PyTorch Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_most_common_errors.html">The Three Most Common Errors in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="extras/pytorch_setup.html">Setup to code PyTorch</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Zero to Mastery Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../zero_to_mastery_ml/README.html">Zero to Mastery Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Practices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1_Practices/1_PT_Linear_Regression.html">Linear Regression</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_Practices/2_PT_Logistic_Regression.html">Logistic Regression</a></li>





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/thangckt/note_ml/edit/main/notebook/pytorch_deep_learning/08_pytorch_paper_replicating.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button"
   title="Suggest edit"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>

</a>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>08. PyTorch Paper Replicating</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-paper-replicating">What is paper replicating?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-machine-learning-research-paper">What is a machine learning research paper?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-replicate-a-machine-learning-research-paper">Why replicate a machine learning research paper?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-can-you-find-code-examples-for-machine-learning-research-papers">Where can you find code examples for machine learning research papers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-were-going-to-cover">What we’re going to cover</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology">Terminology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-can-you-get-help">Where can you get help?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-setup">0. Getting setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-data">1. Get Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-datasets-and-dataloaders">2. Create Datasets and DataLoaders</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-transforms-for-images">2.1 Prepare transforms for images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-images-into-dataloaders">2.2 Turn images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-a-single-image">2.3 Visualize a single image</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-the-vit-paper-an-overview">3. Replicating the ViT paper: an overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs-layers-and-blocks">3.1 Inputs and outputs, layers and blocks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-specific-whats-vit-made-of">3.2 Getting specific: What’s ViT made of?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-figure-1">3.2.1 Exploring Figure 1</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-the-four-equations">3.2.2 Exploring the Four Equations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-1-overview">3.2.3 Equation 1 overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-2-overview">3.2.4 Equation 2 overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-3-overview">3.2.5 Equation 3 overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-4-overview">3.2.6 Equation 4 overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-table-1">3.2.7 Exploring Table 1</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#my-workflow-for-replicating-papers">3.3 My workflow for replicating papers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding">4. Equation 1: Split data into patches and creating the class, position and patch embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-patch-embedding-input-and-output-shapes-by-hand">4.1 Calculating patch embedding input and output shapes by hand</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turning-a-single-image-into-patches">4.2 Turning a single image into patches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-image-patches-with-torch-nn-conv2d">4.3 Creating image patches with <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flattening-the-patch-embedding-with-torch-nn-flatten">4.4 Flattening the patch embedding with <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turning-the-vit-patch-embedding-layer-into-a-pytorch-module">4.5 Turning the ViT patch embedding layer into a PyTorch module</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-class-token-embedding">4.6 Creating the class token embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-position-embedding">4.7 Creating the position embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-from-image-to-embedding">4.8 Putting it all together: from image to embedding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-2-multi-head-attention-msa">5. Equation 2: Multi-Head Attention (MSA)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-layernorm-ln-layer">5.1 The LayerNorm (LN) layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-multi-head-self-attention-msa-layer">5.2 The Multi-Head Self Attention (MSA) layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-equation-2-with-pytorch-layers">5.3 Replicating Equation 2 with PyTorch layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-3-multilayer-perceptron-mlp">6. Equation 3: Multilayer Perceptron (MLP)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mlp-layer-s">6.1 The MLP layer(s)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-equation-3-with-pytorch-layers">6.2 Replicating Equation 3 with PyTorch layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-transformer-encoder">7. Create the Transformer Encoder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-transformer-encoder-by-combining-our-custom-made-layers">7.1 Creating a Transformer Encoder by combining our custom made layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-transformer-encoder-with-pytorchs-transformer-layers">7.2 Creating a Transformer Encoder with PyTorch’s Transformer layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-to-create-vit">8. Putting it all together to create ViT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-visual-summary-of-our-vit-model">8.1 Getting a visual summary of our ViT model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-training-code-for-our-vit-model">9. Setting up training code for our ViT model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-an-optimizer">9.1 Creating an optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-loss-function">9.2 Creating a loss function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-our-vit-model">9.3 Training our ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-our-training-setup-is-missing">9.4 What our training setup is missing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss-curves-of-our-vit-model">9.5 Plot the loss curves of our ViT model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-a-pretrained-vit-from-torchvision-models-on-the-same-dataset">10. Using a pretrained ViT from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> on the same dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-a-pretrained-model">10.1 Why use a pretrained model?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-pretrained-vit-model-and-creating-a-feature-extractor">10.2 Getting a pretrained ViT model and creating a feature extractor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-data-for-the-pretrained-vit-model">10.3 Preparing data for the pretrained ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-feature-extractor-vit-model">10.4 Train feature extractor ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-feature-extractor-vit-model-loss-curves">10.5 Plot feature extractor ViT model loss curves</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-feature-extractor-vit-model-and-check-file-size">10.6 Save feature extractor ViT model and check file size</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-predictions-on-a-custom-image">11. Make predictions on a custom image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-takeaways">Main takeaways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extra-curriculum">Extra-curriculum</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/thangckt/pytorch-deep-learning/blob/main/08_pytorch_paper_replicating.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p><a class="reference external" href="https://thangckt.github.io/pytorch_deep_learning/slides/08_pytorch_paper_replicating.pdf">View Slides</a></p>
<section class="tex2jax_ignore mathjax_ignore" id="pytorch-paper-replicating">
<h1>08. PyTorch Paper Replicating<a class="headerlink" href="#pytorch-paper-replicating" title="Permalink to this heading">#</a></h1>
<p>Welcome to Milestone Project 2: PyTorch Paper Replicating!</p>
<p>In this project, we’re going to be <strong>replicating a machine learning research paper</strong> and creating a Vision Transformer (ViT) from scratch using PyTorch.</p>
<p>We’ll then see how ViT, a state-of-the-art computer vision architecture, performs on our FoodVision Mini problem.</p>
<img alt="appyling the vision transformer architecture to FoodVision mini" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-applying-vit-to-food-vision-mini.png" />
<p><em>For Milestone Project 2 we’re going to focus on recreating the Vision Transformer (ViT) computer vision architecture and applying it to our FoodVision Mini problem to classify different images of pizza, steak and sushi.</em></p>
<section id="what-is-paper-replicating">
<h2>What is paper replicating?<a class="headerlink" href="#what-is-paper-replicating" title="Permalink to this heading">#</a></h2>
<p>It’s no secret machine learning is advancing fast.</p>
<p>Many of these advances get published in machine learning research papers.</p>
<p>And the goal of <strong>paper replicating</strong> is to take replicate these advances with code so you can use the techniques for your own problem.</p>
<p>For example, let’s say a new model architecture gets released that performs better than any other architecture before on various benchmarks, wouldn’t it be nice to try that architecture on your own problems?</p>
<img alt="paper replicating involves turning a machine learning reserch paper comprised of images/diagrams, text and math into usable code" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-what-is-paper-replicating-images-math-text-to-code.png" />
<p><em>Machine learning paper replicating involves turning a machine learning paper comprised of images/diagrams, math and text into usable code and in our case, usable PyTorch code. Diagram, math equations and text from the <a class="reference external" href="https://arxiv.org/abs/2010.11929">ViT paper</a>.</em></p>
</section>
<section id="what-is-a-machine-learning-research-paper">
<h2>What is a machine learning research paper?<a class="headerlink" href="#what-is-a-machine-learning-research-paper" title="Permalink to this heading">#</a></h2>
<p>A machine learning research paper is a scientific paper that details findings of a research group on a specific area.</p>
<p>The contents of a machine learning research paper can vary from paper to paper but they generally follow the structure:</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Section</strong></p></th>
<th class="head"><p><strong>Contents</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Abstract</strong></p></td>
<td><p>An overview/summary of the paper’s main findings/contributions.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Introduction</strong></p></td>
<td><p>What’s the paper’s main problem and details of previous methods used to try and solve it.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Method</strong></p></td>
<td><p>How did the researchers go about conducting their research? For example, what model(s), data sources, training setups were used?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Results</strong></p></td>
<td><p>What are the outcomes of the paper? If a new type of model or training setup was used, how did the results of findings compare to previous works? (this is where <strong>experiment tracking</strong> comes in handy)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Conclusion</strong></p></td>
<td><p>What are the limitations of the suggested methods? What are some next steps for the research community?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>References</strong></p></td>
<td><p>What resources/other papers did the researchers look at to build their own body of work?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Appendix</strong></p></td>
<td><p>Are there any extra resources/findings to look at that weren’t included in any of the above sections?</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="why-replicate-a-machine-learning-research-paper">
<h2>Why replicate a machine learning research paper?<a class="headerlink" href="#why-replicate-a-machine-learning-research-paper" title="Permalink to this heading">#</a></h2>
<p>A machine learning research paper is often a presentation of months of work and experiments done by some of the best machine learning teams in the world condensed into a few pages of text.</p>
<p>And if these experiments lead to better results in an area related to the problem you’re working on, it’d be nice to them out.</p>
<p>Also, replicating the work of others is a fantastic way to practice your skills.</p>
<img alt="george hotz quote saying to get better at being a machine learning engineer, download a paper, implement it and keep going until you have skills" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-george-hotz-quote.png" />
<p><em>George Hotz is founder of <a class="reference external" href="https://comma.ai/">comma.ai</a>, a self-driving car company and livestreams machine learning coding on <a class="reference external" href="https://www.twitch.tv/georgehotz">Twitch</a> and those videos get posted in full to <a class="reference external" href="https://www.youtube.com/c/georgehotzarchive">YouTube</a>. I pulled this quote from one of his livestreams. The “٭” is to note that machine learning engineering often involves the extra step(s) of preprocessing data and making your models available for others to use (deployment).</em></p>
<p>When you first start trying to replicate research papers, you’ll likely be overwhelmed.</p>
<p>That’s normal.</p>
<p>Research teams spend weeks, months and sometimes years creating these works so it makes sense if it takes you sometime to even read let alone reproduce the works.</p>
<p>Replicating research is such a tough problem, phenomenal machine learning libraries and tools such as, <a class="reference external" href="https://huggingface.co/">HuggingFace</a>, <a class="reference external" href="https://github.com/rwightman/pytorch-image-models">PyTorch Image Models</a> (<code class="docutils literal notranslate"><span class="pre">timm</span></code> library) and <a class="reference external" href="https://www.fast.ai/">fast.ai</a> have been born out of making machine learning research more accessible.</p>
</section>
<section id="where-can-you-find-code-examples-for-machine-learning-research-papers">
<h2>Where can you find code examples for machine learning research papers?<a class="headerlink" href="#where-can-you-find-code-examples-for-machine-learning-research-papers" title="Permalink to this heading">#</a></h2>
<p>One of the first things you’ll notice when it comes to machine learning research is: there’s a lot of it.</p>
<p>So beware, trying to stay on top of it is like trying to outrun a hamster wheel.</p>
<p>Follow your interest, pick a few things that stand out to you.</p>
<p>In saying this, there are several places to find and read machine learning research papers (and code):</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Resource</strong></p></th>
<th class="head"><p><strong>What is it?</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://arxiv.org/">arXiv</a></p></td>
<td><p>Pronounced “archive”, arXiv is a free and open resource for reading technical articles on everything from physics to computer science (inlcuding machine learning).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://twitter.com/ak92501">AK Twitter</a></p></td>
<td><p>The AK Twitter account publishes machine learning research highlights, often with live demos almost every day. I don’t understand 9/10 posts but I find it fun to explore every so often.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://paperswithcode.com/">Papers with Code</a></p></td>
<td><p>A curated collection of trending, active and greatest machine learning papers, many of which include code resources attached. Also includes a collection of common machine learning datasets, benchmarks and current state-of-the-art models.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/lucidrains/vit-pytorch">lucidrains’ <code class="docutils literal notranslate"><span class="pre">vit-pytorch</span></code> GitHub repository</a></p></td>
<td><p>Less of a place to find research papers and more of an example of what paper replicating with code on a larger-scale and with a specific focus looks like. The <code class="docutils literal notranslate"><span class="pre">vit-pytorch</span></code> repository is a collection of Vision Transformer model architectures from various research papers replicated with PyTorch code (much of the inspiration for this notebook was gathered from this repository).</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><p><strong>Note:</strong> This list is far from exhaustive. I only list a few places, the ones I use most frequently personally. So beware the bias. However, I’ve noticed that even this short list often sully satisfies my needs for knowing what’s going on in the field. Anymore and I might go crazy.</p>
</div></blockquote>
</section>
<section id="what-were-going-to-cover">
<h2>What we’re going to cover<a class="headerlink" href="#what-were-going-to-cover" title="Permalink to this heading">#</a></h2>
<p>Rather than talk about a replicating a paper, we’re going to get hands-on and <em>actually</em> replicate a paper.</p>
<p>The process for replicating all papers will be slightly different but by seeing what it’s like to do one, we’ll get the momentum to do more.</p>
<p>More specifically, we’re going to be replicating the machine learning research paper <a class="reference external" href="https://arxiv.org/abs/2010.11929"><em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em></a>  (ViT paper) with PyTorch.</p>
<p>The Transformer neural network architecture was originally introduced in the machine learning research paper <a class="reference external" href="https://arxiv.org/abs/1706.03762"><em>Attention is all you need</em></a>.</p>
<p>And the original Transformer architecture was designed to work on one-dimensional (1D) sequences of text.</p>
<p>A <strong>Transformer architecture</strong> is generally considered to be any neural network that uses the <a class="reference external" href="https://en.wikipedia.org/wiki/Attention_(machine_learning)"><strong>attention mechanism</strong></a> as its primary learning layer. Similar to a how a convolutional neural network (CNN) uses convolutions as its primary learning layer.</p>
<p>Like the name suggests, <strong>the Vision Transformer (ViT) architecture was designed to adapt the original Transformer architecture to vision problem(s)</strong> (classification being the first and since many others have followed).</p>
<p>The original Vision Transformer has been through several iterations over the past couple of years, however, we’re going to focus on replicating the original, otherwise known as the “vanilla Vision Transformer”. Because if you can recreate the original, you can adapt to the others.</p>
<p>We’re going to be focusing on building the ViT architecture as per the original ViT paper and applying it to FoodVision Mini.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Topic</strong></p></th>
<th class="head"><p><strong>Contents</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#0-getting-setup">0. Getting setup</a></strong></p></td>
<td><p>We’ve written a fair bit of useful code over the past few sections, let’s download it and make sure we can use it again.</p></td>
</tr>
<tr class="row-odd"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#1-get-data">1. Get data</a></strong></p></td>
<td><p>Let’s get the pizza, steak and sushi image classification dataset we’ve been using and build a Vision Transformer to try and improve FoodVision Mini model’s results.</p></td>
</tr>
<tr class="row-even"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#2-create-datasets-and-dataloaders">2. Create Datasets and DataLoaders</a></strong></p></td>
<td><p>We’ll use the <code class="docutils literal notranslate"><span class="pre">data_setup.py</span></code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders.</p></td>
</tr>
<tr class="row-odd"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview">3. Replicating the ViT paper: an overview</a></strong></p></td>
<td><p>Replicating a machine learning research paper can be bit a fair challenge, so before we jump in, let’s break the ViT paper down into smaller chunks, so we can replicate the paper chunk by chunk.</p></td>
</tr>
<tr class="row-even"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#4-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding">4. Equation 1: The Patch Embedding</a></strong></p></td>
<td><p>The ViT architecture is comprised of four main equations, the first being the patch and position embedding. Or turning an image into a sequence of learnable patches.</p></td>
</tr>
<tr class="row-odd"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#5-equation-2-multi-head-attention-msa">5. Equation 2: Multi-Head Attention (MSA)</a></strong></p></td>
<td><p>The self-attention/multi-head self-attention (MSA) mechanism is at the heart of every Transformer architecture, including the ViT architecture, let’s create an MSA block using PyTorch’s in-built layers.</p></td>
</tr>
<tr class="row-even"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#6-equation-3-multilayer-perceptron-mlp">6. Equation 3: Multilayer Perceptron (MLP)</a></strong></p></td>
<td><p>The ViT architecture uses a multilayer perceptron as part of its Transformer Encoder and for its output layer. Let’s start by creating an MLP for the Transformer Encoder.</p></td>
</tr>
<tr class="row-odd"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#7-create-the-transformer-encoder">7. Creating the Transformer Encode</a></strong></p></td>
<td><p>A Transformer Encoder is typically comprised of alternating layers of MSA (equation 2) and MLP (equation 3) joined together via residual connections. Let’s create one by stacking the layers we created in sections 5 &amp; 6 on top of each other.</p></td>
</tr>
<tr class="row-even"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#8-putting-it-all-together-to-create-vit">8. Putting it all together to create ViT</a></strong></p></td>
<td><p>We’ve got all the pieces of the puzzle to create the ViT architecture, let’s put them all together into a single class we can call as our model.</p></td>
</tr>
<tr class="row-odd"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#9-setting-up-training-code-for-our-vit-model">9. Setting up training code for our ViT model</a></strong></p></td>
<td><p>Training our custom ViT implementation is similar to all of the other model’s we’ve trained previously. And thanks to our <code class="docutils literal notranslate"><span class="pre">train()</span></code> function in <code class="docutils literal notranslate"><span class="pre">engine.py</span></code> we can start training with a few lines of code.</p></td>
</tr>
<tr class="row-even"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset">10. Using a pretrained ViT from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a></strong></p></td>
<td><p>Training a large model like ViT usually takes a fair amount of data. Since we’re only working with a small amount of pizza, steak and sushi images, let’s see if we can leverage the power of transfer learning to improve our performance.</p></td>
</tr>
<tr class="row-odd"><td><p><strong><a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#11-make-predictions-on-a-custom-image">11. Make predictions on a custom image</a></strong></p></td>
<td><p>The magic of machine learning is seeing it work on your own data, so let’s take our best performing model and put FoodVision Mini to the test on the infamous <em>pizza-dad</em> image (a photo of my dad eating pizza).</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><p><strong>Note:</strong> Despite the fact we’re going to be focused on replicating the ViT paper, avoid getting too bogged down on a particular paper as newer better methods will often come along, quickly, so the skill should be to remain curious whilst building the fundamental skills of turning math and words on a page into working code.</p>
</div></blockquote>
</section>
<section id="terminology">
<h2>Terminology<a class="headerlink" href="#terminology" title="Permalink to this heading">#</a></h2>
<p>There are going to be a fair few acronyms throughout this notebook.</p>
<p>In light of this, here are some definitions:</p>
<ul class="simple">
<li><p><strong>ViT</strong> - Stands for Vision Transformer (the main neural network architecture we’re going to be focused on replicating).</p></li>
<li><p><strong>ViT paper</strong> - Short hand for the original machine learning research paper that introduced the ViT architecture, <a class="reference external" href="https://arxiv.org/abs/2010.11929"><em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em></a>, anytime <em>ViT paper</em> is mentioned, you can be assured it is referencing this paper.</p></li>
</ul>
</section>
<section id="where-can-you-get-help">
<h2>Where can you get help?<a class="headerlink" href="#where-can-you-get-help" title="Permalink to this heading">#</a></h2>
<p>All of the materials for this course <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning">are available on GitHub</a>.</p>
<p>If you run into trouble, you can ask a question on the course <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/discussions">GitHub Discussions page</a>.</p>
<p>And of course, there’s the <a class="reference external" href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a> and <a class="reference external" href="https://discuss.pytorch.org/">PyTorch developer forums</a>, a very helpful place for all things PyTorch.</p>
</section>
<section id="getting-setup">
<h2>0. Getting setup<a class="headerlink" href="#getting-setup" title="Permalink to this heading">#</a></h2>
<p>As we’ve done previously, let’s make sure we’ve got all of the modules we’ll need for this section.</p>
<p>We’ll import the Python scripts (such as <code class="docutils literal notranslate"><span class="pre">data_setup.py</span></code> and <code class="docutils literal notranslate"><span class="pre">engine.py</span></code>) we created in <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/">05. PyTorch Going Modular</a>.</p>
<p>To do so, we’ll download <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/tree/main/going_modular"><code class="docutils literal notranslate"><span class="pre">going_modular</span></code></a> directory from the <code class="docutils literal notranslate"><span class="pre">pytorch-deep-learning</span></code> repository (if we don’t already have it).</p>
<p>We’ll also get the <a class="reference external" href="https://github.com/TylerYep/torchinfo"><code class="docutils literal notranslate"><span class="pre">torchinfo</span></code></a> package if it’s not available.</p>
<p><code class="docutils literal notranslate"><span class="pre">torchinfo</span></code> will help later on to give us a visual representation of our model.</p>
<p>And since later on we’ll be using <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> v0.13 package (available as of July 2022), we’ll make sure we’ve got the latest versions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">import</span> <span class="nn">torchvision</span>
    <span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">12</span><span class="p">,</span> <span class="s2">&quot;torch version should be 1.12+&quot;</span>
    <span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">13</span><span class="p">,</span> <span class="s2">&quot;torchvision version should be 0.13+&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torchvision version: </span><span class="si">{</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] torch/torchvision versions not as required, installing nightly versions.&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip3<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--extra-index-url<span class="w"> </span>https://download.pytorch.org/whl/cu113
    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">import</span> <span class="nn">torchvision</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torch version: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torchvision version: </span><span class="si">{</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch version: 1.12.0+cu102
torchvision version: 0.13.0+cu102
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> If you’re using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you’ve got the right versions of <code class="docutils literal notranslate"><span class="pre">torch</span></code> and <code class="docutils literal notranslate"><span class="pre">torchvision</span></code>.</p>
</div></blockquote>
<p>Now we’ll continue with the regular imports, setting up device agnostic code and this time we’ll also get the <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/helper_functions.py"><code class="docutils literal notranslate"><span class="pre">helper_functions.py</span></code></a> script from GitHub.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">helper_functions.py</span></code> script contains several functions we created in previous sections:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">set_seeds()</span></code> to set the random seeds (created in <a class="reference external" href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds">07. PyTorch Experiment Tracking section 0</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">download_data()</span></code> to download a data source given a link (created in <a class="reference external" href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data">07. PyTorch Experiment Tracking section 1</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plot_loss_curves()</span></code> to inspect our model’s training results (created in <a class="reference external" href="https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0">04. PyTorch Custom Datasets section 7.8</a>)</p></li>
</ul>
<blockquote>
<div><p><strong>Note:</strong> It may be a better idea for many of the functions in the <code class="docutils literal notranslate"><span class="pre">helper_functions.py</span></code> script to be merged into <code class="docutils literal notranslate"><span class="pre">going_modular/going_modular/utils.py</span></code>, perhaps that’s an extension you’d like to try.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Continue with regular imports</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="c1"># Try to get torchinfo, install it if it doesn&#39;t work</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="k">except</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO] Couldn&#39;t find torchinfo... installing it.&quot;</span><span class="p">)</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-q<span class="w"> </span>torchinfo
    <span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="c1"># Try to import the going_modular directory, download it from GitHub if it doesn&#39;t work</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">going_modular.going_modular</span> <span class="kn">import</span> <span class="n">data_setup</span><span class="p">,</span> <span class="n">engine</span>
    <span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">download_data</span><span class="p">,</span> <span class="n">set_seeds</span><span class="p">,</span> <span class="n">plot_loss_curves</span>
<span class="k">except</span><span class="p">:</span>
    <span class="c1"># Get the going_modular scripts</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO] Couldn&#39;t find going_modular or helper_functions scripts... downloading them from GitHub.&quot;</span><span class="p">)</span>
    <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/thangckt/pytorch-deep-learning
    <span class="o">!</span>mv<span class="w"> </span>pytorch-deep-learning/going_modular<span class="w"> </span>.
    <span class="o">!</span>mv<span class="w"> </span>pytorch-deep-learning/helper_functions.py<span class="w"> </span>.<span class="w"> </span>#<span class="w"> </span>get<span class="w"> </span>the<span class="w"> </span>helper_functions.py<span class="w"> </span>script
    <span class="o">!</span>rm<span class="w"> </span>-rf<span class="w"> </span>pytorch-deep-learning
    <span class="kn">from</span> <span class="nn">going_modular.going_modular</span> <span class="kn">import</span> <span class="n">data_setup</span><span class="p">,</span> <span class="n">engine</span>
    <span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">download_data</span><span class="p">,</span> <span class="n">set_seeds</span><span class="p">,</span> <span class="n">plot_loss_curves</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> If you’re using Google Colab, and you don’t have a GPU turned on yet, it’s now time to turn one on via <code class="docutils literal notranslate"><span class="pre">Runtime</span> <span class="pre">-&gt;</span> <span class="pre">Change</span> <span class="pre">runtime</span> <span class="pre">type</span> <span class="pre">-&gt;</span> <span class="pre">Hardware</span> <span class="pre">accelerator</span> <span class="pre">-&gt;</span> <span class="pre">GPU</span></code>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="get-data">
<h2>1. Get Data<a class="headerlink" href="#get-data" title="Permalink to this heading">#</a></h2>
<p>Since we’re continuing on with FoodVision Mini, let’s download the pizza, steak and sushi image dataset we’ve been using.</p>
<p>To do so we can use the <code class="docutils literal notranslate"><span class="pre">download_data()</span></code> function from <code class="docutils literal notranslate"><span class="pre">helper_functions.py</span></code> that we created in <a class="reference external" href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data">07. PyTorch Experiment Tracking section 1</a>.</p>
<p>We’ll <code class="docutils literal notranslate"><span class="pre">source</span></code> to the raw GitHub link of the <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip"><code class="docutils literal notranslate"><span class="pre">pizza_steak_sushi.zip</span></code> data</a> and the <code class="docutils literal notranslate"><span class="pre">destination</span></code> to <code class="docutils literal notranslate"><span class="pre">pizza_steak_sushi</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download pizza, steak, sushi images from GitHub</span>
<span class="n">image_path</span> <span class="o">=</span> <span class="n">download_data</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s2">&quot;https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip&quot;</span><span class="p">,</span>
                           <span class="n">destination</span><span class="o">=</span><span class="s2">&quot;pizza_steak_sushi&quot;</span><span class="p">)</span>
<span class="n">image_path</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] data/pizza_steak_sushi directory exists, skipping download.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PosixPath(&#39;data/pizza_steak_sushi&#39;)
</pre></div>
</div>
</div>
</div>
<p>Beautiful! Data downloaded, let’s setup the training and test directories.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup directory paths to train and test images</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;train&quot;</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;test&quot;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="create-datasets-and-dataloaders">
<h2>2. Create Datasets and DataLoaders<a class="headerlink" href="#create-datasets-and-dataloaders" title="Permalink to this heading">#</a></h2>
<p>Now we’ve got some data, let’s now turn it into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s.</p>
<p>To do so we can use the <code class="docutils literal notranslate"><span class="pre">create_dataloaders()</span></code> function in <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py"><code class="docutils literal notranslate"><span class="pre">data_setup.py</span></code></a>.</p>
<p>First, we’ll create a transform to prepare our images.</p>
<p>This where one of the first references to the ViT paper will come in.</p>
<p>In Table 3, the training resolution is mentioned as being 224 (height=224, width=224).</p>
<img alt="Table 3 from the Vision Transformer paper showing the image size and batch size" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-image-size-and-batch-size.png" />
<p><em>You can often find various hyperparameter settings listed in a table. In this case we’re still preparing our data, so we’re mainly concerned with things like image size and batch size. Source: Table 3 in <a class="reference external" href="https://arxiv.org/abs/2010.11929">ViT paper</a>.</em></p>
<p>So we’ll make sure our transform resizes our images appropriately.</p>
<p>And since we’ll be training our model from scratch (no transfer learning to begin with), we won’t provide a <code class="docutils literal notranslate"><span class="pre">normalize</span></code> transform like we did in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#21-creating-a-transform-for-torchvisionmodels-manual-creation">06. PyTorch Transfer Learning section 2.1</a>.</p>
<section id="prepare-transforms-for-images">
<h3>2.1 Prepare transforms for images<a class="headerlink" href="#prepare-transforms-for-images" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create image size (from Table 3 in the ViT paper) </span>
<span class="n">IMG_SIZE</span> <span class="o">=</span> <span class="mi">224</span>

<span class="c1"># Create transform pipeline manually</span>
<span class="n">manual_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">IMG_SIZE</span><span class="p">,</span> <span class="n">IMG_SIZE</span><span class="p">)),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
<span class="p">])</span>           
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Manually created transforms: </span><span class="si">{</span><span class="n">manual_transforms</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Manually created transforms: Compose(
    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)
    ToTensor()
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="turn-images-into-dataloaders">
<h3>2.2 Turn images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s<a class="headerlink" href="#turn-images-into-dataloaders" title="Permalink to this heading">#</a></h3>
<p>Transforms created!</p>
<p>Let’s now create our <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s.</p>
<p>The ViT paper states the use of a batch size of 4096 which is 128x the size of the batch size we’ve been using (32).</p>
<p>However, we’re going to stick with a batch size of 32.</p>
<p>Why?</p>
<p>Because some hardware (including the free tier of Google Colab) may not be able to handle a batch size of 4096.</p>
<p>Having a batch size of 4096 means that 4096 images need to fit into the GPU memory at a time.</p>
<p>This works when you’ve got the hardware to handle it like a research team from Google often does but when you’re running on a single GPU (such as using Google Colab), making sure things work with smaller batch size first is a good idea.</p>
<p>An extension of this project could be to try a higher batch size value and see what happens.</p>
<blockquote>
<div><p><strong>Note:</strong> We’re using the <code class="docutils literal notranslate"><span class="pre">pin_memory=True</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">create_dataloaders()</span></code> function to speed up computation. <code class="docutils literal notranslate"><span class="pre">pin_memory=True</span></code> avoids unnecessary copying of memory between the CPU and GPU memory by “pinning” examples that have been seen before. Though the benefits of this will likely be seen with larger dataset sizes (our FoodVision Mini dataset is quite small). However, setting <code class="docutils literal notranslate"><span class="pre">pin_memory=True</span></code> doesn’t <em>always</em> improve performance (this is another one of those we’re scenarios in machine learning where some things work sometimes and don’t other times), so best to <em>experiment, experiment, experiment</em>. See the PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> documentation</a> or <a class="reference external" href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr from First Principles</a> by Horace He for more.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the batch size</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span> <span class="c1"># this is lower than the ViT paper but it&#39;s because we&#39;re starting small</span>

<span class="c1"># Create data loaders</span>
<span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span> <span class="o">=</span> <span class="n">data_setup</span><span class="o">.</span><span class="n">create_dataloaders</span><span class="p">(</span>
    <span class="n">train_dir</span><span class="o">=</span><span class="n">train_dir</span><span class="p">,</span>
    <span class="n">test_dir</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">manual_transforms</span><span class="p">,</span> <span class="c1"># use manually created transforms</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span>
<span class="p">)</span>

<span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="p">,</span> <span class="n">class_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f18845ff0d0&gt;,
 &lt;torch.utils.data.dataloader.DataLoader at 0x7f17f3f5f520&gt;,
 [&#39;pizza&#39;, &#39;steak&#39;, &#39;sushi&#39;])
</pre></div>
</div>
</div>
</div>
</section>
<section id="visualize-a-single-image">
<h3>2.3 Visualize a single image<a class="headerlink" href="#visualize-a-single-image" title="Permalink to this heading">#</a></h3>
<p>Now we’ve loaded our data, let’s <em>visualize, visualize, visualize!</em></p>
<p>An important step in the ViT paper is preparing the images into patches.</p>
<p>We’ll get to what this means in <a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#4-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding">section 4</a> but for now, let’s view a single image and its label.</p>
<p>To do so, let’s get a single image and label from a batch of data and inspect their shapes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get a batch of images</span>
<span class="n">image_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>

<span class="c1"># Get a single image from the batch</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># View the batch shapes</span>
<span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([3, 224, 224]), tensor(2))
</pre></div>
</div>
</div>
</div>
<p>Wonderful!</p>
<p>Now let’s plot the image and its label with <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot image with matplotlib</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/5b6269bcaf2b1f7df6f51d177cf38ec99f9bc6380f70394a57adc600c2a4c8bd.png" src="../../_images/5b6269bcaf2b1f7df6f51d177cf38ec99f9bc6380f70394a57adc600c2a4c8bd.png" />
</div>
</div>
<p>Nice!</p>
<p>Looks like our images are importing correctly, let’s continue with the paper replication.</p>
</section>
</section>
<section id="replicating-the-vit-paper-an-overview">
<h2>3. Replicating the ViT paper: an overview<a class="headerlink" href="#replicating-the-vit-paper-an-overview" title="Permalink to this heading">#</a></h2>
<p>Before we write anymore code, let’s discuss what we’re doing.</p>
<p>We’d like to replicate the ViT paper for our own problem, FoodVision Mini.</p>
<p>So our <strong>model inputs</strong> are: images of pizza, steak and sushi.</p>
<p>And our ideal <strong>model outputs</strong> are: predicted labels of pizza, steak or sushi.</p>
<p>No different to what we’ve been doing throughout the previous sections.</p>
<p>The question is: how do we go from our inputs to the desired outputs?</p>
<section id="inputs-and-outputs-layers-and-blocks">
<h3>3.1 Inputs and outputs, layers and blocks<a class="headerlink" href="#inputs-and-outputs-layers-and-blocks" title="Permalink to this heading">#</a></h3>
<p>ViT is a deep learning neural network architecture.</p>
<p>And any neural network architecture is generally comprised of <strong>layers</strong>.</p>
<p>And a collection of layers is often referred to as a <strong>block</strong>.</p>
<p>And stacking many blocks together is what gives us the whole architecture.</p>
<p>A <strong>layer</strong> takes an input (say an image tensor), performs some kind of function on it (for example what’s in the layer’s <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method) and then returns an output.</p>
<p>So if a <strong>single layer</strong> takes an input and gives an output, then a collection of layers or a <strong>block</strong> also takes an input and gives an output.</p>
<p>Let’s make this concrete:</p>
<ul class="simple">
<li><p><strong>Layer</strong> - takes an input, performs a function on it, returns an output.</p></li>
<li><p><strong>Block</strong> - a collection of layers, takes an input, performs a series of functions on it, returns an output.</p></li>
<li><p><strong>Architecture (or model)</strong> - a collection of blocks, takes an input, performs a series of functions on it, returns an output.</p></li>
</ul>
<p>This ideology is what we’re going to be using to replicate the ViT paper.</p>
<p>We’re going to take it layer by layer, block by block, function by function putting the pieces of the puzzle together like Lego to get our desired overall architecture.</p>
<p>The reason we do this is because looking at a whole research paper can be intimidating.</p>
<p>So for a better understanding, we’ll break it down, starting with the inputs and outputs of single layer and working up to the inputs and outputs of the whole model.</p>
<img alt="inputs and outputs, layers and blocks of a model" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-intputs-outputs-layers-and-blocks.png" />
<p><em>A modern deep learning architecture is usually collection of layers and blocks. Where layers take an input (data as a numerical representation) and manipulate it using some kind of function (for example, the self-attention formula pictured above, however, this function could be almost anything) and then output it. Blocks are generally stacks of layers on top of each other doing a similar thing to a single layer but multiple times.</em></p>
</section>
<section id="getting-specific-whats-vit-made-of">
<h3>3.2 Getting specific: What’s ViT made of?<a class="headerlink" href="#getting-specific-whats-vit-made-of" title="Permalink to this heading">#</a></h3>
<p>There are many little details about the ViT model sprinkled throughout the paper.</p>
<p>Finding them all is like one big treasure hunt!</p>
<p>Remember, a research paper is often months of work compressed into a few pages so it’s understandable for it to take of practice to replicate.</p>
<p>However, the main three resources we’ll be looking at for the architecture design are:</p>
<ol class="arabic simple">
<li><p><strong>Figure 1</strong> - This gives an overview of the model in a graphical sense, you could <em>almost</em> recreate the architecture with this figure alone.</p></li>
<li><p><strong>Four equations in section 3.1</strong> - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1.</p></li>
<li><p><strong>Table 1</strong> - This table shows the various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. We’ll be focused on the smallest version, ViT-Base.</p></li>
</ol>
<section id="exploring-figure-1">
<h4>3.2.1 Exploring Figure 1<a class="headerlink" href="#exploring-figure-1" title="Permalink to this heading">#</a></h4>
<p>Let’s start by going through Figure 1 of the ViT Paper.</p>
<p>The main things we’ll be paying attention to are:</p>
<ol class="arabic simple">
<li><p><strong>Layers</strong> - takes an <strong>input</strong>, performs an operation or function on the input, produces an <strong>output</strong>.</p></li>
<li><p><strong>Blocks</strong> - a collection of layers, which in turn also takes an <strong>input</strong> and produces an <strong>output</strong>.</p></li>
</ol>
<img alt="figure 1 from the original vision transformer paper" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs.png" />
<p><em>Figure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture. Our goal will be to replicate each of these using PyTorch code.</em></p>
<p>The ViT architecture is comprised of several stages:</p>
<ul class="simple">
<li><p><strong>Patch + Position Embedding (inputs)</strong> - Turns the input image into a sequence of image patches and add a position number what order the patch comes in.</p></li>
<li><p><strong>Linear projection of flattened patches (Embedded Patches)</strong> - The image patches get turned into an <strong>embedding</strong>, the benefit of using an embedding rather than just the image values is that an embedding is a <em>learnable</em> representation (typically in the form of a vector) of the image that can improve with training.</p></li>
<li><p><strong>Norm</strong> - This is short for “<a class="reference external" href="https://paperswithcode.com/method/layer-normalization">Layer Normalization</a>” or “LayerNorm”, a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code></a>.</p></li>
<li><p><strong>Multi-Head Attention</strong> - This is a <a class="reference external" href="https://paperswithcode.com/method/multi-head-attention">Multi-Headed Self-Attention layer</a> or “MSA” for short. You can create an MSA layer via the PyTorch layer <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.MultiheadAttention()</span></code></a>.</p></li>
<li><p><strong>MLP (or <a class="reference external" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron</a>)</strong> - A MLP can often refer to any collection of feedforward layers (or in PyTorch’s case, a collection of layers with a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method). In the ViT Paper, the authors refer to the MLP as “MLP block” and it contains two <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Linear()</span></code></a> layers with a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.GELU()</span></code></a> non-linearity activation in between them (section 3.1) and a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Dropout()</span></code></a> layer after each (Appendex B.1).</p></li>
<li><p><strong>Transformer Encoder</strong> - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the “+” symbols) meaning the layer’s inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.</p></li>
<li><p><strong>MLP Head</strong> - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we’re working on image classification, you could also call this the “classifier head”. The structure of the MLP Head is similar to the MLP block.</p></li>
</ul>
<p>You might notice that many of the pieces of the ViT architecture can be created with existing PyTorch layers.</p>
<p>This is because of how PyTorch is designed, it’s one of the main purposes of PyTorch to create reusable neural network layers for both researchers and machine learning practitioners.</p>
<blockquote>
<div><p><strong>Question:</strong> Why not code everything from scratch?</p>
<p>You could definitely do that by reproducing all of the math equations from the paper with custom PyTorch layers and that would certainly be an educative exercise, however, using pre-existing PyTorch layers is usually favoured as pre-existing layers have often been extensively tested and performance checked to make sure they run correctly and fast.</p>
</div></blockquote>
<blockquote>
<div><p><strong>Note:</strong> We’re going to focused on write PyTorch code to create these layers, for the background on what each of these layers does, I’d suggest reading the ViT Paper in full or reading the linked resources for each layer.</p>
</div></blockquote>
<p>Let’s take Figure 1 and adapt it to our FoodVision Mini problem of classifying images of food into pizza, steak or sushi.</p>
<img alt="figure 1 from the original vision transformer paper adapted to work with food images, an image of pizza goes in and gets classified as 'pizza'" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs-food-mini.png" />
<p><em>Figure 1 from the ViT Paper adapted for use with FoodVision Mini. An image of food goes in (pizza), the image gets turned into patches and then projected to an embedding. The embedding then travels through the various layers and blocks and (hopefully) the class “pizza” is returned.</em></p>
</section>
<section id="exploring-the-four-equations">
<h4>3.2.2 Exploring the Four Equations<a class="headerlink" href="#exploring-the-four-equations" title="Permalink to this heading">#</a></h4>
<p>The next main part(s) of the ViT paper we’re going to look at are the four equations in section 3.1.</p>
<img alt="four mathematical equations from the vision transformer machine learning paper" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png" />
<p><em>These four equations represent the math behind the four major parts of the ViT architecture.</em></p>
<p>Section 3.1 describes each of these (some of the text has been omitted for brevity, bolded text is mine):</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Equation number</strong></p></th>
<th class="head"><p><strong>Description from ViT paper section 3.1</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>…The Transformer uses constant latent vector size <span class="math notranslate nohighlight">\(D\)</span> through all of its layers, so we flatten the patches and map to <span class="math notranslate nohighlight">\(D\)</span> dimensions with a <strong>trainable linear projection</strong> (Eq. 1). We refer to the output of this projection as the <strong>patch embeddings</strong>… <strong>Position embeddings</strong> are added to the patch embeddings to retain positional information. We use standard <strong>learnable 1D position embeddings</strong>…</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). <strong>Layernorm (LN) is applied before every block</strong>, and <strong>residual connections after every block</strong> (Wang et al., 2019; Baevski &amp; Auli, 2019).</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Same as equation 2.</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>Similar to BERT’s [ class ] token, we <strong>prepend a learnable embedding to the sequence of embedded patches</strong> <span class="math notranslate nohighlight">\(\left(\mathbf{z}_{0}^{0}=\mathbf{x}_{\text {class }}\right)\)</span>, whose state at the output of the Transformer encoder <span class="math notranslate nohighlight">\(\left(\mathbf{z}_{L}^{0}\right)\)</span> serves as the image representation <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> (Eq. 4)…</p></td>
</tr>
</tbody>
</table>
</div>
<p>Let’s map these descriptions to the ViT architecture in Figure 1.</p>
<img alt="mapping the vision transformer paper figure 1 to the four equations listed in the paper" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-mapping-the-four-equations-to-figure-1.png" />
<p><em>Connecting Figure 1 from the ViT paper to the four equations from section 3.1 describing the math behind each of the layers/blocks.</em></p>
<p>There’s a lot happening in the image above but following the coloured lines and arrows reveals the main concepts of the ViT architecture.</p>
<p>How about we break down each equation further (it will be our goal to recreate these with code)?</p>
<p>In all equations (except equation 4), “<span class="math notranslate nohighlight">\(\mathbf{z}\)</span>” is the raw output of a particular layer:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{z}_{0}\)</span> is “z zero” (this is the output of the initial patch embedding layer).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{z}_{\ell}^{\prime}\)</span> is “z of a particular layer <em>prime</em>” (or an intermediary value of z).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{z}_{\ell}\)</span> is “z of a particular layer”.</p></li>
</ol>
<p>And <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is the overall output of the architecture.</p>
</section>
<section id="equation-1-overview">
<h4>3.2.3 Equation 1 overview<a class="headerlink" href="#equation-1-overview" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{z}_{0} &amp;=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1} \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{\text {pos }}, &amp; &amp; \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{\text {pos }} \in \mathbb{R}^{(N+1) \times D}
\end{aligned}
\]</div>
<p>This equation deals with the class token, patch embedding and position embedding (<span class="math notranslate nohighlight">\(\mathbf{E}\)</span> is for embedding) of the input image.</p>
<p>In vector form, the embedding might look something like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">class_token</span><span class="p">,</span> <span class="n">image_patch_1</span><span class="p">,</span> <span class="n">image_patch_2</span><span class="p">,</span> <span class="n">image_patch_3</span><span class="o">...</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">class_token_position</span><span class="p">,</span> <span class="n">image_patch_1_position</span><span class="p">,</span> <span class="n">image_patch_2_position</span><span class="p">,</span> <span class="n">image_patch_3_position</span><span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
<p>Where each of the elements in the vector is learnable (their <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>).</p>
</section>
<section id="equation-2-overview">
<h4>3.2.4 Equation 2 overview<a class="headerlink" href="#equation-2-overview" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{z}_{\ell}^{\prime} &amp;=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1}, &amp; &amp; \ell=1 \ldots L
\end{aligned}
\]</div>
<p>This says that for every layer from <span class="math notranslate nohighlight">\(1\)</span> through to <span class="math notranslate nohighlight">\(L\)</span> (the total number of layers), there’s a Multi-Head Attention layer (MSA) wrapping a LayerNorm layer (LN).</p>
<p>The addition on the end is the equivalent of adding the input to the output and forming a <a class="reference external" href="https://paperswithcode.com/method/residual-connection">skip/residual connection</a>.</p>
<p>We’ll call this layer the “MSA block”.</p>
<p>In pseudocode, this might look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_output_MSA_block</span> <span class="o">=</span> <span class="n">MSA_layer</span><span class="p">(</span><span class="n">LN_layer</span><span class="p">(</span><span class="n">x_input</span><span class="p">))</span> <span class="o">+</span> <span class="n">x_input</span>
</pre></div>
</div>
<p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p>
</section>
<section id="equation-3-overview">
<h4>3.2.5 Equation 3 overview<a class="headerlink" href="#equation-3-overview" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{z}_{\ell} &amp;=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, &amp; &amp; \ell=1 \ldots L \\
\end{aligned}
\end{split}\]</div>
<p>This says that for every layer from <span class="math notranslate nohighlight">\(1\)</span> through to <span class="math notranslate nohighlight">\(L\)</span> (the total number of layers), there’s also a Multilayer Perceptron layer (MLP) wrapping a LayerNorm layer (LN).</p>
<p>The addition on the end is showing the presence of a skip/residual connection.</p>
<p>We’ll call this layer the “MLP block”.</p>
<p>In pseudocode, this might look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_output_MLP_block</span> <span class="o">=</span> <span class="n">MLP_layer</span><span class="p">(</span><span class="n">LN_layer</span><span class="p">(</span><span class="n">x_output_MSA_block</span><span class="p">))</span> <span class="o">+</span> <span class="n">x_output_MSA_block</span>
</pre></div>
</div>
<p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p>
</section>
<section id="equation-4-overview">
<h4>3.2.6 Equation 4 overview<a class="headerlink" href="#equation-4-overview" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{y} &amp;=\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right) &amp; &amp;
\end{aligned}
\]</div>
<p>This says for the last layer <span class="math notranslate nohighlight">\(L\)</span>, the output <span class="math notranslate nohighlight">\(y\)</span> is the 0 index token of <span class="math notranslate nohighlight">\(z\)</span> wrapped in a LayerNorm layer (LN).</p>
<p>Or in our case, the 0 index of <code class="docutils literal notranslate"><span class="pre">x_output_MLP_block</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">LN_layer</span><span class="p">(</span><span class="n">Linear_layer</span><span class="p">(</span><span class="n">x_output_MLP_block</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
<p>Of course there are some simplifications above but we’ll take care of those when we start to write PyTorch code for each section.</p>
<blockquote>
<div><p><strong>Note:</strong> The above section covers alot of information. But don’t forget if something doesn’t make sense, you can always research it further. By asking questions like “what is a residual connection?”.</p>
</div></blockquote>
</section>
<section id="exploring-table-1">
<h4>3.2.7 Exploring Table 1<a class="headerlink" href="#exploring-table-1" title="Permalink to this heading">#</a></h4>
<p>The final piece of the ViT architecture puzzle we’ll focus on (for now) is Table 1.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-left head"><p>Model</p></th>
<th class="text-center head"><p>Layers</p></th>
<th class="text-center head"><p>Hidden size <span class="math notranslate nohighlight">\(D\)</span></p></th>
<th class="text-center head"><p>MLP size</p></th>
<th class="text-center head"><p>Heads</p></th>
<th class="text-center head"><p>Params</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>ViT-Base</p></td>
<td class="text-center"><p>12</p></td>
<td class="text-center"><p>768</p></td>
<td class="text-center"><p>3072</p></td>
<td class="text-center"><p>12</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(86M\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>ViT-Large</p></td>
<td class="text-center"><p>24</p></td>
<td class="text-center"><p>1024</p></td>
<td class="text-center"><p>4096</p></td>
<td class="text-center"><p>16</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(307M\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>ViT-Huge</p></td>
<td class="text-center"><p>32</p></td>
<td class="text-center"><p>1280</p></td>
<td class="text-center"><p>5120</p></td>
<td class="text-center"><p>16</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(632M\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div align=center>
    <i>Table 1: Details of Vision Transformer model variants. Source: <a href="https://arxiv.org/abs/2010.11929">ViT paper</a>.</i>
</div>
<br>
<p>This table showcasing the various hyperparameters of each of the ViT architectures.</p>
<p>You can see the numbers gradually increase from ViT-Base to ViT-Huge.</p>
<p>We’re going to focus on replicating ViT-Base (start small and scale up when necessary) but we’ll be writing code that could easily scale up to the larger variants.</p>
<p>Breaking the hyperparameters down:</p>
<ul class="simple">
<li><p><strong>Layers</strong> - How many Transformer Encoder blocks are there? (each of these will contain a MSA block and MLP block)</p></li>
<li><p><strong>Hidden size <span class="math notranslate nohighlight">\(D\)</span></strong> - This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, the better results. However, a larger embedding comes at the cost of more compute.</p></li>
<li><p><strong>MLP size</strong> - What are the number of hidden units in the MLP layers?</p></li>
<li><p><strong>Heads</strong> - How many heads are there in the Multi-Head Attention layers?</p></li>
<li><p><strong>Params</strong> - What are the total number of parameters of the model? Generally, more parameters leads to better performance but at the cost of more compute. You’ll notice even ViT-Base has far more parameters than any other model we’ve used so far.</p></li>
</ul>
<p>We’ll use these values as the hyperparameter settings for our ViT architecture.</p>
</section>
</section>
<section id="my-workflow-for-replicating-papers">
<h3>3.3 My workflow for replicating papers<a class="headerlink" href="#my-workflow-for-replicating-papers" title="Permalink to this heading">#</a></h3>
<p>When I start working on replicating a paper, I go through the following steps:</p>
<ol class="arabic simple">
<li><p>Read the whole paper end-to-end once (to get an idea of the main concepts).</p></li>
<li><p>Go back through each section and see how they line up with each other and start thinking about how they might be turned into code (just like above).</p></li>
<li><p>Repeat step 2 until I’ve got a fairly good outline.</p></li>
<li><p>Use <a class="reference external" href="https://mathpix.com/">mathpix.com</a> (a very handy tool) to turn any sections of the paper into markdown/LaTeX to put into notebooks.</p></li>
<li><p>Replicate the simplest version of the model possible.</p></li>
<li><p>If I get stuck, look up other examples.</p></li>
</ol>
<img alt="using mathpix.com to turn text from a machine learning research paper (PDF) into editable LaTeX" src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-mathpix-demo.gif" />
<p><em>Turning the four equations from the ViT paper into editable LaTeX/markdown using <a class="reference external" href="https://mathpix.com/">mathpix.com</a>.</em></p>
<p>We’ve already gone through the first few steps above (and if you haven’t read the full paper yet, I’d encourage you to give it a go) but what we’ll be focusing on next is step 5: replicating the simplest version fo the model possible.</p>
<p>This is why we’re starting with ViT-Base.</p>
<p>Replicating the smallest version of the architecture possible, get it working and then we can scale up if we wanted to.</p>
<blockquote>
<div><p><strong>Note:</strong> If you’ve never read a research paper before, many of the above steps can be intimidating. But don’t worry, like anything, your skills at reading <em>and</em> replicating papers will improve with practice. Don’t forget, a research paper is often <em>months</em> of work by many people compressed into a few pages. So trying to replicate it on your own is no small feat.</p>
</div></blockquote>
</section>
</section>
<section id="equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding">
<h2>4. Equation 1: Split data into patches and creating the class, position and patch embedding<a class="headerlink" href="#equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding" title="Permalink to this heading">#</a></h2>
<p>I remember one of my machine learning engineer friends used to say “it’s all about the embedding.”</p>
<p>As in, if you can represent your data in a good, learnable way (as <strong>embeddings are learnable representations</strong>), chances are, a learning algorithm will be able to perform well on them.</p>
<p>With that being said, let’s start by creating the class, position and patch embeddings for the ViT architecture.</p>
<p>We’ll start with the <strong>patch embedding</strong>.</p>
<p>This means we’ll be turning our input images in a sequence of patches and then embedding those patches.</p>
<p>Recall that an <strong>embedding</strong> is a learnable representation of some form and is often a vector.</p>
<p>The term learnable is important because this means the numerical representation of an input image (that the model sees) can be improved over time.</p>
<p>We’ll begin by following the opening paragraph of section 3.1 of the ViT paper (bold mine):</p>
<blockquote>
<div><p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{H \times W \times C}\)</span> into a sequence of flattened 2D patches <span class="math notranslate nohighlight">\(\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}\)</span>, where <span class="math notranslate nohighlight">\((H, W)\)</span> is the resolution of the original image, <span class="math notranslate nohighlight">\(C\)</span> is the number of channels, <span class="math notranslate nohighlight">\((P, P)\)</span> is the resolution of each image patch, and <span class="math notranslate nohighlight">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size <span class="math notranslate nohighlight">\(D\)</span> through all of its layers, so we flatten the patches and map to <span class="math notranslate nohighlight">\(D\)</span> dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the <strong>patch embeddings</strong>.</p>
</div></blockquote>
<p>And size we’re dealing with image shapes, let’s keep in mind the line from Table 3 of the ViT paper:</p>
<blockquote>
<div><p>Training resolution is <strong>224</strong>.</p>
</div></blockquote>
<p>Let’s break down the text above.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D\)</span> is the size of the <strong>patch embeddings</strong>, different values for <span class="math notranslate nohighlight">\(D\)</span> for various sized ViT models can be found in Table 1.</p></li>
<li><p>The image starts as 2D with size <span class="math notranslate nohighlight">\({H \times W \times C}\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\((H, W)\)</span> is the resolution of the original image (height, width).</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span> is the number of channels.</p></li>
</ul>
</li>
<li><p>The image gets converted to a sequence of flattened 2D patches with size <span class="math notranslate nohighlight">\({N \times\left(P^{2} \cdot C\right)}\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\((P, P)\)</span> is the resolution of each image patch (<strong>patch size</strong>).</p></li>
<li><p><span class="math notranslate nohighlight">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the input sequence length for the Transformer.</p></li>
</ul>
</li>
</ul>
<img alt="mapping the vit architecture diagram positional and patch embeddings portion to the relative mathematical equation describing what's going on" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-1-annotated.png" />
<p><em>Mapping the patch and position embedding portion of the ViT architecture from Figure 1 to Equation 1. The opening paragraph of section 3.1 describes the different input and output shapes of the patch embedding layer.</em></p>
<section id="calculating-patch-embedding-input-and-output-shapes-by-hand">
<h3>4.1 Calculating patch embedding input and output shapes by hand<a class="headerlink" href="#calculating-patch-embedding-input-and-output-shapes-by-hand" title="Permalink to this heading">#</a></h3>
<p>How about we start by calculating these input and output shape values by hand?</p>
<p>To do so, let’s create some variables to mimic each of the terms (such as <span class="math notranslate nohighlight">\(H\)</span>, <span class="math notranslate nohighlight">\(W\)</span> etc) above.</p>
<p>We’ll use a patch size (<span class="math notranslate nohighlight">\(P\)</span>) of 16 since it’s the best performing version of ViT-Base uses (see column “ViT-B/16” of Table 5 in the ViT paper for more).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create example values</span>
<span class="n">height</span> <span class="o">=</span> <span class="mi">224</span> <span class="c1"># H (&quot;The training resolution is 224.&quot;)</span>
<span class="n">width</span> <span class="o">=</span> <span class="mi">224</span> <span class="c1"># W</span>
<span class="n">color_channels</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># C</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">16</span> <span class="c1"># P</span>

<span class="c1"># Calculate N (number of patches)</span>
<span class="n">number_of_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span> <span class="o">/</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of patches (N) with image height (H=</span><span class="si">{</span><span class="n">height</span><span class="si">}</span><span class="s2">), width (W=</span><span class="si">{</span><span class="n">width</span><span class="si">}</span><span class="s2">) and patch size (P=</span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">number_of_patches</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196
</pre></div>
</div>
</div>
</div>
<p>We’ve got the number of patches, how about we create the image output size as well?</p>
<p>Better yet, let’s replicate the input and output shapes of the patch embedding layer.</p>
<p>Recall:</p>
<ul class="simple">
<li><p><strong>Input:</strong> The image starts as 2D with size <span class="math notranslate nohighlight">\({H \times W \times C}\)</span>.</p></li>
<li><p><strong>Output:</strong> The image gets converted to a sequence of flattened 2D patches with size <span class="math notranslate nohighlight">\({N \times\left(P^{2} \cdot C\right)}\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Input shape (this is the size of a single image)</span>
<span class="n">embedding_layer_input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">color_channels</span><span class="p">)</span>

<span class="c1"># Output shape</span>
<span class="n">embedding_layer_output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">number_of_patches</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">color_channels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input shape (single 2D image): </span><span class="si">{</span><span class="n">embedding_layer_input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape (single 2D image flattened into patches): </span><span class="si">{</span><span class="n">embedding_layer_output_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input shape (single 2D image): (224, 224, 3)
Output shape (single 2D image flattened into patches): (196, 768)
</pre></div>
</div>
</div>
</div>
<p>Input and output shapes acquired!</p>
</section>
<section id="turning-a-single-image-into-patches">
<h3>4.2 Turning a single image into patches<a class="headerlink" href="#turning-a-single-image-into-patches" title="Permalink to this heading">#</a></h3>
<p>Now we know the ideal input and output shapes for our <strong>patch embedding</strong> layer, let’s move towards making it.</p>
<p>What we’re doing is breaking down the overall architecture into smaller pieces, focusing on the inputs and outputs of individual layers.</p>
<p>So how do we create the patch embedding layer?</p>
<p>We’ll get to that shortly, first, let’s <em>visualize, visualize, visualize!</em> what it looks like to turn an image into patches.</p>
<p>Let’s start with our single image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View single image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># adjust for matplotlib</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/5b6269bcaf2b1f7df6f51d177cf38ec99f9bc6380f70394a57adc600c2a4c8bd.png" src="../../_images/5b6269bcaf2b1f7df6f51d177cf38ec99f9bc6380f70394a57adc600c2a4c8bd.png" />
</div>
</div>
<p>We want to turn this image into patches of itself inline with Figure 1 of the ViT paper.</p>
<p>How about we start by just visualizing the top row of patched pixels?</p>
<p>We can do this by indexing on the different image dimensions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels) </span>
<span class="n">image_permuted</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Index to plot the top row of patched pixels</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_permuted</span><span class="p">[:</span><span class="n">patch_size</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/f127b67047df91e7b6cc9465d0b37284ef0fdeb8ace2e66aa81a4d2df2152a55.png" src="../../_images/f127b67047df91e7b6cc9465d0b37284ef0fdeb8ace2e66aa81a4d2df2152a55.png" />
</div>
</div>
<p>Now we’ve got the top row, let’s turn it into patches.</p>
<p>We can do this by iterating through the number of patches there’d be in the top row.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup hyperparameters and make sure img_size and patch_size are compatible</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="mi">224</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">num_patches</span> <span class="o">=</span> <span class="n">img_size</span><span class="o">/</span><span class="n">patch_size</span> 
<span class="k">assert</span> <span class="n">img_size</span> <span class="o">%</span> <span class="n">patch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Image size must be divisible by patch size&quot;</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of patches per row: </span><span class="si">{</span><span class="n">num_patches</span><span class="si">}</span><span class="se">\n</span><span class="s2">Patch size: </span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s2"> pixels x </span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s2"> pixels&quot;</span><span class="p">)</span>

<span class="c1"># Create a series of subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                        <span class="n">ncols</span><span class="o">=</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span> <span class="c1"># one column for each patch</span>
                        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">num_patches</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">),</span>
                        <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Iterate through number of patches in the top row</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">patch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)):</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_permuted</span><span class="p">[:</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">patch</span><span class="p">:</span><span class="n">patch</span><span class="o">+</span><span class="n">patch_size</span><span class="p">,</span> <span class="p">:]);</span> <span class="c1"># keep height index constant, alter the width index</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># set the label</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of patches per row: 14.0
Patch size: 16 pixels x 16 pixels
</pre></div>
</div>
<img alt="../../_images/ffb20866c2c73c7bbcbf32f31080a490ca0257fb7c22e4c5161771526fff69e8.png" src="../../_images/ffb20866c2c73c7bbcbf32f31080a490ca0257fb7c22e4c5161771526fff69e8.png" />
</div>
</div>
<p>Those are some nice looking patches!</p>
<p>How about we do it for the whole image?</p>
<p>This time we’ll iterate through the indexs for height and width and plot each patch as it’s own subplot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup hyperparameters and make sure img_size and patch_size are compatible</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="mi">224</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">num_patches</span> <span class="o">=</span> <span class="n">img_size</span><span class="o">/</span><span class="n">patch_size</span> 
<span class="k">assert</span> <span class="n">img_size</span> <span class="o">%</span> <span class="n">patch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Image size must be divisible by patch size&quot;</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of patches per row: </span><span class="si">{</span><span class="n">num_patches</span><span class="si">}</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Number of patches per column: </span><span class="si">{</span><span class="n">num_patches</span><span class="si">}</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Total patches: </span><span class="si">{</span><span class="n">num_patches</span><span class="o">*</span><span class="n">num_patches</span><span class="si">}</span><span class="se">\</span>
<span class="s2">        </span><span class="se">\n</span><span class="s2">Patch size: </span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s2"> pixels x </span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s2"> pixels&quot;</span><span class="p">)</span>

<span class="c1"># Create a series of subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span> <span class="c1"># need int not float</span>
                        <span class="n">ncols</span><span class="o">=</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">,</span> 
                        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">num_patches</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">),</span>
                        <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Loop through height and width of image</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">patch_height</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)):</span> <span class="c1"># iterate through height</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">patch_width</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)):</span> <span class="c1"># iterate through width</span>
        
        <span class="c1"># Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_permuted</span><span class="p">[</span><span class="n">patch_height</span><span class="p">:</span><span class="n">patch_height</span><span class="o">+</span><span class="n">patch_size</span><span class="p">,</span> <span class="c1"># iterate through height </span>
                                        <span class="n">patch_width</span><span class="p">:</span><span class="n">patch_width</span><span class="o">+</span><span class="n">patch_size</span><span class="p">,</span> <span class="c1"># iterate through width</span>
                                        <span class="p">:])</span> <span class="c1"># get all color channels</span>
        
        <span class="c1"># Set up label information, remove the ticks for clarity and set labels to outside</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> 
                             <span class="n">rotation</span><span class="o">=</span><span class="s2">&quot;horizontal&quot;</span><span class="p">,</span> 
                             <span class="n">horizontalalignment</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span> 
                             <span class="n">verticalalignment</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span> 
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> 
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">label_outer</span><span class="p">()</span>

<span class="c1"># Set a super title</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="si">}</span><span class="s2"> -&gt; Patchified&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of patches per row: 14.0        
Number of patches per column: 14.0        
Total patches: 196.0        
Patch size: 16 pixels x 16 pixels
</pre></div>
</div>
<img alt="../../_images/a74fae94c1c6c0e1d4813915e960d87140dfa78bbf51f60f0b894f87249478c8.png" src="../../_images/a74fae94c1c6c0e1d4813915e960d87140dfa78bbf51f60f0b894f87249478c8.png" />
</div>
</div>
<p>Image patchified!</p>
<p>Woah, that looks cool.</p>
<p>Now how do we turn each of these patches into an embedding and convert them into a sequence?</p>
<p>Hint: we can use PyTorch layers. Can you guess which?</p>
</section>
<section id="creating-image-patches-with-torch-nn-conv2d">
<h3>4.3 Creating image patches with <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d()</span></code><a class="headerlink" href="#creating-image-patches-with-torch-nn-conv2d" title="Permalink to this heading">#</a></h3>
<p>We’ve seen what an image looks like when it gets turned into patches, now let’s start moving towards replicating the patch embedding layers with PyTorch.</p>
<p>To visualize our single image we wrote code to loop through the different height and width dimensions of a single image and plot individual patches.</p>
<p>This operation is very similar to the convolutional operation we saw in <a class="reference external" href="https://www.learnpytorch.io/03_pytorch_computer_vision/#71-stepping-through-nnconv2d">03. PyTorch Computer Vision section 7.1: Stepping through <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code></a>.</p>
<p>In fact, the authors of the ViT paper mention in section 3.1 that the patch embedding is achievable with a convolutional neural network (CNN):</p>
<blockquote>
<div><p><strong>Hybrid Architecture.</strong> As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> (Eq. 1) is applied to patches extracted from a <strong>CNN feature map</strong>. As a special case, the patches can have spatial size <span class="math notranslate nohighlight">\(1 \times 1\)</span>, which means that the <strong>input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension</strong>. The classification input embedding and position embeddings are added as described above.</p>
</div></blockquote>
<p>The “<strong>feature map</strong>” they’re refering to are the weights/activations produced by a convolutional layer passing over a given image.</p>
<img alt="example of creating a patch embedding by passing a convolutional layer over a single image" src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-patch-embedding-animation.gif" />
<p><em>By setting the <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> and <code class="docutils literal notranslate"><span class="pre">stride</span></code> parameters of a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d()</span></code></a> layer equal to the <code class="docutils literal notranslate"><span class="pre">patch_size</span></code>, we can effectively get a layer that splits our image into patches and creates a learnable embedding (referred to as a “Linear Projection” in the ViT paper) of each patch.</em></p>
<p>Remember our ideal input and output shapes for the patch embedding layer?</p>
<ul class="simple">
<li><p><strong>Input:</strong> The image starts as 2D with size <span class="math notranslate nohighlight">\({H \times W \times C}\)</span>.</p></li>
<li><p><strong>Output:</strong> The image gets converted to a 1D sequence of flattened 2D patches with size <span class="math notranslate nohighlight">\({N \times\left(P^{2} \cdot C\right)}\)</span>.</p></li>
</ul>
<p>Or for an image size of 224 and patch size of 16:</p>
<ul class="simple">
<li><p><strong>Input (2D image):</strong> (224, 224, 3) -&gt; (height, width, color channels)</p></li>
<li><p><strong>Output (flattened 2D patches):</strong> (196, 768) -&gt; (number of patches, embedding dimension)</p></li>
</ul>
<p>We can recreate these with:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d()</span></code></a> for turning our image into patches of CNN feature maps.</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code></a> for flattening the spatial dimensions of the feature map.</p></li>
</ul>
<p>Let’s start with the <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d()</span></code> layer.</p>
<p>We can replicate the creation of patches by setting the <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> and <code class="docutils literal notranslate"><span class="pre">stride</span></code> equal to <code class="docutils literal notranslate"><span class="pre">patch_size</span></code>.</p>
<p>This means each convolutional kernel will be of size <code class="docutils literal notranslate"><span class="pre">(patch_size</span> <span class="pre">x</span> <span class="pre">patch_size)</span></code> or if <code class="docutils literal notranslate"><span class="pre">patch_size=16</span></code>, <code class="docutils literal notranslate"><span class="pre">(16</span> <span class="pre">x</span> <span class="pre">16)</span></code> (the equivalent of one whole patch).</p>
<p>And each step or <code class="docutils literal notranslate"><span class="pre">stride</span></code> of the convolutional kernel will be <code class="docutils literal notranslate"><span class="pre">patch_size</span></code> pixels long or <code class="docutils literal notranslate"><span class="pre">16</span></code> pixels long (equivalent of stepping to the next patch).</p>
<p>We’ll set <code class="docutils literal notranslate"><span class="pre">in_channels=3</span></code> for the number of color channels in our image and we’ll set <code class="docutils literal notranslate"><span class="pre">out_channels=768</span></code>, the same as the <span class="math notranslate nohighlight">\(D\)</span> value in Table 1 for ViT-Base (this is the embedding dimension, each image will be embedded into a learnable vector of size 768).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Set the patch size</span>
<span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span>

<span class="c1"># Create the Conv2d layer with hyperparameters from the ViT paper</span>
<span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># number of color channels</span>
                   <span class="n">out_channels</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># from Table 1: Hidden size D, this is the embedding size</span>
                   <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="c1"># could also use (patch_size, patch_size)</span>
                   <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
                   <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we’ve got a convoluational layer, let’s see what happens when we pass a single image through it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View single image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># adjust for matplotlib</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/5b6269bcaf2b1f7df6f51d177cf38ec99f9bc6380f70394a57adc600c2a4c8bd.png" src="../../_images/5b6269bcaf2b1f7df6f51d177cf38ec99f9bc6380f70394a57adc600c2a4c8bd.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pass the image through the convolutional layer </span>
<span class="n">image_out_of_conv</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="c1"># add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">image_out_of_conv</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 768, 14, 14])
</pre></div>
</div>
</div>
</div>
<p>Passing our image through the convolutional layer turns it into a series of 768 (this is the embedding size or <span class="math notranslate nohighlight">\(D\)</span>) feature/activation maps.</p>
<p>So its output shape can be read as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">feature_map_height</span><span class="p">,</span> <span class="n">feature_map_width</span><span class="p">]</span>
</pre></div>
</div>
<p>Let’s visualize five random feature maps and see what they look like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot random 5 convolutional feature maps</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random_indexes</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">758</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># pick 5 numbers between 0 and the embedding size</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Showing random convolutional feature maps from indexes: </span><span class="si">{</span><span class="n">random_indexes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Create plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="c1"># Plot random image feature maps</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">random_indexes</span><span class="p">):</span>
    <span class="n">image_conv_feature_map</span> <span class="o">=</span> <span class="n">image_out_of_conv</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="c1"># index on the output tensor of the convolutional layer</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_conv_feature_map</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticklabels</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[],</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Showing random convolutional feature maps from indexes: [571, 727, 734, 380, 90]
</pre></div>
</div>
<img alt="../../_images/81d7ea2ca6156b28ddaf34c2d14b552dcbf88c19ae1aaaa0bd9e3b5c1e8eb9da.png" src="../../_images/81d7ea2ca6156b28ddaf34c2d14b552dcbf88c19ae1aaaa0bd9e3b5c1e8eb9da.png" />
</div>
</div>
<p>Notice how the feature maps all kind of represent the original image, after visualizing a few more you can start to see the different major outlines and some major features.</p>
<p>The important thing to note is that these features may change over time as the neural network learns.</p>
<p>And because of these, these feature maps can be considered a <strong>learnable embedding</strong> of our image.</p>
<p>Let’s check one out in numerical form.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get a single feature map in tensor form</span>
<span class="n">single_feature_map</span> <span class="o">=</span> <span class="n">image_out_of_conv</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
<span class="n">single_feature_map</span><span class="p">,</span> <span class="n">single_feature_map</span><span class="o">.</span><span class="n">requires_grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[ 0.4732,  0.3567,  0.3377,  0.3736,  0.3208,  0.3913,  0.3464,
            0.3702,  0.2541,  0.3594,  0.1984,  0.3982,  0.3741,  0.1251],
          [ 0.4178,  0.4771,  0.3374,  0.3353,  0.3159,  0.4008,  0.3448,
            0.3345,  0.5850,  0.4115,  0.2969,  0.2751,  0.6150,  0.4188],
          [ 0.3209,  0.3776,  0.4970,  0.4272,  0.3301,  0.4787,  0.2754,
            0.3726,  0.3298,  0.4631,  0.3087,  0.4915,  0.4129,  0.4592],
          [ 0.4540,  0.4930,  0.5570,  0.2660,  0.2150,  0.2044,  0.2766,
            0.2076,  0.3278,  0.3727,  0.2637,  0.2493,  0.2782,  0.3664],
          [ 0.4920,  0.5671,  0.3298,  0.2992,  0.1437,  0.1701,  0.1554,
            0.1375,  0.1377,  0.3141,  0.2694,  0.2771,  0.2412,  0.3700],
          [ 0.5783,  0.5790,  0.4229,  0.5032,  0.1216,  0.1000,  0.0356,
            0.1258, -0.0023,  0.1640,  0.2809,  0.2418,  0.2606,  0.3787],
          [ 0.5334,  0.5645,  0.4781,  0.3307,  0.2391,  0.0461,  0.0095,
            0.0542,  0.1012,  0.1331,  0.2446,  0.2526,  0.3323,  0.4120],
          [ 0.5724,  0.2840,  0.5188,  0.3934,  0.1328,  0.0776,  0.0235,
            0.1366,  0.3149,  0.2200,  0.2793,  0.2351,  0.4722,  0.4785],
          [ 0.4009,  0.4570,  0.4972,  0.5785,  0.2261,  0.1447, -0.0028,
            0.2772,  0.2697,  0.4008,  0.3606,  0.3372,  0.4535,  0.4492],
          [ 0.5678,  0.5870,  0.5824,  0.3438,  0.5113,  0.0757,  0.1772,
            0.3677,  0.3572,  0.3742,  0.3820,  0.4868,  0.3781,  0.4694],
          [ 0.5845,  0.5877,  0.5826,  0.3212,  0.5276,  0.4840,  0.4825,
            0.5523,  0.5308,  0.5085,  0.5606,  0.5720,  0.4928,  0.5581],
          [ 0.5853,  0.5849,  0.5793,  0.3410,  0.4428,  0.4044,  0.3275,
            0.4958,  0.4366,  0.5750,  0.5494,  0.5868,  0.5557,  0.5069],
          [ 0.5880,  0.5888,  0.5796,  0.3377,  0.2635,  0.2347,  0.3145,
            0.3486,  0.5158,  0.5722,  0.5347,  0.5753,  0.5816,  0.4378],
          [ 0.5692,  0.5843,  0.5721,  0.5081,  0.2694,  0.2032,  0.1589,
            0.3464,  0.5349,  0.5768,  0.5739,  0.5764,  0.5394,  0.4482]]],
        grad_fn=&lt;SliceBackward0&gt;),
 True)
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> output of the <code class="docutils literal notranslate"><span class="pre">single_feature_map</span></code> and the <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> attribute means PyTorch is tracking the gradients of this feature map and it will be updated by gradient descent during training.</p>
</section>
<section id="flattening-the-patch-embedding-with-torch-nn-flatten">
<h3>4.4 Flattening the patch embedding with <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code><a class="headerlink" href="#flattening-the-patch-embedding-with-torch-nn-flatten" title="Permalink to this heading">#</a></h3>
<p>We’ve turned our image into patch embeddings but they’re still in 2D format.</p>
<p>How do we get them into the desired output shape of the patch embedding layer of the ViT model?</p>
<ul class="simple">
<li><p><strong>Desried output (1D sequence of flattened 2D patches):</strong> (196, 768) -&gt; (number of patches, embedding dimension) -&gt; <span class="math notranslate nohighlight">\({N \times\left(P^{2} \cdot C\right)}\)</span></p></li>
</ul>
<p>Let’s check the current shape.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Current tensor shape</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Current tensor shape: </span><span class="si">{</span><span class="n">image_out_of_conv</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Current tensor shape: torch.Size([1, 768, 14, 14]) -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]
</pre></div>
</div>
</div>
</div>
<p>Well we’ve got the 768 part ( <span class="math notranslate nohighlight">\((P^{2} \cdot C)\)</span> ) but we still need the number of patches (<span class="math notranslate nohighlight">\(N\)</span>).</p>
<p>Reading back through section 3.1 of the ViT paper it says (bold mine):</p>
<blockquote>
<div><p>As a special case, the patches can have spatial size <span class="math notranslate nohighlight">\(1 \times 1\)</span>, which means that the <strong>input sequence is obtained by simply <em>flattening</em> the spatial dimensions of the feature map and projecting to the Transformer dimension</strong>.</p>
</div></blockquote>
<p>Flattening the spatial dimensions of the feature map hey?</p>
<p>What layer do we have in PyTorch that can flatten?</p>
<p>How about <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code></a>?</p>
<p>But we don’t want to flatten the whole tensor, we only want to flatten the “spatial dimensions of the feature map”.</p>
<p>Which in our case is the <code class="docutils literal notranslate"><span class="pre">feature_map_height</span></code> and <code class="docutils literal notranslate"><span class="pre">feature_map_width</span></code> dimensions of <code class="docutils literal notranslate"><span class="pre">image_out_of_conv</span></code>.</p>
<p>So how about we create a <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code> layer to only flatten those dimensions, we can use the <code class="docutils literal notranslate"><span class="pre">start_dim</span></code> and <code class="docutils literal notranslate"><span class="pre">end_dim</span></code> parameters to set that up?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create flatten layer</span>
<span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># flatten feature_map_height (dimension 2)</span>
                     <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># flatten feature_map_width (dimension 3)</span>
</pre></div>
</div>
</div>
</div>
<p>Nice! Now let’s put it all together!</p>
<p>We’ll:</p>
<ol class="arabic simple">
<li><p>Take a single image.</p></li>
<li><p>Put in through the convolutional layer (<code class="docutils literal notranslate"><span class="pre">conv2d</span></code>) to turn the image into 2D feature maps (patch embeddings).</p></li>
<li><p>Flatten the 2D feature map into a single sequence.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. View single image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1"># adjust for matplotlib</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original image shape: </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 2. Turn image into feature maps</span>
<span class="n">image_out_of_conv</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="c1"># add batch dimension to avoid shape errors</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image feature map shape: </span><span class="si">{</span><span class="n">image_out_of_conv</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 3. Flatten the feature maps</span>
<span class="n">image_out_of_conv_flattened</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">image_out_of_conv</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Flattened image feature map shape: </span><span class="si">{</span><span class="n">image_out_of_conv_flattened</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original image shape: torch.Size([3, 224, 224])
Image feature map shape: torch.Size([1, 768, 14, 14])
Flattened image feature map shape: torch.Size([1, 768, 196])
</pre></div>
</div>
<img alt="../../_images/5b6269bcaf2b1f7df6f51d177cf38ec99f9bc6380f70394a57adc600c2a4c8bd.png" src="../../_images/5b6269bcaf2b1f7df6f51d177cf38ec99f9bc6380f70394a57adc600c2a4c8bd.png" />
</div>
</div>
<p>Woohoo! It looks like our <code class="docutils literal notranslate"><span class="pre">image_out_of_conv_flattened</span></code> shape is very close to our desired output shape:</p>
<ul class="simple">
<li><p><strong>Desried output (flattened 2D patches):</strong> (196, 768) -&gt; <span class="math notranslate nohighlight">\({N \times\left(P^{2} \cdot C\right)}\)</span></p></li>
<li><p><strong>Current shape:</strong> (1, 768, 196)</p></li>
</ul>
<p>The only difference is our current shape has a batch size and the dimensions are in a different order to the desired output.</p>
<p>How could we fix this?</p>
<p>Well, how about we rearrange the dimensions?</p>
<p>We can do so with <code class="docutils literal notranslate"><span class="pre">torch.Tensor.permute()</span></code> just like we do when rearranging image tensors to plot them with matplotlib.</p>
<p>Let’s try.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get flattened image patch embeddings in right shape </span>
<span class="n">image_out_of_conv_flattened_reshaped</span> <span class="o">=</span> <span class="n">image_out_of_conv_flattened</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># [batch_size, P^2•C, N] -&gt; [batch_size, N, P^2•C]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Patch embedding sequence shape: </span><span class="si">{</span><span class="n">image_out_of_conv_flattened_reshaped</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, num_patches, embedding_size]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Patch embedding sequence shape: torch.Size([1, 196, 768]) -&gt; [batch_size, num_patches, embedding_size]
</pre></div>
</div>
</div>
</div>
<p>Yes!!!</p>
<p>We’ve now matched the desired input and output shapes for the patch embedding layer of the ViT architecture using a couple of PyTorch layers.</p>
<p>How about we visualize one of the flattened feature maps?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get a single flattened feature map</span>
<span class="n">single_flattened_feature_map</span> <span class="o">=</span> <span class="n">image_out_of_conv_flattened_reshaped</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># index: (batch_size, number_of_patches, embedding_dimension)</span>

<span class="c1"># Plot the flattened feature map visually</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="mi">22</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">single_flattened_feature_map</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Flattened feature map shape: </span><span class="si">{</span><span class="n">single_flattened_feature_map</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/63545bceadf3f5ba740cfc98f3d954f29280d6c7669814786e6b4b329cb9d383.png" src="../../_images/63545bceadf3f5ba740cfc98f3d954f29280d6c7669814786e6b4b329cb9d383.png" />
</div>
</div>
<p>Hmm, the flattened feature map doesn’t look like much visually, but that’s not what we’re concerned about, this is what will be the output of the patching embedding layer and the input to the rest of the ViT architecture.</p>
<blockquote>
<div><p><strong>Note:</strong> The <a class="reference external" href="https://arxiv.org/abs/1706.03762">original Transformer architecture</a> was designed to work with text. The Vision Transformer architecture (ViT) had the goal of using the original Transformer for images. This is why the input to the ViT architecture is processed in the way it is. We’re essentially taking a 2D image and formatting it so it appears as a 1D sequence of text.</p>
</div></blockquote>
<p>How about we view the flattened feature map in tensor form?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># See the flattened feature map as a tensor</span>
<span class="n">single_flattened_feature_map</span><span class="p">,</span> <span class="n">single_flattened_feature_map</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">single_flattened_feature_map</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[ 0.4732,  0.3567,  0.3377,  0.3736,  0.3208,  0.3913,  0.3464,  0.3702,
           0.2541,  0.3594,  0.1984,  0.3982,  0.3741,  0.1251,  0.4178,  0.4771,
           0.3374,  0.3353,  0.3159,  0.4008,  0.3448,  0.3345,  0.5850,  0.4115,
           0.2969,  0.2751,  0.6150,  0.4188,  0.3209,  0.3776,  0.4970,  0.4272,
           0.3301,  0.4787,  0.2754,  0.3726,  0.3298,  0.4631,  0.3087,  0.4915,
           0.4129,  0.4592,  0.4540,  0.4930,  0.5570,  0.2660,  0.2150,  0.2044,
           0.2766,  0.2076,  0.3278,  0.3727,  0.2637,  0.2493,  0.2782,  0.3664,
           0.4920,  0.5671,  0.3298,  0.2992,  0.1437,  0.1701,  0.1554,  0.1375,
           0.1377,  0.3141,  0.2694,  0.2771,  0.2412,  0.3700,  0.5783,  0.5790,
           0.4229,  0.5032,  0.1216,  0.1000,  0.0356,  0.1258, -0.0023,  0.1640,
           0.2809,  0.2418,  0.2606,  0.3787,  0.5334,  0.5645,  0.4781,  0.3307,
           0.2391,  0.0461,  0.0095,  0.0542,  0.1012,  0.1331,  0.2446,  0.2526,
           0.3323,  0.4120,  0.5724,  0.2840,  0.5188,  0.3934,  0.1328,  0.0776,
           0.0235,  0.1366,  0.3149,  0.2200,  0.2793,  0.2351,  0.4722,  0.4785,
           0.4009,  0.4570,  0.4972,  0.5785,  0.2261,  0.1447, -0.0028,  0.2772,
           0.2697,  0.4008,  0.3606,  0.3372,  0.4535,  0.4492,  0.5678,  0.5870,
           0.5824,  0.3438,  0.5113,  0.0757,  0.1772,  0.3677,  0.3572,  0.3742,
           0.3820,  0.4868,  0.3781,  0.4694,  0.5845,  0.5877,  0.5826,  0.3212,
           0.5276,  0.4840,  0.4825,  0.5523,  0.5308,  0.5085,  0.5606,  0.5720,
           0.4928,  0.5581,  0.5853,  0.5849,  0.5793,  0.3410,  0.4428,  0.4044,
           0.3275,  0.4958,  0.4366,  0.5750,  0.5494,  0.5868,  0.5557,  0.5069,
           0.5880,  0.5888,  0.5796,  0.3377,  0.2635,  0.2347,  0.3145,  0.3486,
           0.5158,  0.5722,  0.5347,  0.5753,  0.5816,  0.4378,  0.5692,  0.5843,
           0.5721,  0.5081,  0.2694,  0.2032,  0.1589,  0.3464,  0.5349,  0.5768,
           0.5739,  0.5764,  0.5394,  0.4482]], grad_fn=&lt;SelectBackward0&gt;),
 True,
 torch.Size([1, 196]))
</pre></div>
</div>
</div>
</div>
<p>Beautiful!</p>
<p>We’ve turned our single 2D image into a 1D learnable embedding vector (or “Linear Projection of Flattned Patches” in Figure 1 of the ViT paper).</p>
</section>
<section id="turning-the-vit-patch-embedding-layer-into-a-pytorch-module">
<h3>4.5 Turning the ViT patch embedding layer into a PyTorch module<a class="headerlink" href="#turning-the-vit-patch-embedding-layer-into-a-pytorch-module" title="Permalink to this heading">#</a></h3>
<p>Time to put everything we’ve done for creating the patch embedding into a single PyTorch layer.</p>
<p>We can do so by subclassing <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and creating a small PyTorch “model” to do all of the steps above.</p>
<p>Specifically we’ll:</p>
<ol class="arabic simple">
<li><p>Create a class called <code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> which subclasses <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> (so it can be used a PyTorch layer).</p></li>
<li><p>Initialize the class with the parameters <code class="docutils literal notranslate"><span class="pre">in_channels=3</span></code>, <code class="docutils literal notranslate"><span class="pre">patch_size=16</span></code> (for ViT-Base) and <code class="docutils literal notranslate"><span class="pre">embedding_dim=768</span></code> (this is <span class="math notranslate nohighlight">\(D\)</span> for ViT-Base from Table 1).</p></li>
<li><p>Create a layer to turn an image into patches using <code class="docutils literal notranslate"><span class="pre">nn.Conv2d()</span></code> (just like in 4.3 above).</p></li>
<li><p>Create a layer to flatten the patch feature maps into a single dimension (just like in 4.4 above).</p></li>
<li><p>Define a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method to take an input and pass it through the layers created in 3 and 4.</p></li>
<li><p>Make sure the output shape reflects the required output shape of the ViT architecture (<span class="math notranslate nohighlight">\({N \times\left(P^{2} \cdot C\right)}\)</span>).</p></li>
</ol>
<p>Let’s do it!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create a class which subclasses nn.Module</span>
<span class="k">class</span> <span class="nc">PatchEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Turns a 2D input image into a 1D sequence learnable embedding vector.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of color channels for the input images. Defaults to 3.</span>
<span class="sd">        patch_size (int): Size of patches to convert input image into. Defaults to 16.</span>
<span class="sd">        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.</span>
<span class="sd">    &quot;&quot;&quot;</span> 
    <span class="c1"># 2. Initialize the class with appropriate variables</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">in_channels</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">patch_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                 <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># 3. Create a layer to turn an image into patches</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patcher</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                                 <span class="n">out_channels</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                 <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
                                 <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
                                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 4. Create a layer to flatten the patch feature maps into a single dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># only flatten the feature map dimensions into a single vector</span>
                                  <span class="n">end_dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="c1"># 5. Define the forward method </span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Create assertion to check that inputs are the correct shape</span>
        <span class="n">image_resolution</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">image_resolution</span> <span class="o">%</span> <span class="n">patch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Input image size must be divisble by patch size, image shape: </span><span class="si">{</span><span class="n">image_resolution</span><span class="si">}</span><span class="s2">, patch size: </span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Perform the forward pass</span>
        <span class="n">x_patched</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patcher</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x_flattened</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x_patched</span><span class="p">)</span> 
        <span class="c1"># 6. Make sure the output shape has the right order </span>
        <span class="k">return</span> <span class="n">x_flattened</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -&gt; [batch_size, N, P^2•C]</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> layer created!</p>
<p>Let’s try it out on a single image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_seeds</span><span class="p">()</span>

<span class="c1"># Create an instance of patch embedding layer</span>
<span class="n">patchify</span> <span class="o">=</span> <span class="n">PatchEmbedding</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                          <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                          <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">)</span>

<span class="c1"># Pass a single image through</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input image shape: </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">patch_embedded_image</span> <span class="o">=</span> <span class="n">patchify</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="c1"># add an extra batch dimension on the 0th index, otherwise will error</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output patch embedding shape: </span><span class="si">{</span><span class="n">patch_embedded_image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input image shape: torch.Size([1, 3, 224, 224])
Output patch embedding shape: torch.Size([1, 196, 768])
</pre></div>
</div>
</div>
</div>
<p>Beautiful!</p>
<p>The output shape matches the ideal input and output shapes we’d like to see from the patch embedding layer:</p>
<ul class="simple">
<li><p><strong>Input:</strong> The image starts as 2D with size <span class="math notranslate nohighlight">\({H \times W \times C}\)</span>.</p></li>
<li><p><strong>Output:</strong> The image gets converted to a 1D sequence of flattened 2D patches with size <span class="math notranslate nohighlight">\({N \times\left(P^{2} \cdot C\right)}\)</span>.</p></li>
</ul>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((H, W)\)</span> is the resolution of the original image.</p></li>
<li><p><span class="math notranslate nohighlight">\(C\)</span> is the number of channels.</p></li>
<li><p><span class="math notranslate nohighlight">\((P, P)\)</span> is the resolution of each image patch (<strong>patch size</strong>).</p></li>
<li><p><span class="math notranslate nohighlight">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.</p></li>
</ul>
<p>We’ve now replicated the patch embedding for equation 1 but not the class token/position embedding.</p>
<p>We’ll get to these later on.</p>
<img alt="replicating the vision transformer architecture patch embedding layer" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-replicating-the-patch-embedding-layer.png" />
<p><em>Our <code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> class (right) replicates the patch embedding of the ViT architecture from Figure 1 and Equation 1 from the ViT paper (left). However, the learnable class embedding and position embeddings haven’t been created yet. These will come soon.</em></p>
<p>Let’s now get a summary of our <code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create random input sizes</span>
<span class="n">random_input_image</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">random_input_image_error</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span> <span class="c1"># will error because image size is incompatible with patch_size</span>

<span class="c1"># # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)</span>
<span class="c1"># summary(PatchEmbedding(), </span>
<span class="c1">#         input_size=random_input_image, # try swapping this for &quot;random_input_image_error&quot; </span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;])</span>
</pre></div>
</div>
</div>
</div>
<img alt="summary output of patchembedding layer for vision transformer with input and output shapes" src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-summary-output-patchembedding.png" />
</section>
<section id="creating-the-class-token-embedding">
<h3>4.6 Creating the class token embedding<a class="headerlink" href="#creating-the-class-token-embedding" title="Permalink to this heading">#</a></h3>
<p>Okay we’ve made the image patch embedding, time to get to work on the class token embedding.</p>
<p>Or <span class="math notranslate nohighlight">\(\mathbf{x}_\text {class }\)</span> from equation 1.</p>
<img alt="class token embedding highlight from the vision transformer paper figure 1 and section 3.1" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-1-the-class-token.png" />
<p><em>Left: Figure 1 from the ViT paper with the “classification token” or <code class="docutils literal notranslate"><span class="pre">[class]</span></code> embedding token we’re going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the learnable class embedding token.</em></p>
<p>Reading the second paragraph of section 3.1 from the ViT paper, we see the following description:</p>
<blockquote>
<div><p>Similar to BERT’s <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">class</span> <span class="pre">]</span></code> token, we prepend a learnable embedding to the sequence of embedded patches <span class="math notranslate nohighlight">\(\left(\mathbf{z}_{0}^{0}=\mathbf{x}_{\text {class }}\right)\)</span>, whose state at the output of the Transformer encoder <span class="math notranslate nohighlight">\(\left(\mathbf{z}_{L}^{0}\right)\)</span> serves as the image representation <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> (Eq. 4).</p>
</div></blockquote>
<blockquote>
<div><p><strong>Note:</strong> <a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT</a> (Bidirectional Encoder Representations from Transformers) is one of the original machine learning research papers to use the Transformer architecture to achieve outstanding results on natural language processing (NLP) tasks and is where the idea of having a <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">class</span> <span class="pre">]</span></code> token at the start of a sequence originated, class being a description for the “classification” class the sequence belonged to.</p>
</div></blockquote>
<p>So we need to “preprend a learnable embedding to the sequence of embedded patches”.</p>
<p>Let’s start by viewing our sequence of embedded patches tensor (created in section 4.5) and its shape.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View the patch embedding and patch embedding shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">patch_embedded_image</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Patch embedding shape: </span><span class="si">{</span><span class="n">patch_embedded_image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, number_of_patches, embedding_dimension]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],
         [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],
         [-0.7589,  0.2633, -0.1695,  ...,  0.5897, -0.3980,  0.0761],
         ...,
         [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],
         [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],
         [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],
       grad_fn=&lt;PermuteBackward0&gt;)
Patch embedding shape: torch.Size([1, 196, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]
</pre></div>
</div>
</div>
</div>
<p>To “prepend a learnable embedding to the sequence of embedded patches” we need to create a learnable embedding in the shape of the <code class="docutils literal notranslate"><span class="pre">embedding_dimension</span></code> (<span class="math notranslate nohighlight">\(D\)</span>) and then add it to the <code class="docutils literal notranslate"><span class="pre">number_of_patches</span></code> dimension.</p>
<p>Or in pseudocode:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">patch_embedding</span> <span class="o">=</span> <span class="p">[</span><span class="n">image_patch_1</span><span class="p">,</span> <span class="n">image_patch_2</span><span class="p">,</span> <span class="n">image_patch_3</span><span class="o">...</span><span class="p">]</span>
<span class="n">class_token</span> <span class="o">=</span> <span class="n">learnable_embedding</span>
<span class="n">patch_embedding_with_class_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">class_token</span><span class="p">,</span> <span class="n">patch_embedding</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Notice the concatenation (<code class="docutils literal notranslate"><span class="pre">torch.cat()</span></code>) happens on <code class="docutils literal notranslate"><span class="pre">dim=1</span></code> (the <code class="docutils literal notranslate"><span class="pre">number_of_patches</span></code> dimension).</p>
<p>Let’s create a learnable embedding for the class token.</p>
<p>To do so, we’ll get the batch size and embedding dimension shape and then we’ll create a <code class="docutils literal notranslate"><span class="pre">torch.ones()</span></code> tensor in the shape <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">1,</span> <span class="pre">embedding_dimension]</span></code>.</p>
<p>And we’ll make the tensor learnable by passing it to <code class="docutils literal notranslate"><span class="pre">nn.Parameter()</span></code> with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the batch size and embedding dimension</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">patch_embedded_image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">embedding_dimension</span> <span class="o">=</span> <span class="n">patch_embedded_image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)</span>
<span class="n">class_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">),</span> <span class="c1"># [batch_size, number_of_tokens, embedding_dimension]</span>
                           <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># make sure the embedding is learnable</span>

<span class="c1"># Show the first 10 examples of the class_token</span>
<span class="nb">print</span><span class="p">(</span><span class="n">class_token</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">10</span><span class="p">])</span>

<span class="c1"># Print the class_token shape</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Class token shape: </span><span class="si">{</span><span class="n">class_token</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, number_of_tokens, embedding_dimension]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)
Class token shape: torch.Size([1, 1, 768]) -&gt; [batch_size, number_of_tokens, embedding_dimension]
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> Here we’re only creating the class token embedding as <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.ones.html"><code class="docutils literal notranslate"><span class="pre">torch.ones()</span></code></a> for demonstration purposes, in reality, you’d likely create the class token embedding with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.randn.html"><code class="docutils literal notranslate"><span class="pre">torch.randn()</span></code></a> (since machine learning is all about harnessing the power of controlled randomness, you generally start with a random number and improve it over time).</p>
</div></blockquote>
<p>See how the <code class="docutils literal notranslate"><span class="pre">number_of_tokens</span></code> dimension of <code class="docutils literal notranslate"><span class="pre">class_token</span></code> is <code class="docutils literal notranslate"><span class="pre">1</span></code> since we only want to prepend one class token value to the start of the patch embedding sequence.</p>
<p>Now we’ve got the class token embedding, let’s prepend it to our sequence of image patches, <code class="docutils literal notranslate"><span class="pre">patch_embedded_image</span></code>.</p>
<p>We can do so using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cat.html"><code class="docutils literal notranslate"><span class="pre">torch.cat()</span></code></a> and set <code class="docutils literal notranslate"><span class="pre">dim=1</span></code> (so <code class="docutils literal notranslate"><span class="pre">class_token</span></code>’s <code class="docutils literal notranslate"><span class="pre">number_of_tokens</span></code> dimension is preprended to <code class="docutils literal notranslate"><span class="pre">patch_embedded_image</span></code>’s <code class="docutils literal notranslate"><span class="pre">number_of_patches</span></code> dimension).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add the class token embedding to the front of the patch embedding</span>
<span class="n">patch_embedded_image_with_class_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">class_token</span><span class="p">,</span> <span class="n">patch_embedded_image</span><span class="p">),</span> 
                                                      <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># concat on first dimension</span>

<span class="c1"># Print the sequence of patch embeddings with the prepended class token embedding</span>
<span class="nb">print</span><span class="p">(</span><span class="n">patch_embedded_image_with_class_embedding</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sequence of patch embeddings with class token prepended shape: </span><span class="si">{</span><span class="n">patch_embedded_image_with_class_embedding</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, number_of_patches, embedding_dimension]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],
         [-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],
         [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],
         ...,
         [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],
         [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],
         [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],
       grad_fn=&lt;CatBackward0&gt;)
Sequence of patch embeddings with class token prepended shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]
</pre></div>
</div>
</div>
</div>
<p>Nice! Learnable class token prepended!</p>
<img alt="going from a sequence of patch embeddings, creating a learnable class token and then prepending it to the patch embeddings" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-1-prepending-the-learnable-class-token.png" />
<p><em>Reviewing what we’ve done to create the learnable class token, we start with a sequence of image patch embeddings created by <code class="docutils literal notranslate"><span class="pre">PatchEmbedding()</span></code> on single image, we then created a learnable class token with one value for each of the embedding dimensions and then prepended it to the original sequence of patch embeddings. <strong>Note:</strong> Using <code class="docutils literal notranslate"><span class="pre">torch.ones()</span></code> to create the learnable class token is mostly for demonstration purposes only, in practice, you’d like create it with <code class="docutils literal notranslate"><span class="pre">torch.randn()</span></code>.</em></p>
</section>
<section id="creating-the-position-embedding">
<h3>4.7 Creating the position embedding<a class="headerlink" href="#creating-the-position-embedding" title="Permalink to this heading">#</a></h3>
<p>Well, we’ve got the class token embedding and the patch embedding, now how might we create the position embedding?</p>
<p>Or <span class="math notranslate nohighlight">\(\mathbf{E}_{\text {pos }}\)</span> from equation 1 where <span class="math notranslate nohighlight">\(E\)</span> stands for “embedding”.</p>
<img alt="extracting the position embeddings from the vision transformer architecture and comparing them to other sections of the vision transformer paper" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-1-the-position-embeddings.png" />
<p><em>Left: Figure 1 from the ViT paper with the position embedding we’re going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the position embedding.</em></p>
<p>Let’s find out more by reading section 3.1 of the ViT paper (bold mine):</p>
<blockquote>
<div><p>Position embeddings are added to the patch embeddings to retain positional information. We use <strong>standard learnable 1D position embeddings</strong>, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.</p>
</div></blockquote>
<p>By “retain positional information” the authors mean they want the architecture to know what “order” the patches come in. As in, patch two comes after patch one and patch three comes after patch two and on and on.</p>
<p>This positional information can be important when considering what’s in an image (without positional information an a flattened sequence could be seen as having no order and thus no patch relates to any other patch).</p>
<p>To start creating the position embeddings, let’s view our current embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View the sequence of patch embeddings with the prepended class embedding</span>
<span class="n">patch_embedded_image_with_class_embedding</span><span class="p">,</span> <span class="n">patch_embedded_image_with_class_embedding</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],
          [-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],
          [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],
          ...,
          [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],
          [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],
          [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],
        grad_fn=&lt;CatBackward0&gt;),
 torch.Size([1, 197, 768]))
</pre></div>
</div>
</div>
</div>
<p>Equation 1 states that the position embeddings (<span class="math notranslate nohighlight">\(\mathbf{E}_{\text {pos }}\)</span>) should have the shape <span class="math notranslate nohighlight">\((N + 1) \times D\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{E}_{\text {pos }} \in \mathbb{R}^{(N+1) \times D}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer (number of patches).</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> is the size of the <strong>patch embeddings</strong>, different values for <span class="math notranslate nohighlight">\(D\)</span> can be found in Table 1 (embedding dimension).</p></li>
</ul>
<p>Luckily we’ve got both of these values already.</p>
<p>So let’s make a learnable 1D embedding with <code class="docutils literal notranslate"><span class="pre">torch.ones()</span></code> to create <span class="math notranslate nohighlight">\(\mathbf{E}_{\text {pos }}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate N (number of patches)</span>
<span class="n">number_of_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span> <span class="o">/</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Get embedding dimension</span>
<span class="n">embedding_dimension</span> <span class="o">=</span> <span class="n">patch_embedded_image_with_class_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># Create the learnable 1D position embedding</span>
<span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                                             <span class="n">number_of_patches</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> 
                                             <span class="n">embedding_dimension</span><span class="p">),</span>
                                  <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># make sure it&#39;s learnable</span>

<span class="c1"># Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding</span>
<span class="nb">print</span><span class="p">(</span><span class="n">position_embedding</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Position embeddding shape: </span><span class="si">{</span><span class="n">position_embedding</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, number_of_patches, embedding_dimension]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)
Position embeddding shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> Only creating the position embedding as <code class="docutils literal notranslate"><span class="pre">torch.ones()</span></code> for demonstration purposes, in reality, you’d likely create the position embedding with <code class="docutils literal notranslate"><span class="pre">torch.randn()</span></code> (start with a random number and improve via gradient descent).</p>
</div></blockquote>
<p>Position embeddings created!</p>
<p>Let’s add them to our sequence of patch embeddings with a prepended class token.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add the position embedding to the patch and class token embedding</span>
<span class="n">patch_and_position_embedding</span> <span class="o">=</span> <span class="n">patch_embedded_image_with_class_embedding</span> <span class="o">+</span> <span class="n">position_embedding</span>
<span class="nb">print</span><span class="p">(</span><span class="n">patch_and_position_embedding</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Patch embeddings, class token prepended and positional embeddings added shape: </span><span class="si">{</span><span class="n">patch_and_position_embedding</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; [batch_size, number_of_patches, embedding_dimension]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 2.0000,  2.0000,  2.0000,  ...,  2.0000,  2.0000,  2.0000],
         [ 0.0855,  1.2454,  0.7708,  ...,  1.6768,  0.5485,  1.3496],
         [ 0.2573,  1.1955,  0.6430,  ...,  1.5823,  0.6542,  1.3261],
         ...,
         [-0.0072,  1.2795,  0.7196,  ...,  1.7624,  0.5416,  1.3581],
         [ 0.0161,  1.1652,  0.8424,  ...,  1.7489,  0.4522,  1.3486],
         [ 0.0740,  1.1383,  0.8843,  ...,  1.5847,  0.5283,  1.3112]]],
       grad_fn=&lt;AddBackward0&gt;)
Patch embeddings, class token prepended and positional embeddings added shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]
</pre></div>
</div>
</div>
</div>
<p>Notice how the values of each of the elements in the embedding tensor increases by 1 (this is because of the position embeddings being created with <code class="docutils literal notranslate"><span class="pre">torch.ones()</span></code>).</p>
<blockquote>
<div><p><strong>Note:</strong> We could put both the class token embedding and position embedding into their own layer if we wanted to. But we’ll see later on in section 8 how they can be incorporated into the overall ViT architecture’s <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
</div></blockquote>
<img alt="patch embeddings with learnable class token and position embeddings" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-1-patch-embeddings-with-learnable-class-token-and-position-embeddings.png" />
<p><em>The workflow we’ve used for adding the position embeddings to the sequence of patch embeddings and class token. <strong>Note:</strong> <code class="docutils literal notranslate"><span class="pre">torch.ones()</span></code> only used to create embeddings for illustration purposes, in practice, you’d likely use <code class="docutils literal notranslate"><span class="pre">torch.randn()</span></code> to start with a random number.</em></p>
</section>
<section id="putting-it-all-together-from-image-to-embedding">
<h3>4.8 Putting it all together: from image to embedding<a class="headerlink" href="#putting-it-all-together-from-image-to-embedding" title="Permalink to this heading">#</a></h3>
<p>Alright, we’ve come a long way in terms of turning our input images into an embedding and replicating equation 1 from section 3.1 of the ViT paper:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{z}_{0} &amp;=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1} \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{\text {pos }}, &amp; &amp; \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{\text {pos }} \in \mathbb{R}^{(N+1) \times D}
\end{aligned}
\]</div>
<p>Let’s now put everything together in a single code cell and go from input image (<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>) to output embedding (<span class="math notranslate nohighlight">\(\mathbf{z}_0\)</span>).</p>
<p>We can do so by:</p>
<ol class="arabic simple">
<li><p>Setting the patch size (we’ll use <code class="docutils literal notranslate"><span class="pre">16</span></code> as it’s widely used throughout the paper and for ViT-Base).</p></li>
<li><p>Getting a single image, printing its shape and storing its height and width.</p></li>
<li><p>Adding a batch dimension to the single image so it’s compatible with our <code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> layer.</p></li>
<li><p>Creating a <code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> layer (the one we made in section 4.5) with a <code class="docutils literal notranslate"><span class="pre">patch_size=16</span></code> and <code class="docutils literal notranslate"><span class="pre">embedding_dim=768</span></code> (from Table 1 for ViT-Base).</p></li>
<li><p>Passing the single image through the <code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> layer in 4 to create a sequence of patch embeddings.</p></li>
<li><p>Creating a class token embedding like in section 4.6.</p></li>
<li><p>Prepending the class token emebdding to the patch embeddings created in step 5.</p></li>
<li><p>Creating a position embedding like in section 4.7.</p></li>
<li><p>Adding the position embedding to the class token and patch embeddings created in step 7.</p></li>
</ol>
<p>We’ll also make sure to set the random seeds with <code class="docutils literal notranslate"><span class="pre">set_seeds()</span></code> and print out the shapes of different tensors along the way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_seeds</span><span class="p">()</span>

<span class="c1"># 1. Set patch size</span>
<span class="n">patch_size</span> <span class="o">=</span> <span class="mi">16</span>

<span class="c1"># 2. Print shape of original image tensor and get the image dimensions</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image tensor shape: </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># 3. Get image tensor and add batch dimension</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input image with batch dimension shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 4. Create patch embedding layer</span>
<span class="n">patch_embedding_layer</span> <span class="o">=</span> <span class="n">PatchEmbedding</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                       <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
                                       <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">)</span>

<span class="c1"># 5. Pass image through patch embedding layer</span>
<span class="n">patch_embedding</span> <span class="o">=</span> <span class="n">patch_embedding_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Patching embedding shape: </span><span class="si">{</span><span class="n">patch_embedding</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 6. Create class token embedding</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">patch_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">embedding_dimension</span> <span class="o">=</span> <span class="n">patch_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">class_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">),</span>
                           <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># make sure it&#39;s learnable</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Class token embedding shape: </span><span class="si">{</span><span class="n">class_token</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 7. Prepend class token embedding to patch embedding</span>
<span class="n">patch_embedding_class_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">class_token</span><span class="p">,</span> <span class="n">patch_embedding</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Patch embedding with class token shape: </span><span class="si">{</span><span class="n">patch_embedding_class_token</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 8. Create position embedding</span>
<span class="n">number_of_patches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span> <span class="o">/</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">number_of_patches</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">),</span>
                                  <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># make sure it&#39;s learnable</span>

<span class="c1"># 9. Add position embedding to patch embedding with class token</span>
<span class="n">patch_and_position_embedding</span> <span class="o">=</span> <span class="n">patch_embedding_class_token</span> <span class="o">+</span> <span class="n">position_embedding</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Patch and position embedding shape: </span><span class="si">{</span><span class="n">patch_and_position_embedding</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Image tensor shape: torch.Size([3, 224, 224])
Input image with batch dimension shape: torch.Size([1, 3, 224, 224])
Patching embedding shape: torch.Size([1, 196, 768])
Class token embedding shape: torch.Size([1, 1, 768])
Patch embedding with class token shape: torch.Size([1, 197, 768])
Patch and position embedding shape: torch.Size([1, 197, 768])
</pre></div>
</div>
</div>
</div>
<p>Woohoo!</p>
<p>From a single image to patch and position embeddings in a single cell of code.</p>
<img alt="mapping equation 1 from the vision transformer paper to pytorch code" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-1-putting-it-all-together.png" />
<p><em>Mapping equation 1 from the ViT paper to our PyTorch code. This is the essence of paper replicating, taking a research paper and turning it into usable code.</em></p>
<p>Now we’ve got a way to encode our images and pass them to the Transformer Encoder in Figure 1 of the ViT paper.</p>
<img alt="Vision transformer architecture animation, going from a single image and passing it through a patch embedidng layer and then passing it through the transformer encoder." src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-architecture-animation-full-architecture.gif" />
<p><em>Animating the entire ViT workflow: from patch embeddings to transformer encoder to MLP head.</em></p>
<p>From a code perspective, creating the patch embedding is probably the largest section of replicating the ViT paper.</p>
<p>Many of the other parts of the ViT paper such as the Multi-Head Attention and Norm layers can be created using existing PyTorch layers.</p>
<p>Onwards!</p>
</section>
</section>
<section id="equation-2-multi-head-attention-msa">
<h2>5. Equation 2: Multi-Head Attention (MSA)<a class="headerlink" href="#equation-2-multi-head-attention-msa" title="Permalink to this heading">#</a></h2>
<p>We’ve got our input data patchified and embedded, now let’s move onto the next part of the ViT architecture.</p>
<p>To start, we’ll break down the Transformer Encoder section into two parts (start small and increase when necessary).</p>
<p>The first being equation 2 and the second being equation 3.</p>
<p>Recall equation 2 states:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{z}_{\ell}^{\prime} &amp;=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1}, &amp; &amp; \ell=1 \ldots L
\end{aligned}
\]</div>
<p>This indicates a Multi-Head Attention (MSA) layer wrapped in a LayerNorm (LN) layer with a residual connection (the input to the layer gets added to the output of the layer).</p>
<p>We’ll refer to equation 2 as the “MSA block”.</p>
<img alt="mapping equation 2 from the ViT paper to the ViT architecture diagram in figure 1" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-2-msa-block-mapped-to-equation.png" />
<p><em><strong>Left:</strong> Figure 1 from the ViT paper with Multi-Head Attention and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. <strong>Right:</strong> Mapping the Multi-Head Self Attention (MSA) layer, Norm layer and residual connection to their respective parts of equation 2 in the ViT paper.</em></p>
<p>Many layers you find in research papers are already implemented in modern deep learning frameworks such as PyTorch.</p>
<p>In saying this, to replicate these layers and residual connection with PyTorch code we can use:</p>
<ul class="simple">
<li><p><strong>Multi-Head Self Attention (MSA)</strong> - <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.MultiheadAttention()</span></code></a>.</p></li>
<li><p><strong>Norm (LN or LayerNorm)</strong> - <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code></a>.</p></li>
<li><p><strong>Residual connection</strong> - add the input to output (we’ll see this later on when we create the full Transformer Encoder block in section 7.1).</p></li>
</ul>
<section id="the-layernorm-ln-layer">
<h3>5.1 The LayerNorm (LN) layer<a class="headerlink" href="#the-layernorm-ln-layer" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://paperswithcode.com/method/layer-normalization">Layer Normalization</a> (<code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code> or Norm or LayerNorm or LN) normalizes an input over the last dimension.</p>
<p>You can find the formal definition of <code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code> in the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">PyTorch documentation</a>.</p>
<p>PyTorch’s <code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code>’s main parameter is <code class="docutils literal notranslate"><span class="pre">normalized_shape</span></code> which we can set to be equal to the dimension size we’d like to noramlize over (in our case it’ll be <span class="math notranslate nohighlight">\(D\)</span> or <code class="docutils literal notranslate"><span class="pre">768</span></code> for ViT-Base).</p>
<p>What does it do?</p>
<p>Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data).</p>
<p>I like to think of any kind of normalization as “getting the data into a similar format” or “getting data samples into a similar distribution”.</p>
<p>Imagine trying to walk up (or down) a set of stairs all with differing heights and lengths.</p>
<p>It’d take some adjustment on each step right?</p>
<p>And what you learn for each step wouldn’t necessary help with the next one since they all differ, increasing the time it takes you to navigate the stairs.</p>
<p>Normalization (including Layer Normalization) is the equivalent of making all the stairs the same height and length except the stairs are your data samples.</p>
<p>So just like you can walk up (or down) stairs with similar heights and lengths much easier than those with unequal heights and widths, neural networks can optimize over data samples with similar distributions (similar mean and standard-deviations) easier than those with varying distributions.</p>
</section>
<section id="the-multi-head-self-attention-msa-layer">
<h3>5.2 The Multi-Head Self Attention (MSA) layer<a class="headerlink" href="#the-multi-head-self-attention-msa-layer" title="Permalink to this heading">#</a></h3>
<p>The power of the self-attention and multi-head attention (self-attention applied multiple times) were revealed in the form of the original Transformer architecture introduced in the <a class="reference external" href="https://arxiv.org/abs/1706.03762"><em>Attention is all you need</em></a> research paper.</p>
<p>Originally designed for text inputs, the original self-attention mechanism takes a sequence of words and then calculates which word should pay more “attention” to another word.</p>
<p>In other words, in the sentence “the dog jumped over the fence”, perhaps the word “dog” relates strongly to “jumped” and “fence”.</p>
<p>This is simplified but the premise remains for images.</p>
<p>Since our input is a sequence of image patches rather than words, self-attention and in turn multi-head attention will calculate which patch of an image is most related to another patch, eventually forming a learned representation of an image.</p>
<p>But what’s most important is that the layer does this on it’s own given the data (we don’t tell it what patterns to learn).</p>
<p>And if the learned representation the layers form using MSA are good, we’ll see the results in our model’s performance.</p>
<p>There are many resources online to learn more about the Transformer architeture and attention mechanism online such as Jay Alammar’s wonderful <a class="reference external" href="https://jalammar.github.io/illustrated-transformer/">Illustrated Transformer post</a> and <a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Illustrated Attention post</a>.</p>
<p>We’re going to focus more on coding an existing PyTorch MSA implementation than creating our own.</p>
<p>However, you can find the formal defintion of the ViT paper’s MSA implementation is defined in Appendix A:</p>
<img alt="vision transformer paper figure 1 highlighted with equation 2 and appendix A" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-2-appendix-A.png" />
<p><em><strong>Left:</strong> Vision Transformer architecture overview from Figure 1 of the ViT paper. <strong>Right:</strong> Definitions of equation 2, section 3.1 and Appendix A of the ViT paper highlighted to reflect their respective parts in Figure 1.</em></p>
<p>The image above highlights the triple embedding input to the MSA layer.</p>
<p>This is known as <strong>query, key, value</strong> input or <strong>qkv</strong> for short which is fundamental to the self-attention mechanism.</p>
<p>In our case, the triple embedding input will be three versions of the output of the Norm layer, one for query, key and value.</p>
<p>Or three versions of our layer-normalized image patch and position embeddings created in section 4.8.</p>
<p>We can implement the MSA layer in PyTorch with <code class="docutils literal notranslate"><span class="pre">torch.nn.MultiheadAttention()</span></code> with the parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> - the embedding dimension from Table 1 (Hidden size <span class="math notranslate nohighlight">\(D\)</span>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_heads</span></code> - how many attention heads to use (this is where the term “multihead” comes from), this value is also in Table 1 (Heads).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dropout</span></code> - whether or not to apply dropout to the attention layer (according to Appendix B.1, dropout isn’t used after the qkv-projections).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_first</span></code> - does our batch dimension come first? (yes it does)</p></li>
</ul>
</section>
<section id="replicating-equation-2-with-pytorch-layers">
<h3>5.3 Replicating Equation 2 with PyTorch layers<a class="headerlink" href="#replicating-equation-2-with-pytorch-layers" title="Permalink to this heading">#</a></h3>
<p>Let’s put everything we’ve discussed about the LayerNorm (LN) and Multi-Head Attention (MSA) layers in equation 2 into practice.</p>
<p>To do so, we’ll:</p>
<ol class="arabic simple">
<li><p>Create a class called <code class="docutils literal notranslate"><span class="pre">MultiheadSelfAttentionBlock</span></code> that inherits from <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p></li>
<li><p>Initialize the class with hyperparameters from Table 1 of the ViT paper for the ViT-Base model.</p></li>
<li><p>Create a layer normalization (LN) layer with <code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code> with the <code class="docutils literal notranslate"><span class="pre">normalized_shape</span></code> parameter the same as our embedding dimension (<span class="math notranslate nohighlight">\(D\)</span> from Table 1).</p></li>
<li><p>Create a multi-head attention (MSA) layer with the appropriate <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code>, <code class="docutils literal notranslate"><span class="pre">num_heads</span></code>, <code class="docutils literal notranslate"><span class="pre">dropout</span></code> and <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> parameters.</p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method for our class passing the in the inputs through the LN layer and MSA layer.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create a class that inherits from nn.Module</span>
<span class="k">class</span> <span class="nc">MultiheadSelfAttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a multi-head self-attention block (&quot;MSA block&quot; for short).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 2. Initialize the class with hyperparameters from Table 1</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># Hidden size D from Table 1 for ViT-Base</span>
                 <span class="n">num_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Heads from Table 1 for ViT-Base</span>
                 <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="c1"># doesn&#39;t look like the paper uses any dropout in MSABlocks</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># 3. Create the Norm layer (LN)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>
        
        <span class="c1"># 4. Create the Multi-Head Attention (MSA) layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multihead_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                                    <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                    <span class="n">dropout</span><span class="o">=</span><span class="n">attn_dropout</span><span class="p">,</span>
                                                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># does our batch dimension come first?</span>
        
    <span class="c1"># 5. Create a forward() method to pass the data throguh the layers</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multihead_attn</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="c1"># query embeddings </span>
                                             <span class="n">key</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="c1"># key embeddings</span>
                                             <span class="n">value</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="c1"># value embeddings</span>
                                             <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># do we need the weights or just the layer outputs?</span>
        <span class="k">return</span> <span class="n">attn_output</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> Unlike Figure 1, our <code class="docutils literal notranslate"><span class="pre">MultiheadSelfAttentionBlock</span></code> doesn’t include a skip or residual connection (“<span class="math notranslate nohighlight">\(+\mathbf{z}_{\ell-1}\)</span>” in equation 2), we’ll include this when we create the entire Transformer Encoder later on in section 7.1.</p>
</div></blockquote>
<p>MSABlock created!</p>
<p>Let’s try it out by create an instance of our <code class="docutils literal notranslate"><span class="pre">MultiheadSelfAttentionBlock</span></code> and passing through the <code class="docutils literal notranslate"><span class="pre">patch_and_position_embedding</span></code> variable we created in section 4.8.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an instance of MSABlock</span>
<span class="n">multihead_self_attention_block</span> <span class="o">=</span> <span class="n">MultiheadSelfAttentionBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># from Table 1 </span>
                                                             <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span> <span class="c1"># from Table 1</span>

<span class="c1"># Pass patch and position image embedding through MSABlock</span>
<span class="n">patched_image_through_msa_block</span> <span class="o">=</span> <span class="n">multihead_self_attention_block</span><span class="p">(</span><span class="n">patch_and_position_embedding</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input shape of MSA block: </span><span class="si">{</span><span class="n">patch_and_position_embedding</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape MSA block: </span><span class="si">{</span><span class="n">patched_image_through_msa_block</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input shape of MSA block: torch.Size([1, 197, 768])
Output shape MSA block: torch.Size([1, 197, 768])
</pre></div>
</div>
</div>
</div>
<p>Notice how the input and output shape of our data stays the same when it goes through the MSA block.</p>
<p>This doesn’t mean the data doesn’t change as it goes through.</p>
<p>You could try printing the input and output tensor to see how it changes (though this change will be across <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">*</span> <span class="pre">197</span> <span class="pre">*</span> <span class="pre">768</span></code> values and could be hard to visualize).</p>
<img alt="vision transformer paper with equation 2 of figure 1 highlighted and equation 2 turned into code" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-2-in-code.png" />
<p><em><strong>Left:</strong> Vision Transformer architecture from Figure 1 with Multi-Head Attention and LayerNorm layers highlighted, these layers make up equation 2 from section 3.1 of the paper. <strong>Right:</strong> Replicating equation 2 (without the skip connection on the end) using PyTorch layers.</em></p>
<p>We’ve now officially replicated equation 2 (except for the residual connection on the end but we’ll get to this in section 7.1)!</p>
<p>Onto the next!</p>
</section>
</section>
<section id="equation-3-multilayer-perceptron-mlp">
<h2>6. Equation 3: Multilayer Perceptron (MLP)<a class="headerlink" href="#equation-3-multilayer-perceptron-mlp" title="Permalink to this heading">#</a></h2>
<p>We’re on a roll here!</p>
<p>Let’s keep it going and replicate equation 3:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{z}_{\ell} &amp;=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, &amp; &amp; \ell=1 \ldots L
\end{aligned}
\]</div>
<p>Here MLP stands for “multilayer perceptron” and LN stands for “layer normalization” (as discussed above).</p>
<p>And the addition on the end is the skip/residual connection.</p>
<p>We’ll refer to equation 3 as the “MLP block” of the Transformer encoder (notice how we’re continuing the trend of breaking down the architecture into smaller chunks).</p>
<img alt="mapping equation 3 from the ViT paper to the ViT architecture diagram in figure 1" src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-equation-3-annotated.png" />
<p><em><strong>Left:</strong> Figure 1 from the ViT paper with MLP and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. <strong>Right:</strong> Mapping the multilayer perceptron (MLP) layer, Norm layer (LN) and residual connection to their respective parts of equation 3 in the ViT paper.</em></p>
<section id="the-mlp-layer-s">
<h3>6.1 The MLP layer(s)<a class="headerlink" href="#the-mlp-layer-s" title="Permalink to this heading">#</a></h3>
<p>The term <a class="reference external" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a> is quite broad as it can refer to almost any combination of <em>multiple</em> layers (hence the “multi” in multilayer perceptron).</p>
<p>But it generally follows the pattern of:</p>
<p><code class="docutils literal notranslate"><span class="pre">linear</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">non-linear</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">linear</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">non-linear</span> <span class="pre">layer</span></code></p>
<p>In the the case of the ViT paper, the MLP structure is defined in section 3.1:</p>
<blockquote>
<div><p>The MLP contains two layers with a GELU non-linearity.</p>
</div></blockquote>
<p>Where “two layers” refers to linear layers (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Linear()</span></code></a> in PyTorch) and “GELU non-linearity” is the GELU  (Gaussian Error Linear Units) non-linear activation function (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.GELU()</span></code></a> in PyTorch).</p>
<blockquote>
<div><p><strong>Note:</strong> A linear layer (<code class="docutils literal notranslate"><span class="pre">torch.nn.Linear()</span></code>) can sometimes also be referred to as a “dense layer” or “feedforward layer”. Some papers even use all three terms to describe the same thing (as in the ViT paper).</p>
</div></blockquote>
<p>Another sneaky detail about the MLP block doesn’t appear until Appendix B.1 (Training):</p>
<blockquote>
<div><p>Table 3 summarizes our training setups for our different models. …Dropout, when used, is applied <strong>after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings.</strong></p>
</div></blockquote>
<p>This means that every linear layer (or dense layer) in the MLP block has a dropout layer (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Dropout()</span></code></a> in PyTorch).</p>
<p>The value of which can be found in Table 3 of the ViT paper (for ViT-Base, <code class="docutils literal notranslate"><span class="pre">dropout=0.1</span></code>).</p>
<p>Knowing this, the structure of our MLP block will be:</p>
<p><code class="docutils literal notranslate"><span class="pre">layer</span> <span class="pre">norm</span> <span class="pre">-&gt;</span> <span class="pre">linear</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">non-linear</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">dropout</span> <span class="pre">-&gt;</span> <span class="pre">linear</span> <span class="pre">layer</span> <span class="pre">-&gt;</span> <span class="pre">dropout</span></code></p>
<p>With hyperparameter values for the linear layers available from Table 1 (MLP size is the number of hidden units between the linear layers and hidden size <span class="math notranslate nohighlight">\(D\)</span> is the output size of the MLP block).</p>
</section>
<section id="replicating-equation-3-with-pytorch-layers">
<h3>6.2 Replicating Equation 3 with PyTorch layers<a class="headerlink" href="#replicating-equation-3-with-pytorch-layers" title="Permalink to this heading">#</a></h3>
<p>Let’s put everything we’ve discussed about the LayerNorm (LN) and MLP (MSA) layers in equation 3 into practice.</p>
<p>To do so, we’ll:</p>
<ol class="arabic simple">
<li><p>Create a class called <code class="docutils literal notranslate"><span class="pre">MLPBlock</span></code> that inherits from <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p></li>
<li><p>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</p></li>
<li><p>Create a layer normalization (LN) layer with <code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code> with the <code class="docutils literal notranslate"><span class="pre">normalized_shape</span></code> parameter the same as our embedding dimension (<span class="math notranslate nohighlight">\(D\)</span> from Table 1).</p></li>
<li><p>Create a sequential series of MLP layers(s) using <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear()</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.nn.Dropout()</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.nn.GELU()</span></code> with appropriate hyperparameter values from Table 1 and Table 3.</p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method for our class passing the in the inputs through the LN layer and MLP layer(s).</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create a class that inherits from nn.Module</span>
<span class="k">class</span> <span class="nc">MLPBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a layer normalized multilayer perceptron block (&quot;MLP block&quot; for short).&quot;&quot;&quot;</span>
    <span class="c1"># 2. Initialize the class with hyperparameters from Table 1 and Table 3</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># Hidden Size D from Table 1 for ViT-Base</span>
                 <span class="n">mlp_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="c1"># MLP size from Table 1 for ViT-Base</span>
                 <span class="n">dropout</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span> <span class="c1"># Dropout from Table 3 for ViT-Base</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># 3. Create the Norm layer (LN)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>
        
        <span class="c1"># 4. Create the Multilayer perceptron (MLP) layer(s)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                      <span class="n">out_features</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span> <span class="c1"># &quot;The MLP contains two layers with a GELU non-linearity (section 3.1).&quot;</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">,</span> <span class="c1"># needs to take same in_features as out_features of layer above</span>
                      <span class="n">out_features</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">),</span> <span class="c1"># take back to embedding_dim</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span> <span class="c1"># &quot;Dropout, when used, is applied after every dense layer..&quot;</span>
        <span class="p">)</span>
    
    <span class="c1"># 5. Create a forward() method to pass the data throguh the layers</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Note:</strong> Unlike Figure 1, our <code class="docutils literal notranslate"><span class="pre">MLPBlock()</span></code> doesn’t include a skip or residual connection (“<span class="math notranslate nohighlight">\(+\mathbf{z}_{\ell}^{\prime}\)</span>” in equation 3), we’ll include this when we create the entire Transformer encoder later on.</p>
</div></blockquote>
<p>MLPBlock class created!</p>
<p>Let’s try it out by create an instance of our <code class="docutils literal notranslate"><span class="pre">MLPBlock</span></code> and passing through the <code class="docutils literal notranslate"><span class="pre">patched_image_through_msa_block</span></code> variable we created in section 5.3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an instance of MLPBlock</span>
<span class="n">mlp_block</span> <span class="o">=</span> <span class="n">MLPBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># from Table 1 </span>
                     <span class="n">mlp_size</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="c1"># from Table 1</span>
                     <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># from Table 3</span>

<span class="c1"># Pass output of MSABlock through MLPBlock</span>
<span class="n">patched_image_through_mlp_block</span> <span class="o">=</span> <span class="n">mlp_block</span><span class="p">(</span><span class="n">patched_image_through_msa_block</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input shape of MLP block: </span><span class="si">{</span><span class="n">patched_image_through_mlp_block</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape MLP block: </span><span class="si">{</span><span class="n">patched_image_through_mlp_block</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input shape of MLP block: torch.Size([1, 197, 768])
Output shape MLP block: torch.Size([1, 197, 768])
</pre></div>
</div>
</div>
</div>
<p>Notice how the input and output shape of our data again stays the same when it goes in and out of the MLP block.</p>
<p>However, the shape does change when the data gets passed through the <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> layers within the MLP block (expanded to MLP size from Table 1 and then compressed back to Hidden size <span class="math notranslate nohighlight">\(D\)</span> from Table 1).</p>
<img alt="vision transformer paper with equation 3 of figure 1 highlighted and equation 3 turned into code" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-equation-3-mapped-to-code.png" />
<p><em>Left: Vision Transformer architecture from Figure 1 with MLP and Norm layers highlighted, these layers make up equation 3 from section 3.1 of the paper. Right: Replicating equation 3 (without the skip connection on the end) using PyTorch layers.</em></p>
<p>Ho ho!</p>
<p>Equation 3 replicated (except for the residual connection on the end but we’ll get to this in section 7.1)!</p>
<p>Now we’ve got equation’s 2 and 3 in PyTorch code, let’s now put them together to create the Transformer Encoder.</p>
</section>
</section>
<section id="create-the-transformer-encoder">
<h2>7. Create the Transformer Encoder<a class="headerlink" href="#create-the-transformer-encoder" title="Permalink to this heading">#</a></h2>
<p>Time to stack together our <code class="docutils literal notranslate"><span class="pre">MultiheadSelfAttentionBlock</span></code> (equation 2) and <code class="docutils literal notranslate"><span class="pre">MLPBlock</span></code> (equation 3) and create the Transformer Encoder of the ViT architecture.</p>
<p>In deep learning, an <a class="reference external" href="https://paperswithcode.com/method/autoencoder">“encoder” or “auto encoder”</a> generally refers to a stack of layers that “encodes” an input (turns it into some form of numerical representation).</p>
<p>In our case, the Transformer Encoder will encode our patched image embedding into a learned representation using a series of alternating layers of MSA blocks and MLP blocks, as per section 3.1 of the ViT Paper:</p>
<blockquote>
<div><p>The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). <strong>Layernorm (LN) is applied before every block</strong>, and <strong>residual connections after every block</strong> (Wang et al., 2019; Baevski &amp; Auli, 2019).</p>
</div></blockquote>
<p>We’ve created MSA and MLP blocks but what about the residual connections?</p>
<p><a class="reference external" href="https://paperswithcode.com/method/residual-connection">Residual connections</a> (also called skip connections), were first introduced in the paper <a class="reference external" href="https://arxiv.org/abs/1512.03385v1"><em>Deep Residual Learning for Image Recognition</em></a> and are achieved by adding a layer(s) input to its subsequent output.</p>
<p>Where the subsequence output might be one or more layers later.</p>
<p>In the case of the ViT architecture, the residual connection means the input of the MSA block is added back to the output of the MSA block before it passes to the MLP block.</p>
<p>And the same thing happens with the MLP block before it goes onto the next Transformer Encoder block.</p>
<p>Or in pseudocode:</p>
<p><code class="docutils literal notranslate"><span class="pre">x_input</span> <span class="pre">-&gt;</span> <span class="pre">MSA_block</span> <span class="pre">-&gt;</span> <span class="pre">[MSA_block_output</span> <span class="pre">+</span> <span class="pre">x_input]</span> <span class="pre">-&gt;</span> <span class="pre">MLP_block</span> <span class="pre">-&gt;</span> <span class="pre">[MLP_block_output</span> <span class="pre">+</span> <span class="pre">MSA_block_output</span> <span class="pre">+</span> <span class="pre">x_input]</span> <span class="pre">-&gt;</span> <span class="pre">...</span></code></p>
<p>What does this do?</p>
<p>One of the main ideas behind residual connections is that they prevent weight values and gradient updates from getting too small and thus allow deeper networks and in turn allow deeper representations to be learned.</p>
<blockquote>
<div><p><strong>Note:</strong> The iconic computer vision architecture “ResNet” is named so because of the introduction of <em>res</em>idual connections. You can find many pretrained versions of ResNet architectures in <a class="reference external" href="https://pytorch.org/vision/stable/models.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a>.</p>
</div></blockquote>
<section id="creating-a-transformer-encoder-by-combining-our-custom-made-layers">
<h3>7.1 Creating a Transformer Encoder by combining our custom made layers<a class="headerlink" href="#creating-a-transformer-encoder-by-combining-our-custom-made-layers" title="Permalink to this heading">#</a></h3>
<p>Enough talk, let’s see this in action and make a ViT Transformer Encoder with PyTorch by combining our previously created layers.</p>
<p>To do so, we’ll:</p>
<ol class="arabic simple">
<li><p>Create a class called <code class="docutils literal notranslate"><span class="pre">TransformerEncoderBlock</span></code> that inherits from <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p></li>
<li><p>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</p></li>
<li><p>Instantiate a MSA block for equation 2 using our <code class="docutils literal notranslate"><span class="pre">MultiheadSelfAttentionBlock</span></code> from section 5.2 with the appropriate parameters.</p></li>
<li><p>Instantiate a MLP block for equation 3 using our <code class="docutils literal notranslate"><span class="pre">MLPBlock</span></code> from section 6.2 with the appropriate parameters.</p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method for our <code class="docutils literal notranslate"><span class="pre">TransformerEncoderBlock</span></code> class.</p></li>
<li><p>Create a residual connection for the MSA block (for equation 2).</p></li>
<li><p>Create a residual connection for the MLP block (for equation 3).</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create a class that inherits from nn.Module</span>
<span class="k">class</span> <span class="nc">TransformerEncoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a Transformer Encoder block.&quot;&quot;&quot;</span>
    <span class="c1"># 2. Initialize the class with hyperparameters from Table 1 and Table 3</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># Hidden size D from Table 1 for ViT-Base</span>
                 <span class="n">num_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Heads from Table 1 for ViT-Base</span>
                 <span class="n">mlp_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="c1"># MLP size from Table 1 for ViT-Base</span>
                 <span class="n">mlp_dropout</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="c1"># Amount of dropout for dense layers from Table 3 for ViT-Base</span>
                 <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="c1"># Amount of dropout for attention layers</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># 3. Create MSA block (equation 2)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">msa_block</span> <span class="o">=</span> <span class="n">MultiheadSelfAttentionBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                                     <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                     <span class="n">attn_dropout</span><span class="o">=</span><span class="n">attn_dropout</span><span class="p">)</span>
        
        <span class="c1"># 4. Create MLP block (equation 3)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_block</span> <span class="o">=</span>  <span class="n">MLPBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                   <span class="n">mlp_size</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">,</span>
                                   <span class="n">dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">)</span>
        
    <span class="c1"># 5. Create a forward() method  </span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="c1"># 6. Create residual connection for MSA block (add the input to the output)</span>
        <span class="n">x</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">msa_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span> 
        
        <span class="c1"># 7. Create residual connection for MLP block (add the input to the output)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span> 
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Beautiful!</p>
<p>Transformer Encoder block created!</p>
<img alt="vision transformer architecture with transformer encoder blocks highlighted" src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-transformer-encoder-highlighted.png" />
<p><em><strong>Left:</strong> Figure 1 from the ViT paper with the Transformer Encoder of the ViT architecture highlighted. <strong>Right:</strong> Transformer Encoder mapped to equation 2 and 3 of the ViT paper, the Transformer Encoder is comprised of alternating blocks of equation 2 (Multi-Head Attention) and equation 3 (Multilayer perceptron).</em></p>
<p>See how we’re starting to piece together the overall architecture like legos, coding one brick (or equation) at a time.</p>
<img alt="vision transformer architecture transformer encoder block mapped to code" src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-transformer-encoder-mapped-to-code.png" />
<p><em>Mapping the ViT Transformer Encoder to code.</em></p>
<p>You might’ve noticed that Table 1 from the ViT paper has a Layers column. This refers to the number of Transformer Encoder blocks in the specific ViT architecure.</p>
<p>In our case, for ViT-Base, we’ll be stacking together 12 of these Transformer Encoder blocks to form the backbone of our architecture (we’ll get to this in section 8).</p>
<p>Let’s get a <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code> of passing an input of shape <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">197,</span> <span class="pre">768)</span> <span class="pre">-&gt;</span> <span class="pre">(batch_size,</span> <span class="pre">num_patches,</span> <span class="pre">embedding_dimension)</span></code> to our Transformer Encoder block.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an instance of TransformerEncoderBlock</span>
<span class="n">transformer_encoder_block</span> <span class="o">=</span> <span class="n">TransformerEncoderBlock</span><span class="p">()</span>

<span class="c1"># # Print an input and output summary of our Transformer Encoder (uncomment for full output)</span>
<span class="c1"># summary(model=transformer_encoder_block,</span>
<span class="c1">#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)</span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;])</span>
</pre></div>
</div>
</div>
</div>
<img alt="summary output of a Transformer Encoder layer in PyTorch" src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-summary-output-transformer-encoder.png" />
<p>Woah! Check out all those parameters!</p>
<p>You can see our input changing shape as it moves through all of the various layers in the MSA block and MLP block of the Transformer Encoder block before finally returning to its original shape at the very end.</p>
<blockquote>
<div><p><strong>Note:</strong> Just because our input to the Transformer Encoder block has the same shape at the output of the block doesn’t mean the values weren’t manipulated, the whole goal of the Transformer Encoder block (and stacking them together) is to learn a deep representation of the input using the various layers in between.</p>
</div></blockquote>
</section>
<section id="creating-a-transformer-encoder-with-pytorchs-transformer-layers">
<h3>7.2 Creating a Transformer Encoder with PyTorch’s Transformer layers<a class="headerlink" href="#creating-a-transformer-encoder-with-pytorchs-transformer-layers" title="Permalink to this heading">#</a></h3>
<p>So far we’ve built the components of and the Transformer Encoder layer itself ourselves.</p>
<p>But because of their rise in popularity and effectiveness, PyTorch now has in-built <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#transformer-layers">Transformer layers as part of <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code></a>.</p>
<p>For example, we can recreate the <code class="docutils literal notranslate"><span class="pre">TransformerEncoderBlock</span></code> we just created using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer"><code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoderLayer()</span></code></a> and setting the same hyperparameters as above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the same as above with torch.nn.TransformerEncoderLayer()</span>
<span class="n">torch_transformer_encoder_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># Hidden size D from Table 1 for ViT-Base</span>
                                                             <span class="n">nhead</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Heads from Table 1 for ViT-Base</span>
                                                             <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="c1"># MLP size from Table 1 for ViT-Base</span>
                                                             <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="c1"># Amount of dropout for dense layers from Table 3 for ViT-Base</span>
                                                             <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;gelu&quot;</span><span class="p">,</span> <span class="c1"># GELU non-linear activation</span>
                                                             <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># Do our batches come first?</span>
                                                             <span class="n">norm_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Normalize first or after MSA/MLP layers?</span>

<span class="n">torch_transformer_encoder_layer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TransformerEncoderLayer(
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (linear1): Linear(in_features=768, out_features=3072, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (linear2): Linear(in_features=3072, out_features=768, bias=True)
  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (dropout1): Dropout(p=0.1, inplace=False)
  (dropout2): Dropout(p=0.1, inplace=False)
)
</pre></div>
</div>
</div>
</div>
<p>To inspect it further, let’s get a summary with <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Get the output of PyTorch&#39;s version of the Transformer Encoder (uncomment for full output)</span>
<span class="c1"># summary(model=torch_transformer_encoder_layer,</span>
<span class="c1">#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)</span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;])</span>
</pre></div>
</div>
</div>
</div>
<img alt="output summary of PyTorch's implementation of the Transformer Encoder layer" src="https://github.com/thangckt/pytorch-deep-learning/raw/main/images/08-vit-paper-summary-output-pytorch-transformer-encoder.png" />
<p>The output of the summary is slightly different to ours due to how <code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoderLayer()</span></code> constructs its layer.</p>
<p>But the layers it uses, number of parameters and input and output shapes are the same.</p>
<p>You might be thinking, “if we could create the Transformer Encoder so quickly with PyTorch layers, why did we bother reproducing equation 2 and 3?”</p>
<p>The answer is: practice.</p>
<p>Now we’ve replicated a series of equations and layers from a paper, if you need to change the layers and try something different you can.</p>
<p>But there are benefits of using the PyTorch pre-built layers, such as:</p>
<ul class="simple">
<li><p><strong>Less prone to errors</strong> - Generally, if a layer makes it into the PyTorch standard library, its been tested and tried to work.</p></li>
<li><p><strong>Potentially better performance</strong> - As of July 2022 and PyTorch 1.12, the PyTorch implemented version of <code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoderLayer()</span></code> can see <a class="reference external" href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/">a speedup of more than 2x on many common workloads</a>.</p></li>
</ul>
<p>Finally, since the ViT architecture uses several Transformer Layers stacked on top of each for the full architecture (Table 1 shows 12 Layers in the case of ViT-Base), you can do this with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder"><code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoder(encoder_layer,</span> <span class="pre">num_layers)</span></code></a> where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">encoder_layer</span></code> - The target Transformer Encoder layer created with <code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoderLayer()</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_layers</span></code> - The number of Transformer Encoder layers to stack together.</p></li>
</ul>
</section>
</section>
<section id="putting-it-all-together-to-create-vit">
<h2>8. Putting it all together to create ViT<a class="headerlink" href="#putting-it-all-together-to-create-vit" title="Permalink to this heading">#</a></h2>
<p>Alright, alright, alright, we’ve come a long way!</p>
<p>But now it’s time to do the exciting thing of putting together all of the pieces of the puzzle.</p>
<p>We’re going to combine all of the blocks we’ve created to replicate the full ViT architecture.</p>
<p>From the patch and positional embedding to the Transformer Encoder(s) to the MLP Head.</p>
<p>But wait, we haven’t created equation 4 yet…</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{y} &amp;=\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right) &amp; &amp;
\end{aligned}
\]</div>
<p>Don’t worry, we can put equation 4 into our overall ViT architecture class.</p>
<p>All we need is a <code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code> layer and a <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear()</span></code> layer to convert the 0th index (<span class="math notranslate nohighlight">\(\mathbf{z}_{L}^{0}\)</span>) of the Transformer Encoder logit outputs to the target number of classes we have.</p>
<p>To create the full architecture, we’ll also need to stack a number of our <code class="docutils literal notranslate"><span class="pre">TransformerEncoderBlock</span></code>s on top of each other, we can do this by passing a list of them to <code class="docutils literal notranslate"><span class="pre">torch.nn.Sequential()</span></code> (this will make a sequential range of <code class="docutils literal notranslate"><span class="pre">TransformerEncoderBlock</span></code>s).</p>
<p>We’ll focus on the ViT-Base hyperparameters from Table 1 but our code should be adaptable to other ViT variants.</p>
<p>Creating ViT will be our biggest code block yet but we can do it!</p>
<p>Finally, to bring our own implementation of ViT to life, let’s:</p>
<ol class="arabic simple">
<li><p>Create a class called <code class="docutils literal notranslate"><span class="pre">ViT</span></code> that inherits from <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p></li>
<li><p>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</p></li>
<li><p>Make sure the image size is divisible by the patch size (the image should be split into even patches).</p></li>
<li><p>Calculate the number of patches using the formula <span class="math notranslate nohighlight">\(N=H W / P^{2}\)</span>, where <span class="math notranslate nohighlight">\(H\)</span> is the image height, <span class="math notranslate nohighlight">\(W\)</span> is the image width and <span class="math notranslate nohighlight">\(P\)</span> is the patch size.</p></li>
<li><p>Create a learnable class embedding token (equation 1) as done above in section 4.6.</p></li>
<li><p>Create a learnable position embedding vector (equation 1) as done above in section 4.7.</p></li>
<li><p>Setup the embedding dropout layer as discussed in Appendix B.1 of the ViT paper.</p></li>
<li><p>Create the patch embedding layer using the <code class="docutils literal notranslate"><span class="pre">PatchEmbedding</span></code> class as above in section 4.5.</p></li>
<li><p>Create a series of Transformer Encoder blocks by passing a list of <code class="docutils literal notranslate"><span class="pre">TransformerEncoderBlock</span></code>s created in section 7.1 to <code class="docutils literal notranslate"><span class="pre">torch.nn.Sequential()</span></code> (equations 2 &amp; 3).</p></li>
<li><p>Create the MLP head (also called classifier head or equation 4) by passing a <code class="docutils literal notranslate"><span class="pre">torch.nn.LayerNorm()</span></code> (LN) layer and a <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear(out_features=num_classes)</span></code> layer (where <code class="docutils literal notranslate"><span class="pre">num_classes</span></code> is the target number of classes) linear layer to <code class="docutils literal notranslate"><span class="pre">torch.nn.Sequential()</span></code>.</p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method that accepts an input.</p></li>
<li><p>Get the batch size of the input (the first dimension of the shape).</p></li>
<li><p>Create the patching embedding using the layer created in step 8 (equation 1).</p></li>
<li><p>Create the class token embedding using the layer created in step 5 and expand it across the number of batches found in step 11 using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html"><code class="docutils literal notranslate"><span class="pre">torch.Tensor.expand()</span></code></a> (equation 1).</p></li>
<li><p>Concatenate the class token embedding created in step 13 to the first dimension of the patch embedding created in step 12 using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cat.html"><code class="docutils literal notranslate"><span class="pre">torch.cat()</span></code></a> (equation 1).</p></li>
<li><p>Add the position embedding created in step 6 to the patch and class token embedding created in step 14 (equation 1).</p></li>
<li><p>Pass the patch and position embedding through the dropout layer created in step 7.</p></li>
<li><p>Pass the patch and position embedding from step 16 through the stack of Transformer Encoder layers created in step 9 (equations 2 &amp; 3).</p></li>
<li><p>Pass index 0 of the output of the stack of Transformer Encoder layers from step 17 through the classifier head created in step 10 (equation 4).</p></li>
<li><p>Dance and shout woohoo!!! We just built a Vision Transformer!</p></li>
</ol>
<p>You ready?</p>
<p>Let’s go.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create a ViT class that inherits from nn.Module</span>
<span class="k">class</span> <span class="nc">ViT</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.&quot;&quot;&quot;</span>
    <span class="c1"># 2. Initialize the class with hyperparameters from Table 1 and Table 3</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">img_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="c1"># Training resolution from Table 3 in ViT paper</span>
                 <span class="n">in_channels</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># Number of channels in input image</span>
                 <span class="n">patch_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="c1"># Patch size</span>
                 <span class="n">num_transformer_layers</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Layers from Table 1 for ViT-Base</span>
                 <span class="n">embedding_dim</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="c1"># Hidden size D from Table 1 for ViT-Base</span>
                 <span class="n">mlp_size</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="c1"># MLP size from Table 1 for ViT-Base</span>
                 <span class="n">num_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="c1"># Heads from Table 1 for ViT-Base</span>
                 <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># Dropout for attention projection</span>
                 <span class="n">mlp_dropout</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="c1"># Dropout for dense/MLP layers </span>
                 <span class="n">embedding_dropout</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="c1"># Dropout for patch and position embeddings</span>
                 <span class="n">num_classes</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span> <span class="c1"># Default for ImageNet but can customize this</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span> <span class="c1"># don&#39;t forget the super().__init__()!</span>
        
        <span class="c1"># 3. Make the image size is divisble by the patch size </span>
        <span class="k">assert</span> <span class="n">img_size</span> <span class="o">%</span> <span class="n">patch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Image size must be divisible by patch size, image size: </span><span class="si">{</span><span class="n">img_size</span><span class="si">}</span><span class="s2">, patch size: </span><span class="si">{</span><span class="n">patch_size</span><span class="si">}</span><span class="s2">.&quot;</span>
        
        <span class="c1"># 4. Calculate number of patches (height * width/patch^2)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span> <span class="o">*</span> <span class="n">img_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">patch_size</span><span class="o">**</span><span class="mi">2</span>
                 
        <span class="c1"># 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>
                                            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># 6. Create learnable position embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_patches</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>
                                               <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                
        <span class="c1"># 7. Create embedding dropout value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">embedding_dropout</span><span class="p">)</span>
        
        <span class="c1"># 8. Create patch embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_embedding</span> <span class="o">=</span> <span class="n">PatchEmbedding</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                                              <span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
                                              <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>
        
        <span class="c1"># 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) </span>
        <span class="c1"># Note: The &quot;*&quot; means &quot;all&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">TransformerEncoderBlock</span><span class="p">(</span><span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                                                            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                                            <span class="n">mlp_size</span><span class="o">=</span><span class="n">mlp_size</span><span class="p">,</span>
                                                                            <span class="n">mlp_dropout</span><span class="o">=</span><span class="n">mlp_dropout</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_transformer_layers</span><span class="p">)])</span>
       
        <span class="c1"># 10. Create classifier head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> 
                      <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>
    
    <span class="c1"># 11. Create a forward() method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="c1"># 12. Get batch size</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># 13. Create class token embedding and expand it to match the batch size (equation 1)</span>
        <span class="n">class_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_embedding</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># &quot;-1&quot; means to infer the dimension (try this line on its own)</span>

        <span class="c1"># 14. Create patch embedding (equation 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># 15. Concat class embedding and patch embedding (equation 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">class_token</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 16. Add position embedding to patch embedding (equation 1) </span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span> <span class="o">+</span> <span class="n">x</span>

        <span class="c1"># 17. Run embedding dropout (Appendix B.1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># 19. Put 0 index logit through classifier (equation 4)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># run on each sample in a batch at 0 index</span>

        <span class="k">return</span> <span class="n">x</span>       
</pre></div>
</div>
</div>
</div>
<ol class="arabic simple" start="20">
<li><p>🕺💃🥳 Woohoo!!! We just built a vision transformer!</p></li>
</ol>
<p>What an effort!</p>
<p>Slowly but surely we created layers and blocks, inputs and outputs and put them all together to build our own ViT!</p>
<p>Let’s create a quick demo to showcase what’s happening with the class token embedding being expanded over the batch dimensions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example of creating the class embedding and expanding over a batch dimension</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">class_token_embedding_single</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">768</span><span class="p">))</span> <span class="c1"># create a single learnable class token</span>
<span class="n">class_token_embedding_expanded</span> <span class="o">=</span> <span class="n">class_token_embedding_single</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># expand the single learnable class token across the batch dimension, &quot;-1&quot; means to &quot;infer the dimension&quot;</span>

<span class="c1"># Print out the change in shapes</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of class token embedding single: </span><span class="si">{</span><span class="n">class_token_embedding_single</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape of class token embedding expanded: </span><span class="si">{</span><span class="n">class_token_embedding_expanded</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of class token embedding single: torch.Size([1, 1, 768])
Shape of class token embedding expanded: torch.Size([32, 1, 768])
</pre></div>
</div>
</div>
</div>
<p>Notice how the first dimension gets expanded to the batch size and the other dimensions stay the same (because they’re inferred by the “<code class="docutils literal notranslate"><span class="pre">-1</span></code>” dimensions in <code class="docutils literal notranslate"><span class="pre">.expand(batch_size,</span> <span class="pre">-1,</span> <span class="pre">-1)</span></code>).</p>
<p>Alright time to test out <code class="docutils literal notranslate"><span class="pre">ViT()</span></code> class.</p>
<p>Let’s create a random tensor in the same shape as a single image, pass to an instance of <code class="docutils literal notranslate"><span class="pre">ViT</span></code> and see what happens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_seeds</span><span class="p">()</span>

<span class="c1"># Create a random tensor with same shape as a single image</span>
<span class="n">random_image_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span> <span class="c1"># (batch_size, color_channels, height, width)</span>

<span class="c1"># Create an instance of ViT with the number of classes we&#39;re working with (pizza, steak, sushi)</span>
<span class="n">vit</span> <span class="o">=</span> <span class="n">ViT</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))</span>

<span class="c1"># Pass the random image tensor to our ViT instance</span>
<span class="n">vit</span><span class="p">(</span><span class="n">random_image_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-0.2377,  0.7360,  1.2137]], grad_fn=&lt;AddmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Outstanding!</p>
<p>It looks like our random image tensor made it all the way through our ViT architecture and it’s outputting three logit values (one for each class).</p>
<p>And because our <code class="docutils literal notranslate"><span class="pre">ViT</span></code> class has plenty of parameters we could customize the <code class="docutils literal notranslate"><span class="pre">img_size</span></code>, <code class="docutils literal notranslate"><span class="pre">patch_size</span></code> or <code class="docutils literal notranslate"><span class="pre">num_classes</span></code> if we wanted to.</p>
<section id="getting-a-visual-summary-of-our-vit-model">
<h3>8.1 Getting a visual summary of our ViT model<a class="headerlink" href="#getting-a-visual-summary-of-our-vit-model" title="Permalink to this heading">#</a></h3>
<p>We handcrafted our own version of the ViT architecture and seen that a random image tensor can flow all the way through it.</p>
<p>How about we use <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code> to get a visual overview of the input and output shapes of all the layers in our model?</p>
<blockquote>
<div><p><strong>Note:</strong> The ViT paper states the use of a batch size of 4096 for training, however, this requires a far bit of CPU/GPU compute memory to handle (the larger the batch size the more memory required). So to make sure we don’t get memory errors, we’ll stick with a batch size of 32. You could always increase this later if you have access to hardware with more memory.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="c1"># # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)</span>
<span class="c1"># summary(model=vit, </span>
<span class="c1">#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)</span>
<span class="c1">#         # col_names=[&quot;input_size&quot;], # uncomment for smaller output</span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;]</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
</div>
<img alt="input and output summary of our custom made ViT model" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-summary-output-custom-vit-model.png" />
<p>Now those are some nice looking layers!</p>
<p>Checkout the total number of parameters too, 85,800,963, our biggest model yet!</p>
<p>The number is very close to PyTorch’s pretrained ViT-Base with patch size 16 at <a class="reference external" href="https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16"><code class="docutils literal notranslate"><span class="pre">torch.vision.models.vit_b_16()</span></code></a> with 86,567,656 total parameters (though this number of parameters is for the 1000 classes in ImageNet).</p>
<blockquote>
<div><p><strong>Exercise:</strong> Try changing the <code class="docutils literal notranslate"><span class="pre">num_classes</span></code> parameter of our <code class="docutils literal notranslate"><span class="pre">ViT()</span></code> model to 1000 and then creating another summary with <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code> and see if the number of parameters lines up between our code and <code class="docutils literal notranslate"><span class="pre">torchvision.models.vit_b_16()</span></code>.</p>
</div></blockquote>
</section>
</section>
<section id="setting-up-training-code-for-our-vit-model">
<h2>9. Setting up training code for our ViT model<a class="headerlink" href="#setting-up-training-code-for-our-vit-model" title="Permalink to this heading">#</a></h2>
<p>Ok time for the easy part.</p>
<p>Training!</p>
<p>Why easy?</p>
<p>Because we’ve got most of what we need ready to go, from our model (<code class="docutils literal notranslate"><span class="pre">vit</span></code>) to our DataLoaders (<code class="docutils literal notranslate"><span class="pre">train_dataloader</span></code>, <code class="docutils literal notranslate"><span class="pre">test_dataloader</span></code>) to the training functions we created in <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them">05. PyTorch Going Modular section 4</a>.</p>
<p>To train our model we can import the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function from <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/going_modular/going_modular/train.py"><code class="docutils literal notranslate"><span class="pre">going_modular.going_modular.engine</span></code></a>.</p>
<p>All we need is a loss function and an optimizer.</p>
<section id="creating-an-optimizer">
<h3>9.1 Creating an optimizer<a class="headerlink" href="#creating-an-optimizer" title="Permalink to this heading">#</a></h3>
<p>Searching the ViT paper for “optimizer”, section 4.1 on Training &amp; Fine-tuning states:</p>
<blockquote>
<div><p><strong>Training &amp; Fine-tuning.</strong> We train all models, including ResNets, using Adam (Kingma &amp; Ba, 2015 ) with <span class="math notranslate nohighlight">\(\beta_{1}=0.9, \beta_{2}=0.999\)</span>, a batch size of 4096 and apply a high weight decay of <span class="math notranslate nohighlight">\(0.1\)</span>, which we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common practices, Adam works slightly better than SGD for ResNets in our setting).</p>
</div></blockquote>
<p>So we can see they chose to use the “Adam” optimizer (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam"><code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code></a>) rather than SGD (stochastic gradient descent, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"><code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code></a>).</p>
<p>The authors set Adam’s <span class="math notranslate nohighlight">\(\beta\)</span> (beta) values to <span class="math notranslate nohighlight">\(\beta_{1}=0.9, \beta_{2}=0.999\)</span>, these are the default values for the <code class="docutils literal notranslate"><span class="pre">betas</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam(betas=(0.9,</span> <span class="pre">0.999))</span></code>.</p>
<p>They also state the use of <a class="reference external" href="https://paperswithcode.com/method/weight-decay">weight decay</a> (slowly reducing the values of the weights during optimization to prevent overfitting), we can set this with the <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam(weight_decay=0.1)</span></code>.</p>
<p>We’ll set the learning rate of the optimizer to 0.1 as per Table 3.</p>
<p>And as discussed previously, we’re going to use a lower batch size than 4096 due to hardware limitations (if you have a large GPU, feel free to increase this).</p>
</section>
<section id="creating-a-loss-function">
<h3>9.2 Creating a loss function<a class="headerlink" href="#creating-a-loss-function" title="Permalink to this heading">#</a></h3>
<p>Strangely, searching the ViT paper for “loss” or “loss function” or “criterion” returns no results.</p>
<p>However, since the target problem we’re working with is multi-class classification (the same for the ViT paper), we’ll use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss()</span></code></a>.</p>
</section>
<section id="training-our-vit-model">
<h3>9.3 Training our ViT model<a class="headerlink" href="#training-our-vit-model" title="Permalink to this heading">#</a></h3>
<p>Okay, now we know what optimizer and loss function we’re going to use, let’s setup the training code for training our ViT.</p>
<p>We’ll start by importing the <code class="docutils literal notranslate"><span class="pre">engine.py</span></code> script from <code class="docutils literal notranslate"><span class="pre">going_modular.going_modular</span></code> then we’ll setup the optimizer and loss function and finally we’ll use the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function from <code class="docutils literal notranslate"><span class="pre">engine.py</span></code> to train our ViT model for 10 epochs (we’re using a smaller number of epochs than the ViT paper to make sure everything works).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">going_modular.going_modular</span> <span class="kn">import</span> <span class="n">engine</span>

<span class="c1"># Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper </span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">vit</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
                             <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="c1"># Base LR from Table 3 </span>
                             <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="c1"># default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)</span>
                             <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># from the ViT paper section 4.1 (Training &amp; Fine-tuning)</span>

<span class="c1"># Setup the loss function for multi-class classification</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Set the seeds</span>
<span class="n">set_seeds</span><span class="p">()</span>

<span class="c1"># Train the model and save the training results to a dictionary</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">vit</span><span class="p">,</span>
                       <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
                       <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
                       <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                       <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                       <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                       <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "97484323a38248e98ded3df3e074655c", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 4.8759 | train_acc: 0.2891 | test_loss: 1.0465 | test_acc: 0.5417
Epoch: 2 | train_loss: 1.5900 | train_acc: 0.2617 | test_loss: 1.5876 | test_acc: 0.1979
Epoch: 3 | train_loss: 1.4644 | train_acc: 0.2617 | test_loss: 1.2738 | test_acc: 0.1979
Epoch: 4 | train_loss: 1.3159 | train_acc: 0.2773 | test_loss: 1.7498 | test_acc: 0.1979
Epoch: 5 | train_loss: 1.3114 | train_acc: 0.3008 | test_loss: 1.7444 | test_acc: 0.2604
Epoch: 6 | train_loss: 1.2445 | train_acc: 0.3008 | test_loss: 1.9704 | test_acc: 0.1979
Epoch: 7 | train_loss: 1.2050 | train_acc: 0.3984 | test_loss: 3.5480 | test_acc: 0.1979
Epoch: 8 | train_loss: 1.4368 | train_acc: 0.4258 | test_loss: 1.8324 | test_acc: 0.2604
Epoch: 9 | train_loss: 1.5757 | train_acc: 0.2344 | test_loss: 1.2848 | test_acc: 0.5417
Epoch: 10 | train_loss: 1.4658 | train_acc: 0.4023 | test_loss: 1.2389 | test_acc: 0.2604
</pre></div>
</div>
</div>
</div>
<p>Wonderful!</p>
<p>Our ViT model has come to life!</p>
<p>Though the results on our pizza, steak and sushi dataset don’t look too good.</p>
<p>Perhaps it’s because we’re missing a few things?</p>
</section>
<section id="what-our-training-setup-is-missing">
<h3>9.4 What our training setup is missing<a class="headerlink" href="#what-our-training-setup-is-missing" title="Permalink to this heading">#</a></h3>
<p>The original ViT architecture achieves good results on several image classification benchmarks (on par or better than many state-of-the-art results when it was released).</p>
<p>However, our results (so far) aren’t as good.</p>
<p>There’s a few reasons this could be but the main one is scale.</p>
<p>The original ViT paper uses a far larger amount of data than ours (in deep learning, more data is generally always a good thing) and a longer training schedule (see Table 3).</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter value</strong></p></th>
<th class="head"><p><strong>ViT Paper</strong></p></th>
<th class="head"><p><strong>Our implementation</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Number of training images</p></td>
<td><p>1.3M (ImageNet-1k), 14M (ImageNet-21k), 303M (JFT)</p></td>
<td><p>225</p></td>
</tr>
<tr class="row-odd"><td><p>Epochs</p></td>
<td><p>7 (for largest dataset), 90, 300 (for ImageNet)</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>Batch size</p></td>
<td><p>4096</p></td>
<td><p>32</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://paperswithcode.com/method/linear-warmup">Learning rate warmup</a></p></td>
<td><p>10k steps (Table 3)</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b#:~:text=Learning%20rate%20decay%20is%20a,help%20both%20optimization%20and%20generalization.">Learning rate decay</a></p></td>
<td><p>Linear/Cosine (Table 3)</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://paperswithcode.com/method/gradient-clipping">Gradient clipping</a></p></td>
<td><p>Global norm 1 (Table 3)</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>
</div>
<p>Even though our ViT architecture is the same as the paper, the results from the ViT paper were achieved using far more data and a more elaborate training scheme than ours.</p>
<p>Because of the size of the ViT architecture and its high number of parameters (increased learning capabilities), and amount of data it uses (increased learning opportunities), many of the techniques used in the ViT paper training scheme such as learning rate warmup, learning rate decay and gradient clipping are specifically designed to <a class="reference external" href="https://www.learnpytorch.io/04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting">prevent overfitting</a> (regularization).</p>
<blockquote>
<div><p><strong>Note:</strong> For any technique you’re unsure of, you can often quickly find an example by searching “pytorch TECHNIQUE NAME”, for exmaple, say you wanted to learn about learning rate warmup and what it does, you could search “pytorch learning rate warmup”.</p>
</div></blockquote>
<p>Good news is, there are many pretrained ViT models (using vast amounts of data) available online, we’ll see one in action in section 10.</p>
</section>
<section id="plot-the-loss-curves-of-our-vit-model">
<h3>9.5 Plot the loss curves of our ViT model<a class="headerlink" href="#plot-the-loss-curves-of-our-vit-model" title="Permalink to this heading">#</a></h3>
<p>We’ve trained our ViT model and seen the results as numbers on a page.</p>
<p>But let’s now follow the data explorer’s motto of <em>visualize, visualize, visualize!</em></p>
<p>And one of the best things to visualize for a model is its loss curves.</p>
<p>To check out our ViT model’s loss curves, we can use the <code class="docutils literal notranslate"><span class="pre">plot_loss_curves</span></code> function from <code class="docutils literal notranslate"><span class="pre">helper_functions.py</span></code> we created in <a class="reference external" href="https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0">04. PyTorch Custom Datasets section 7.8</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">plot_loss_curves</span>

<span class="c1"># Plot our ViT model&#39;s loss curves</span>
<span class="n">plot_loss_curves</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/27b0d92773d1175abab1a06482dfa8538fcfe145402bf45598d5d825741fa2bf.png" src="../../_images/27b0d92773d1175abab1a06482dfa8538fcfe145402bf45598d5d825741fa2bf.png" />
</div>
</div>
<p>Hmm, it looks like our model’s loss curves are all over the place.</p>
<p>At least the loss looks like it’s heading the right direction but the accuracy curves don’t really show much promise.</p>
<p>These results are likely because of the difference in data resources and training regime of our ViT model versus the ViT paper.</p>
<p>It seems our model is <a class="reference internal" href="#learnpytorch.io/04_pytorch_custom_datasets/#82-how-to-deal-with-underfitting"><span class="xref myst">severly underfitting</span></a> (not achieving the results we’d like it to).</p>
<p>How about we see if we can fix that by bringing in a pretrained ViT model?</p>
</section>
</section>
<section id="using-a-pretrained-vit-from-torchvision-models-on-the-same-dataset">
<h2>10. Using a pretrained ViT from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> on the same dataset<a class="headerlink" href="#using-a-pretrained-vit-from-torchvision-models-on-the-same-dataset" title="Permalink to this heading">#</a></h2>
<p>We’ve discussed the benefits of using pretrained models in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/">06. PyTorch Transfer Learning</a>.</p>
<p>But since we’ve now trained our own ViT from scratch and achieved less than optimal results, the benefits of transfer learning (using a pretrained model) really shine.</p>
<section id="why-use-a-pretrained-model">
<h3>10.1 Why use a pretrained model?<a class="headerlink" href="#why-use-a-pretrained-model" title="Permalink to this heading">#</a></h3>
<p>An important note on many modern machine learning research papers is that much of the results are obtained with large datasets and vast compute resources.</p>
<p>And in modern day machine learning, the original fully trained ViT would likely not be considered a “super large” training setup (models are continually getting bigger and bigger).</p>
<p>Reading the ViT paper section 4.2:</p>
<blockquote>
<div><p>Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately <strong>30 days</strong>.</p>
</div></blockquote>
<p>As of July 2022, the <a class="reference external" href="https://cloud.google.com/tpu/pricing">price for renting a TPUv3</a> (Tensor Processing Unit version 3) with 8 cores on Google Cloud is $8 USD per hour.</p>
<p>To rent one for 30 straight days would cost <strong>$5,760 USD</strong>.</p>
<p>This cost (monetary and time) may be viable for some larger research teams or enterprises but for many people it’s not.</p>
<p>So having a pretrained model available through resources like <a class="reference external" href="https://pytorch.org/vision/stable/models.html"><code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code></a>, the <a class="reference external" href="https://github.com/rwightman/pytorch-image-models"><code class="docutils literal notranslate"><span class="pre">timm</span></code> (Torch Image Models) library</a>, the <a class="reference external" href="https://huggingface.co/models">HuggingFace Hub</a> or even from the authors of the papers themselves (there’s a growing trend for machine learning researchers to release the code and pretrained models from their research papers, I’m a big fan of this trend, many of these resources can be found on <a class="reference external" href="https://paperswithcode.com/">Paperswithcode.com</a>).</p>
<p>If you’re focused on leveraging the benefits of a specific model architecture rather than creating your custom architecture, I’d highly recommend using a pretrained model.</p>
</section>
<section id="getting-a-pretrained-vit-model-and-creating-a-feature-extractor">
<h3>10.2 Getting a pretrained ViT model and creating a feature extractor<a class="headerlink" href="#getting-a-pretrained-vit-model-and-creating-a-feature-extractor" title="Permalink to this heading">#</a></h3>
<p>We can get a pretrained ViT model from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code>.</p>
<p>We’ll go from the top by first making sure we’ve got the right versions of <code class="docutils literal notranslate"><span class="pre">torch</span></code> and <code class="docutils literal notranslate"><span class="pre">torchvision</span></code>.</p>
<blockquote>
<div><p><strong>Note:</strong> The following code requires <code class="docutils literal notranslate"><span class="pre">torch</span></code> v0.12+ and <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> v0.13+ to use the latest <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> model weights API.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following requires torch v0.12+ and torchvision v0.13+</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.12.0+cu102
0.13.0+cu102
</pre></div>
</div>
</div>
</div>
<p>Then we’ll setup device-agonistc code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
<p>Finally, we’ll get the pretrained ViT-Base with patch size 16 from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> and prepare it for our FoodVision Mini use case by turning it into a feature extractor transfer learning model.</p>
<p>Specifically, we’ll:</p>
<ol class="arabic simple">
<li><p>Get the pretrained weights for ViT-Base trained on ImageNet-1k from <a class="reference external" href="https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights"><code class="docutils literal notranslate"><span class="pre">torchvision.models.ViT_B_16_Weights.DEFAULT</span></code></a> (<code class="docutils literal notranslate"><span class="pre">DEFAULT</span></code> stands for best available).</p></li>
<li><p>Setup a ViT model instance via <code class="docutils literal notranslate"><span class="pre">torchvision.models.vit_b_16</span></code>, pass it the pretrained weights step 1 and send it to the target device.</p></li>
<li><p>Freeze all of the parameters in the base ViT model created in step 2 by setting their <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> attribute to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>Update the classifier head of the ViT model created in step 2 to suit our own problem by changing the number of <code class="docutils literal notranslate"><span class="pre">out_features</span></code> to our number of classes (pizza, steak, sushi).</p></li>
</ol>
<p>We covered steps like this in 06. PyTorch Transfer Learning <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#32-setting-up-a-pretrained-model">section 3.2: Setting up a pretrained model</a> and <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs">section 3.4: Freezing the base model and changing the output layer to suit our needs</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Get pretrained weights for ViT-Base</span>
<span class="n">pretrained_vit_weights</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ViT_B_16_Weights</span><span class="o">.</span><span class="n">DEFAULT</span> <span class="c1"># requires torchvision &gt;= 0.13, &quot;DEFAULT&quot; means best available</span>

<span class="c1"># 2. Setup a ViT model instance with pretrained weights</span>
<span class="n">pretrained_vit</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">vit_b_16</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">pretrained_vit_weights</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 3. Freeze the base parameters</span>
<span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">pretrained_vit</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">parameter</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    
<span class="c1"># 4. Change the classifier head (set the seeds to ensure same initialization with linear head)</span>
<span class="n">set_seeds</span><span class="p">()</span>
<span class="n">pretrained_vit</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># pretrained_vit # uncomment for model output </span>
</pre></div>
</div>
</div>
</div>
<p>Pretrained ViT feature extractor model created!</p>
<p>Let’s now check it out by printing a <code class="docutils literal notranslate"><span class="pre">torchinfo.summary()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Print a summary using torchinfo (uncomment for actual output)</span>
<span class="c1"># summary(model=pretrained_vit, </span>
<span class="c1">#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)</span>
<span class="c1">#         # col_names=[&quot;input_size&quot;], # uncomment for smaller output</span>
<span class="c1">#         col_names=[&quot;input_size&quot;, &quot;output_size&quot;, &quot;num_params&quot;, &quot;trainable&quot;],</span>
<span class="c1">#         col_width=20,</span>
<span class="c1">#         row_settings=[&quot;var_names&quot;]</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
</div>
<img alt="output of pytorch pretrained ViT model summary" src="https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/08-vit-paper-summary-output-pytorch-vit.png" />
<p>Woohoo!</p>
<p>Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen).</p>
<p>And the total number of parameters, 85,800,963, is the same as our custom made ViT model above.</p>
<p>But the number of trainable parameters for <code class="docutils literal notranslate"><span class="pre">pretrained_vit</span></code> is much, much lower than our custom <code class="docutils literal notranslate"><span class="pre">vit</span></code> at only 2,307 compared to 85,800,963 (in our custom <code class="docutils literal notranslate"><span class="pre">vit</span></code>, since we’re training from scratch, all parameters are trainable).</p>
<p>This means the pretrained model should train a lot faster, we could potentially even use a larger batch size since less parameter updates are going to be taking up memory.</p>
</section>
<section id="preparing-data-for-the-pretrained-vit-model">
<h3>10.3 Preparing data for the pretrained ViT model<a class="headerlink" href="#preparing-data-for-the-pretrained-vit-model" title="Permalink to this heading">#</a></h3>
<p>We downloaded and created DataLoaders for our own ViT model back in section 2.</p>
<p>So we don’t necessarily need to do it again.</p>
<p>But in the name of practice, let’s download some image data (pizza, steak and sushi images for Food Vision Mini), setup train and test directories and then transform the images into tensors and DataLoaders.</p>
<p>We can download pizza, steak and sushi images from the course GitHub and the <code class="docutils literal notranslate"><span class="pre">download_data()</span></code> function we creating in <a class="reference external" href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data">07. PyTorch Experiment Tracking section 1</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">download_data</span>

<span class="c1"># Download pizza, steak, sushi images from GitHub</span>
<span class="n">image_path</span> <span class="o">=</span> <span class="n">download_data</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s2">&quot;https://github.com/thangckt/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip&quot;</span><span class="p">,</span>
                           <span class="n">destination</span><span class="o">=</span><span class="s2">&quot;pizza_steak_sushi&quot;</span><span class="p">)</span>
<span class="n">image_path</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] data/pizza_steak_sushi directory exists, skipping download.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PosixPath(&#39;data/pizza_steak_sushi&#39;)
</pre></div>
</div>
</div>
</div>
<p>And now we’ll setup the training and test directory paths.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup train and test directory paths</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;train&quot;</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;test&quot;</span> 
<span class="n">train_dir</span><span class="p">,</span> <span class="n">test_dir</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(PosixPath(&#39;data/pizza_steak_sushi/train&#39;),
 PosixPath(&#39;data/pizza_steak_sushi/test&#39;))
</pre></div>
</div>
</div>
</div>
<p>Finally, we’ll transform our images into tensors and turn the tensors into DataLoaders.</p>
<p>Since we’re using a pretrained model form <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> we can call the <code class="docutils literal notranslate"><span class="pre">transforms()</span></code> method on it to get its required transforms.</p>
<p>Remember, if you’re going to use a pretrained model, it’s generally important to <strong>ensure your own custom data is transformed/formatted in the same way the data the original model was trained on</strong>.</p>
<p>We covered this method of “automatic” transform creation in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#22-creating-a-transform-for-torchvisionmodels-auto-creation">06. PyTorch Transfer Learning section 2.2</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get automatic transforms from pretrained ViT weights</span>
<span class="n">pretrained_vit_transforms</span> <span class="o">=</span> <span class="n">pretrained_vit_weights</span><span class="o">.</span><span class="n">transforms</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pretrained_vit_transforms</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ImageClassification(
    crop_size=[224]
    resize_size=[256]
    mean=[0.485, 0.456, 0.406]
    std=[0.229, 0.224, 0.225]
    interpolation=InterpolationMode.BILINEAR
)
</pre></div>
</div>
</div>
</div>
<p>And now we’ve got transforms ready, we can turn our images into DataLoaders using the <code class="docutils literal notranslate"><span class="pre">data_setup.create_dataloaders()</span></code> method we created in <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy">05. PyTorch Going Modular section 2</a>.</p>
<p>Since we’re using a feature extractor model (less trainable parameters), we could increase the batch size to a higher value (if we set it to 1024, we’d be mimicing an improvement found in <a class="reference external" href="https://arxiv.org/abs/2205.01580"><em>Better plain ViT baselines for ImageNet-1k</em></a>, a paper which improves upon the original ViT paper and suggested extra reading). But since we only have ~200 training samples total, we’ll stick with 32.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup dataloaders</span>
<span class="n">train_dataloader_pretrained</span><span class="p">,</span> <span class="n">test_dataloader_pretrained</span><span class="p">,</span> <span class="n">class_names</span> <span class="o">=</span> <span class="n">data_setup</span><span class="o">.</span><span class="n">create_dataloaders</span><span class="p">(</span><span class="n">train_dir</span><span class="o">=</span><span class="n">train_dir</span><span class="p">,</span>
                                                                                                     <span class="n">test_dir</span><span class="o">=</span><span class="n">test_dir</span><span class="p">,</span>
                                                                                                     <span class="n">transform</span><span class="o">=</span><span class="n">pretrained_vit_transforms</span><span class="p">,</span>
                                                                                                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span> <span class="c1"># Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-feature-extractor-vit-model">
<h3>10.4 Train feature extractor ViT model<a class="headerlink" href="#train-feature-extractor-vit-model" title="Permalink to this heading">#</a></h3>
<p>Feature extractor model ready, DataLoaders ready, time to train!</p>
<p>As before we’ll use the Adam optimizer (<code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code>) with a learning rate of <code class="docutils literal notranslate"><span class="pre">1e-3</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss()</span></code> as the loss function.</p>
<p>Our <code class="docutils literal notranslate"><span class="pre">engine.train()</span></code> function we created in <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them">05. PyTorch Going Modular section 4</a> will take care of the rest.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">going_modular.going_modular</span> <span class="kn">import</span> <span class="n">engine</span>

<span class="c1"># Create optimizer and loss function</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">pretrained_vit</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
                             <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Train the classifier head of the pretrained ViT feature extractor model</span>
<span class="n">set_seeds</span><span class="p">()</span>
<span class="n">pretrained_vit_results</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">pretrained_vit</span><span class="p">,</span>
                                      <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_dataloader_pretrained</span><span class="p">,</span>
                                      <span class="n">test_dataloader</span><span class="o">=</span><span class="n">test_dataloader_pretrained</span><span class="p">,</span>
                                      <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                      <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                                      <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                      <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e47702187773418aafc32e0078ff1895", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1 | train_loss: 0.7665 | train_acc: 0.7227 | test_loss: 0.5432 | test_acc: 0.8665
Epoch: 2 | train_loss: 0.3428 | train_acc: 0.9453 | test_loss: 0.3263 | test_acc: 0.8977
Epoch: 3 | train_loss: 0.2064 | train_acc: 0.9531 | test_loss: 0.2707 | test_acc: 0.9081
Epoch: 4 | train_loss: 0.1556 | train_acc: 0.9570 | test_loss: 0.2422 | test_acc: 0.9081
Epoch: 5 | train_loss: 0.1246 | train_acc: 0.9727 | test_loss: 0.2279 | test_acc: 0.8977
Epoch: 6 | train_loss: 0.1216 | train_acc: 0.9766 | test_loss: 0.2129 | test_acc: 0.9280
Epoch: 7 | train_loss: 0.0938 | train_acc: 0.9766 | test_loss: 0.2352 | test_acc: 0.8883
Epoch: 8 | train_loss: 0.0797 | train_acc: 0.9844 | test_loss: 0.2281 | test_acc: 0.8778
Epoch: 9 | train_loss: 0.1098 | train_acc: 0.9883 | test_loss: 0.2074 | test_acc: 0.9384
Epoch: 10 | train_loss: 0.0650 | train_acc: 0.9883 | test_loss: 0.1804 | test_acc: 0.9176
</pre></div>
</div>
</div>
</div>
<p>Holy cow!</p>
<p>Looks like our pretrained ViT feature extractor performed far better than our custom ViT model trained from scratch (in the same amount of time).</p>
<p>Let’s get visual.</p>
</section>
<section id="plot-feature-extractor-vit-model-loss-curves">
<h3>10.5 Plot feature extractor ViT model loss curves<a class="headerlink" href="#plot-feature-extractor-vit-model-loss-curves" title="Permalink to this heading">#</a></h3>
<p>Our pretrained ViT feature model numbers look good on the training and test sets.</p>
<p>How do the loss curves look?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the loss curves</span>
<span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">plot_loss_curves</span>

<span class="n">plot_loss_curves</span><span class="p">(</span><span class="n">pretrained_vit_results</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/62c702b498da8a73114bbf458510a1522e78820d0150c408077809526f2f2b76.png" src="../../_images/62c702b498da8a73114bbf458510a1522e78820d0150c408077809526f2f2b76.png" />
</div>
</div>
<p>Woah!</p>
<p>Those are some close to textbook looking (really good) loss curves (check out <a class="reference external" href="https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like">04. PyTorch Custom Datasets section 8</a> for what an ideal loss curve should look like).</p>
<p>That’s the power of transfer learning!</p>
<p>We managed to get outstanding results with the <em>same</em> model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it.</p>
<p>What do you think?</p>
<p>Would our feature extractor model improve more if you kept training it?</p>
</section>
<section id="save-feature-extractor-vit-model-and-check-file-size">
<h3>10.6 Save feature extractor ViT model and check file size<a class="headerlink" href="#save-feature-extractor-vit-model-and-check-file-size" title="Permalink to this heading">#</a></h3>
<p>It looks like our ViT feature extractor model is performing quite well for our Food Vision Mini problem.</p>
<p>Perhaps we might want to try deploying it and see how it goes in production (in this case, deploying means putting our trained model in an application someone could use, say taking photos on their smartphone of food and seeing if our model thinks its pizza, steak or sushi).</p>
<p>To do so we can first save our model with the <code class="docutils literal notranslate"><span class="pre">utils.save_model()</span></code> function we created in <a class="reference external" href="https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy">05. PyTorch Going Modular section 5</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the model</span>
<span class="kn">from</span> <span class="nn">going_modular.going_modular</span> <span class="kn">import</span> <span class="n">utils</span>

<span class="n">utils</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">pretrained_vit</span><span class="p">,</span>
                 <span class="n">target_dir</span><span class="o">=</span><span class="s2">&quot;models&quot;</span><span class="p">,</span>
                 <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] Saving model to: models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth
</pre></div>
</div>
</div>
</div>
<p>And since we’re thinking about deploying this model, it’d be good to know the size of it (in megabytes or MB).</p>
<p>Since we want our Food Vision Mini application to run fast, generally a smaller model with good performance will be better than a larger model with great performance.</p>
<p>We can check the size of our model in bytes using the <code class="docutils literal notranslate"><span class="pre">st_size</span></code> attribute of Python’s <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat"><code class="docutils literal notranslate"><span class="pre">pathlib.Path().stat()</span></code></a> method whilst passing it our model’s filepath name.</p>
<p>We can then scale the size in bytes to megabytes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># Get the model size in bytes then convert to megabytes</span>
<span class="n">pretrained_vit_model_size</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">stat</span><span class="p">()</span><span class="o">.</span><span class="n">st_size</span> <span class="o">//</span> <span class="p">(</span><span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="p">)</span> <span class="c1"># division converts bytes to megabytes (roughly) </span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pretrained ViT feature extractor model size: </span><span class="si">{</span><span class="n">pretrained_vit_model_size</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pretrained ViT feature extractor model size: 327 MB
</pre></div>
</div>
</div>
</div>
<p>Hmm, looks like our ViT feature extractor model for Food Vision Mini turned out to be about 327 MB in size.</p>
<p>How does this compare to the EffNetB2 feature extractor model in <a class="reference external" href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it">07. PyTorch Experiment Tracking section 9</a>?</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Model</strong></p></th>
<th class="head"><p><strong>Model size (MB)</strong></p></th>
<th class="head"><p><strong>Test loss</strong></p></th>
<th class="head"><p><strong>Test accuracy</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>EffNetB2 feature extractor^</p></td>
<td><p>29</p></td>
<td><p>~0.3906</p></td>
<td><p>~0.9384</p></td>
</tr>
<tr class="row-odd"><td><p>ViT feature extractor</p></td>
<td><p>327</p></td>
<td><p>~0.1084</p></td>
<td><p>~0.9384</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><p><strong>Note:</strong> ^ the EffNetB2 model in reference was trained with 20% of pizza, steak and sushi data (double the amount of images) rather than the ViT feature extractor which was trained with 10% of pizza, steak and sushi data. An exercise would be to train the ViT feature extractor model on the same amount of data and see how much the results improve.</p>
</div></blockquote>
<p>The EffNetB2 model is ~11x smaller than the ViT model with similiar results for test loss and accuracy.</p>
<p>However, the ViT model’s results may improve more when trained with the same data (20% pizza, steak and sushi data).</p>
<p>But in terms of deployment, if we were comparing these two models, something we’d need to consider is whether the extra accuracy from the ViT model is worth the ~11x increase in model size?</p>
<p>Perhaps such a large model would take longer to load/run and wouldn’t provide as good an experience as EffNetB2 which performs similarly but at a much reduced size.</p>
</section>
</section>
<section id="make-predictions-on-a-custom-image">
<h2>11. Make predictions on a custom image<a class="headerlink" href="#make-predictions-on-a-custom-image" title="Permalink to this heading">#</a></h2>
<p>And finally, we’ll finish with the ultimate test, predicting on our own custom data.</p>
<p>Let’s download the pizza dad image (a photo of my dad eating pizza) and use our ViT feature extractor to predict on it.</p>
<p>To do we, let’s can use the <code class="docutils literal notranslate"><span class="pre">pred_and_plot()</span></code> function we created in <a class="reference external" href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set">06. PyTorch Transfer Learning section 6</a>, for convenience, I saved this function to <a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py"><code class="docutils literal notranslate"><span class="pre">going_modular.going_modular.predictions.py</span></code></a> on the course GitHub.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>

<span class="c1"># Import function to make predictions on images and plot them </span>
<span class="kn">from</span> <span class="nn">going_modular.going_modular.predictions</span> <span class="kn">import</span> <span class="n">pred_and_plot_image</span>

<span class="c1"># Setup custom image path</span>
<span class="n">custom_image_path</span> <span class="o">=</span> <span class="n">image_path</span> <span class="o">/</span> <span class="s2">&quot;04-pizza-dad.jpeg&quot;</span>

<span class="c1"># Download the image if it doesn&#39;t already exist</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">custom_image_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">custom_image_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="c1"># When downloading from GitHub, need to use the &quot;raw&quot; file link</span>
        <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/thangckt/pytorch-deep-learning/main/images/04-pizza-dad.jpeg&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Downloading </span><span class="si">{</span><span class="n">custom_image_path</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">custom_image_path</span><span class="si">}</span><span class="s2"> already exists, skipping download.&quot;</span><span class="p">)</span>

<span class="c1"># Predict on custom image</span>
<span class="n">pred_and_plot_image</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">pretrained_vit</span><span class="p">,</span>
                    <span class="n">image_path</span><span class="o">=</span><span class="n">custom_image_path</span><span class="p">,</span>
                    <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>data/pizza_steak_sushi/04-pizza-dad.jpeg already exists, skipping download.
</pre></div>
</div>
<img alt="../../_images/e2f5c2ef70ab7bbabdd578d09938736c27c5fe37a7129422590f3c29eedfdc43.png" src="../../_images/e2f5c2ef70ab7bbabdd578d09938736c27c5fe37a7129422590f3c29eedfdc43.png" />
</div>
</div>
<p>Two thumbs up!</p>
<p>Congratulations!</p>
<p>We’ve gone all the way from research paper to usable model code on our own custom images!</p>
</section>
<section id="main-takeaways">
<h2>Main takeaways<a class="headerlink" href="#main-takeaways" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>With the explosion of machine learning, new research papers detailing advancements come out every day. And it’s impossible to keep up with it <em>all</em> but you can narrow things down to your own use case, such as what we did here, replicating a computer vision paper for FoodVision Mini.</p></li>
<li><p>Machine learning research papers are often contain months of research by teams of smart people compressed into a few pages (so teasing out all the details and replicating the paper in full can be a bit of challenge).</p></li>
<li><p>The goal of paper replicating is to turn machine learning research papers (text and math) into usable code.</p>
<ul>
<li><p>With this being said, many machine learning research teams are starting to publish code with their papers and one of the best places to see this is at <a class="reference external" href="https://paperswithcode.com/">Paperswithcode.com</a></p></li>
</ul>
</li>
<li><p>Breaking a machine learning research paper into inputs and outputs (what goes in and out of each layer/block/model?) and layers (how does each layer manipulate the input?) and blocks (a collection of layers) and replicating each part step by step (like we’ve done in this notebook) can be very helpful for understanding.</p></li>
<li><p>Pretrained models are available for many state of the art model architectures and with the power of transfer learning, these often perform <em>very</em> well with little data.</p></li>
<li><p>Larger models generally perform better but have a larger footprint too (they take up more storage space and can take longer to perform inference).</p>
<ul>
<li><p>A big question is: deployment wise, is the extra performance of a larger model worth it/aligned with the use case?</p></li>
</ul>
</li>
</ul>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p><strong>Note:</strong> These exercises expect the use of <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> v0.13+ (released July 2022), previous versions may work but will likely have errors.</p>
</div></blockquote>
<p>All of the exercises are focused on practicing the code above.</p>
<p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p>
<p>All exercises should be completed using <a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">device-agnostic code</a>.</p>
<p><strong>Resources:</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/exercises/08_pytorch_paper_replicating_exercises.ipynb">Exercise template notebook for 08</a>.</p></li>
<li><p><a class="reference external" href="https://github.com/thangckt/pytorch-deep-learning/blob/main/extras/solutions/08_pytorch_paper_replicating_exercise_solutions.ipynb">Example solutions notebook for 08</a> (try the exercises <em>before</em> looking at this).</p>
<ul>
<li><p>See a live <a class="reference external" href="https://youtu.be/tjpW_BY8y3g">video walkthrough of the solutions on YouTube</a> (errors and all).</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple">
<li><p>Replicate the ViT architecture we created with in-built <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#transformer-layers">PyTorch transformer layers</a>.</p>
<ul class="simple">
<li><p>You’ll want to look into replacing our <code class="docutils literal notranslate"><span class="pre">TransformerEncoderBlock()</span></code> class with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer"><code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoderLayer()</span></code></a> (these contain the same layers as our custom blocks).</p></li>
<li><p>You can stack <code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoderLayer()</span></code>’s on top of each other with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder"><code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoder()</span></code></a>.</p></li>
</ul>
</li>
<li><p>Turn the custom ViT architecture we created into a Python script, for example, <code class="docutils literal notranslate"><span class="pre">vit.py</span></code>.</p>
<ul class="simple">
<li><p>You should be able to import an entire ViT model using something like<code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">vit</span> <span class="pre">import</span> <span class="pre">ViT</span></code>.</p></li>
</ul>
</li>
<li><p>Train a pretrained ViT feature extractor model (like the one we made in <a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-bring-in-pretrained-vit-from-torchvisionmodels-on-same-dataset">08. PyTorch Paper Replicating section 10</a>) on 20% of the pizza, steak and sushi data like the dataset we used in <a class="reference external" href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#73-download-different-datasets">07. PyTorch Experiment Tracking section 7.3</a>.</p>
<ul class="simple">
<li><p>See how it performs compared to the EffNetB2 model we compared it to in <a class="reference external" href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size">08. PyTorch Paper Replicating section 10.6</a>.</p></li>
</ul>
</li>
<li><p>Try repeating the steps from excercise 3 but this time use the “<code class="docutils literal notranslate"><span class="pre">ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</span></code>” pretrained weights from <a class="reference external" href="https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16"><code class="docutils literal notranslate"><span class="pre">torchvision.models.vit_b_16()</span></code></a>.</p>
<ul class="simple">
<li><p><strong>Note:</strong> ViT pretrained with SWAG weights has a minimum input image size of <code class="docutils literal notranslate"><span class="pre">(384,</span> <span class="pre">384)</span></code> (the pretrained ViT in exercise 3 has a minimum input size of <code class="docutils literal notranslate"><span class="pre">(224,</span> <span class="pre">224)</span></code>), though this is accessible in the weights <code class="docutils literal notranslate"><span class="pre">.transforms()</span></code> method.</p></li>
</ul>
</li>
<li><p>Our custom ViT model architecture closely mimics that of the ViT paper, however, our training recipe misses a few things. Research some of the following topics from Table 3 in the ViT paper that we miss and write a sentence about each and how it might help with training:</p>
<ul class="simple">
<li><p>ImageNet-22k pretraining (more data).</p></li>
<li><p>Learning rate warmup.</p></li>
<li><p>Learning rate decay.</p></li>
<li><p>Gradient clipping.</p></li>
</ul>
</li>
</ol>
</section>
<section id="extra-curriculum">
<h2>Extra-curriculum<a class="headerlink" href="#extra-curriculum" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>There have been several iterations and tweaks to the Vision Transformer since its original release and the most concise and best performing (as of July 2022) can be viewed in <a class="reference external" href="https://arxiv.org/abs/2205.01580"><em>Better plain ViT baselines for ImageNet-1k</em></a>. Depsite of the upgrades, we stuck with replicating a “vanilla Vision Transformer” in this notebook because if you understand the structure of the original, you can bridge to different iterations.</p></li>
<li><p>The <a class="reference external" href="https://github.com/lucidrains/vit-pytorch"><code class="docutils literal notranslate"><span class="pre">vit-pytorch</span></code> repository on GitHub by lucidrains</a> is one of the most extensive resources of different ViT architectures implemented in PyTorch. It’s a phenomenal reference and one I used often to create the materials we’ve been through in this chapter.</p></li>
<li><p>PyTorch have their <a class="reference external" href="https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py">own implementation of the ViT architecture on GitHub</a>, it’s used as the basis of the pretrained ViT models in <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code>.</p></li>
<li><p>Jay Alammar has fantastic illustrations and explanations on his blog of the <a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">attention mechanism</a> (the foundation of Transformer models) and <a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Transformer models</a>.</p></li>
<li><p>Adrish Dey has a fantastic <a class="reference external" href="https://wandb.ai/wandb_fc/LayerNorm/reports/Layer-Normalization-in-Pytorch-With-Examples---VmlldzoxMjk5MTk1">write up of Layer Normalization</a> (a main component of the ViT architecture) can help neural network training.</p></li>
<li><p>The self-attention (and multi-head self-attention) mechanism is at the heart of the ViT architecture as well as many other Transformer architectures, it was originally introduced in the <a class="reference external" href="https://arxiv.org/abs/1706.03762"><em>Attention is all you need</em></a> paper.</p></li>
<li><p>Yannic Kilcher’s YouTube channel is a sensational resource for visual paper walkthroughs, you can see his videos for the following papers:</p>
<ul>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=iDulhoQ2pro">Attention is all you need</a> (the paper that introduced the Transformer architecture).</p></li>
<li><p><a class="reference external" href="https://youtu.be/TrdevFK_am4">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> (the paper that introduced the ViT architecture).</p></li>
</ul>
</li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebook/pytorch_deep_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="07_pytorch_experiment_tracking.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">07. PyTorch Experiment Tracking</p>
      </div>
    </a>
    <a class="right-next"
       href="09_pytorch_model_deployment.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">09. PyTorch Model Deployment</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-paper-replicating">What is paper replicating?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-machine-learning-research-paper">What is a machine learning research paper?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-replicate-a-machine-learning-research-paper">Why replicate a machine learning research paper?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-can-you-find-code-examples-for-machine-learning-research-papers">Where can you find code examples for machine learning research papers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-were-going-to-cover">What we’re going to cover</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology">Terminology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-can-you-get-help">Where can you get help?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-setup">0. Getting setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-data">1. Get Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-datasets-and-dataloaders">2. Create Datasets and DataLoaders</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-transforms-for-images">2.1 Prepare transforms for images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-images-into-dataloaders">2.2 Turn images into <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>’s</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-a-single-image">2.3 Visualize a single image</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-the-vit-paper-an-overview">3. Replicating the ViT paper: an overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inputs-and-outputs-layers-and-blocks">3.1 Inputs and outputs, layers and blocks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-specific-whats-vit-made-of">3.2 Getting specific: What’s ViT made of?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-figure-1">3.2.1 Exploring Figure 1</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-the-four-equations">3.2.2 Exploring the Four Equations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-1-overview">3.2.3 Equation 1 overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-2-overview">3.2.4 Equation 2 overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-3-overview">3.2.5 Equation 3 overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-4-overview">3.2.6 Equation 4 overview</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-table-1">3.2.7 Exploring Table 1</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#my-workflow-for-replicating-papers">3.3 My workflow for replicating papers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding">4. Equation 1: Split data into patches and creating the class, position and patch embedding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-patch-embedding-input-and-output-shapes-by-hand">4.1 Calculating patch embedding input and output shapes by hand</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turning-a-single-image-into-patches">4.2 Turning a single image into patches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-image-patches-with-torch-nn-conv2d">4.3 Creating image patches with <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flattening-the-patch-embedding-with-torch-nn-flatten">4.4 Flattening the patch embedding with <code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turning-the-vit-patch-embedding-layer-into-a-pytorch-module">4.5 Turning the ViT patch embedding layer into a PyTorch module</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-class-token-embedding">4.6 Creating the class token embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-position-embedding">4.7 Creating the position embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-from-image-to-embedding">4.8 Putting it all together: from image to embedding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-2-multi-head-attention-msa">5. Equation 2: Multi-Head Attention (MSA)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-layernorm-ln-layer">5.1 The LayerNorm (LN) layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-multi-head-self-attention-msa-layer">5.2 The Multi-Head Self Attention (MSA) layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-equation-2-with-pytorch-layers">5.3 Replicating Equation 2 with PyTorch layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#equation-3-multilayer-perceptron-mlp">6. Equation 3: Multilayer Perceptron (MLP)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mlp-layer-s">6.1 The MLP layer(s)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-equation-3-with-pytorch-layers">6.2 Replicating Equation 3 with PyTorch layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-transformer-encoder">7. Create the Transformer Encoder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-transformer-encoder-by-combining-our-custom-made-layers">7.1 Creating a Transformer Encoder by combining our custom made layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-transformer-encoder-with-pytorchs-transformer-layers">7.2 Creating a Transformer Encoder with PyTorch’s Transformer layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-to-create-vit">8. Putting it all together to create ViT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-visual-summary-of-our-vit-model">8.1 Getting a visual summary of our ViT model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-training-code-for-our-vit-model">9. Setting up training code for our ViT model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-an-optimizer">9.1 Creating an optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-loss-function">9.2 Creating a loss function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-our-vit-model">9.3 Training our ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-our-training-setup-is-missing">9.4 What our training setup is missing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-loss-curves-of-our-vit-model">9.5 Plot the loss curves of our ViT model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-a-pretrained-vit-from-torchvision-models-on-the-same-dataset">10. Using a pretrained ViT from <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code> on the same dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-a-pretrained-model">10.1 Why use a pretrained model?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-a-pretrained-vit-model-and-creating-a-feature-extractor">10.2 Getting a pretrained ViT model and creating a feature extractor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-data-for-the-pretrained-vit-model">10.3 Preparing data for the pretrained ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-feature-extractor-vit-model">10.4 Train feature extractor ViT model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-feature-extractor-vit-model-loss-curves">10.5 Plot feature extractor ViT model loss curves</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-feature-extractor-vit-model-and-check-file-size">10.6 Save feature extractor ViT model and check file size</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-predictions-on-a-custom-image">11. Make predictions on a custom image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-takeaways">Main takeaways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extra-curriculum">Extra-curriculum</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By thangckt
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>