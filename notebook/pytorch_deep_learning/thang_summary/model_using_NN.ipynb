{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Network\n",
    "\n",
    "A [classification problem](https://en.wikipedia.org/wiki/Statistical_classification) involves predicting whether something is one thing or another.\n",
    "\n",
    "| Problem type | What is it? | Example |\n",
    "| ----- | ----- | ----- |\n",
    "| **Binary classification** | Target can be one of two options, e.g. yes or no | Predict whether or not someone has heart disease based on their health parameters. |\n",
    "| **Multi-class classification** | Target can be one of more than two options | Decide whether a photo of is of food, a person or a dog. |\n",
    "| **Multi-label classification** | Target can be assigned more than one option | Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science & philosohpy). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "\n",
    "\n",
    "### Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors and split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Turn data into tensors\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Build model\n",
    "\n",
    "- Build model based on base-class `nn.Module`\n",
    "- Define `forward()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a model with 2 nn.Linear layers\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n",
    "    \n",
    "    def forward(self, x):       # forward method is required in every model\n",
    "        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n",
    "\n",
    "    ## or code like this\n",
    "    # def forward(self, x):       \n",
    "    #     z = self.layer_1(x)\n",
    "    #     z = self.layer_1(z)\n",
    "    #     return z\n",
    "\n",
    "model_0 = CircleModelV0()\n",
    "\n",
    "## Loss function and optimizer\n",
    "# loss_fn = nn.BCELoss() # Requires sigmoid on input\n",
    "loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=0.1)      # args: params & lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model & testing\n",
    "\n",
    "```{admonition} \"PyTorch training loop steps\"\n",
    "1. **Forward pass** - The model goes through all of the training data once, performing its `forward()` function calculations (compute `model(x_train)`).\n",
    "2. **Calculate the loss** - The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are (`loss = loss_fn(y_pred, y_train)`).\n",
    "3. **Zero gradients** - The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step (`optimizer.zero_grad()`).\n",
    "4. **Perform backpropagation on the loss** - Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with `requires_grad=True`). This is known as backpropagation, hence \"backwards\" (`loss.backward()`).\n",
    "5. **Step the optimizer (gradient descent)** - Update the parameters with `requires_grad=True` with respect to the loss gradients in order to improve them (`optimizer.step()`).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model\n",
    "torch.manual_seed(42)\n",
    "epochs = 100\n",
    "\n",
    "## Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train)          # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred) \n",
    "                      \n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_0(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. Calculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "We may need to normalize the output from model in this step.\n",
    "\n",
    "Model outputs \"raw output\" (called **logits**), but we wanted to figure out exactly which label is; Therefore, we use `activation function` to turn out into *an expected form* (I may call this step as the **normalize output** step). We will go from `logits -> prediction probabilities -> prediction labels`\n",
    "\n",
    "\n",
    "### Improving model (hyperparameters tuning)\n",
    "There are a few ways to make a model better prediction. See [example here](../pytorch_deep_learning/02_pytorch_classification.ipynb)\n",
    "\n",
    "#### Add more layers and neurons\n",
    "This model add more layers and neurons (hidden units), and increase training time compare to `model_0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a model with 3 nn.Linear layers\n",
    "class CircleModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        z = self.layer_1(x)\n",
    "        z = self.layer_2(z)\n",
    "        z = self.layer_3(z)\n",
    "        return z\n",
    "\n",
    "model_1 = CircleModelV1()\n",
    "\n",
    "## Loss function and optimizer\n",
    "# loss_fn = nn.BCELoss() # Requires sigmoid on input\n",
    "loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)      # args: params & lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 1000 # Train for longer\n",
    "\n",
    "## Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    # 1. Forward pass\n",
    "    y_logits = model_1(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> predicition probabilities -> prediction labels\n",
    "\n",
    "    # 2. Calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "                      \n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_1(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. Calculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "                               \n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the activation functions\n",
    "Building a model with non-linearity (using **non-linear activation functions**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build model with non-linear activation function\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.relu = nn.ReLU() # <- add in ReLU activation function\n",
    "        # Can also put sigmoid in the model \n",
    "        # This would mean you don't need to use it on the predictions\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Intersperse the ReLU activation function between layers\n",
    "        z = self.relu(self.layer_1(x))\n",
    "        z = self.relu(self.layer_2(z))\n",
    "        z = self.layer_3(z)\n",
    "        return z\n",
    "\n",
    "model_3 = CircleModelV2()\n",
    "\n",
    "## Loss function and optimizer\n",
    "# loss_fn = nn.BCELoss() # Requires sigmoid on input\n",
    "loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)      # args: params & lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfdfd2918ff9d262c836e8815c89caad5df20c22cef5d9d2cb941140a5368784"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
