{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Overview of Machine Learning\n",
    "\n",
    "We will learn about how machine learning is a method of modeling data, typically with predictive functions. Machine learning includes many techniques, but here we will focus on only those necessary to transition into deep learning. For example, random forests, support vector machines, and nearest neighbor are widely-used machine learning techniques that are effective but not covered here.\n",
    "\n",
    "What is about the model ?\n",
    "\n",
    "We want a model capable of handling our `inputs` and producing something in the shape of our `ouputs`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "Additional Dimensions\n",
    "- Complexity: multiple source and data streams\n",
    "- Variability\n",
    "    - Unpredictable Data flows\n",
    "    - Social media trending\n",
    "\n",
    "Why Big Data is important\n",
    "- Data constains information\n",
    "- information lead to insights\n",
    "- Insights helps in making better decisions\n",
    "\n",
    "How to derive insights from data?\n",
    "\n",
    "--> Machine Leanring\n",
    "\n",
    "Conclusions:\n",
    "- Data is nothing without insights\n",
    "- Machine Learning is the key for deriving inisghts from data\n",
    "- Big Data and Machine Learning ha a huge potential"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm in ML\n",
    "\n",
    "The below picture shows an overview of machine learning\n",
    "\n",
    "<img src=\"image/1_1_machine-learning.png\" style=\"width:600; align:center\" />\n",
    "\n",
    "### Supervised Learning\n",
    "Given `features` we want our model to predict `label`. [See more](https://thangckt.github.io/pytorch_deep_learning/02_pytorch_classification/#2-building-a-model)\n",
    "\n",
    "- Classification\n",
    "    - Decision Trees\n",
    "    - Naive Bayers Classification\n",
    "- Regession\n",
    "    - Ordinary Least Squares Regression\n",
    "    - Logistic Regession\n",
    "    - Support Vector Machines\n",
    "    - Ensemble Methods\n",
    "\n",
    "### Unsuppervised Learning\n",
    "No `label` in this type\n",
    "- Clustering\n",
    "    - Centroid-based algorithm\n",
    "    - Connectivity-based algorithm\n",
    "    - Density-based algorithm\n",
    "    - Probabilistic\n",
    "    - Dimensionality Reduction\n",
    "    - Neural network/ Deep Learning\n",
    "- Pricipal Component Analysis\n",
    "- Independent Component Analysis\n",
    "- Singular Value Decomposition\n",
    "\n",
    "### Reinforement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ingredients \n",
    "\n",
    "Machine learning the fitting of models $\\hat{f}(\\vec{x})$ to data $\\vec{x}, y$ that we know came from some ``data generation'' process $f(x)$ . Firstly, definitions:\n",
    "\n",
    "**Features** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ vectors $\\{\\vec{x}_i\\}$ of dimension $D$. Can be reals, integers, etc.\n",
    "\n",
    "**Labels** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ integers or reals $\\{y_i\\}$. $y_i$ is usually a scalar\n",
    "  \n",
    "**Labeled Data** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ tuples $\\{\\left(\\vec{x}_i, y_i\\right)\\}$ \n",
    "\n",
    "**Unlabeled Data** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;set of $N$ features  $\\{\\vec{x}_i\\}$  that may have unknown $y$ labels\n",
    "\n",
    "**Data generation process**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The unseen process $f(\\vec{x})$ that takes a given feature vector in and returns a real label $y$ (what we're trying to model)\n",
    "\n",
    "**Model**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;A function $\\hat{f}(\\vec{x})$ that takes a given feature vector in and returns a predicted $\\hat{y}$\n",
    "\n",
    "**Predictions**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{y}$, our predicted output for a given input $\\vec{x}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The content in this part is primary from: \n",
    "- [Deep Learning for molecules & materials](https://dmol.pub/ml)\n",
    "```\n",
    "\n",
    "```{seealso}\n",
    "1. [<ins>Introductory Machine Learning</ins>](https://ai.stanford.edu/~nilsson/mlbook.html)\n",
    "2. Two reviews of machine learning in materials{cite}`fung2021benchmarking,balachandran2019machine`\n",
    "3. A review of machine learning in computational chemistry{cite}`gomez2020machine`\n",
    "4. A review of machine learning in metals{cite}`nandy2018strategies`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminologies in ML\n",
    "\n",
    "- The patterns: the learned parameters in model, or the parameters to find in the relationship between inputs and outputs. For e.g., in linear model $y = ax +b$, the learned patterns (paramters to be found) are the weight `a` and the bias `b`.\n",
    "- Hidden units: neurons in hidden layers\n",
    "- Hypeparameters: are all user-choice parameters in model (e.g., learning rate, number of layers, number of neuron in layers,...)\n",
    "- Epoch: step\n",
    "- Loss function: measures how wrong your model predictions are. The higher the loss, the worse your model. It is sometimes calles \"loss criterion\", \"criterion\", or \"cost function\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow in ML\n",
    "\n",
    "This workflow work with PyTorch. See [this lesson](https://thangckt.github.io/pytorch_deep_learning/01_pytorch_workflow/)\n",
    "\n",
    "### 1. Prepare data\n",
    "1. Prepare inputs and output in the format suitable for ML framework will be used (e.g., Pytorch only work with data in the form of torch.tensor)\n",
    "2. Split data into sets of train and test (somtimes are: strain, validation, test)\n",
    "\n",
    "### 2. Build model\n",
    "1. Constructing a model by subclassing `nn.Module` \n",
    "2. Defining a loss function and optimizer.\n",
    "\n",
    "May consider more step: Setting up device agnostic code (so our model can run on CPU or GPU if it's available).\n",
    "\n",
    "### 3. Train model\n",
    "\n",
    "PyTorch steps in training:\n",
    "1. **Forward pass** - The model goes through all of the training data once, performing its `forward()` function calculations (compute `model(x_train)`).\n",
    "2. **Calculate the loss** - The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are (`loss = loss_fn(y_pred, y_train)`).\n",
    "3. **Zero gradients** - The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step (`optimizer.zero_grad()`).\n",
    "4. **Perform backpropagation on the loss** - Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with `requires_grad=True`). This is known as backpropagation, hence \"backwards\" (`loss.backward()`).\n",
    "5. **Step the optimizer (gradient descent)** - Update the parameters with `requires_grad=True` with respect to the loss gradients in order to improve them (`optimizer.step()`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:12:32) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b6e7cfdce5ef245d32482b7f80393907c6182a3f3a40203474e09cb3d62b454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
