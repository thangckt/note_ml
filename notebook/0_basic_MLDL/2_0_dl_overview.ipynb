{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Overview\n",
    "\n",
    "**Deep learning** is a category of **machine learning**. Machine learning is a category of **artificial intelligence**. Deep learning is the use of neural networks to do machine learning, like classify and regress data. This chapter provides an overview and we will dive further into these topics in later chapters.\n",
    "\n",
    "There are many good resources on deep learning to supplement these chapters.\n",
    "- The introduction the from [Ian Goodfellow's book](https://www.deeplearningbook.org/contents/intro.html) to be a good intro. \n",
    "- [short video series](https://www.youtube.com/watch?v=aircAruvnKk) specifically about neural networks that give an applied introduction to the topic.\n",
    "- DeepMind has a high-level video showing what can be accomplished with [deep learning & AI](https://www.youtube.com/watch?v=7R52wiUgxZI). \n",
    "- When people write \"deep learning is a powerful tool\" in their research papers, they typically cite [this Nature paper](https://www.nature.com/articles/nature14539) by Yann LeCun, Yoshua Bengio, and Geoffery Hinton. \n",
    "- A practical and example-driven [online book](http://d2l.ai/index.html) that gives each example in Tensorflow, PyTorch, and MXNet. \n",
    "- Many chemistry-specific examples and information about deep learning in chemistry via the excellent [DeepChem](https://deepchem.io/) project. \n",
    "\n",
    "The main advice I would give to beginners in deep learning are to focus **less** on the neurological inspired language (i.e., connections between neurons), and instead view deep learning as a series of linear algebra operations where many of the matrices are filled with adjustable parameters. Of course nonlinear functions (activations) are used to join the linear algebra operations, but deep learning is essentially linear algebra operations specified via a \"computation network\" (aka computation graph) that vaguely looks like neurons connected in a brain.\n",
    "\n",
    "```{admonition} nonlinearity\n",
    "A function $f(\\vec{x})$ is linear if two conditions hold:\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\vec{x} + \\vec{y}) = f(\\vec{x}) + f(\\vec{y})\n",
    "\\end{equation}\n",
    "\n",
    "for all $\\vec{x}$ and $\\vec{y}$. And\n",
    "\n",
    "\\begin{equation}\n",
    "f(s\\vec{x}) = sf(\\vec{x})\n",
    "\\end{equation}\n",
    "\n",
    "where $s$ is a scalar. A function is **nonlinear** if these conditions do not hold for some $\\vec{x}$.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The content in this part is primary from: \n",
    "- [Deep Learning for molecules & materials](https://dmol.pub/ml)\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrtalpha\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:15:42) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b6e7cfdce5ef245d32482b7f80393907c6182a3f3a40203474e09cb3d62b454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
